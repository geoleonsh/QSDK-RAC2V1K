From c3778eb2e89c5af6528710e17d2dd5526c0de593 Mon Sep 17 00:00:00 2001
From: Karthikeyan Kathirvel <quic_kathirve@quicinc.com>
Date: Mon, 6 Feb 2023 12:31:24 +0530
Subject: [PATCH 2/3] ath12k: add support for Mgmt Rx re-ordering

Consider an MLD formed by multiple SoCs. Management frames received
on individual links will be forwarded or consumed by the firmware
running on that SoC. Each link could have variable delay in FW->Host
messaging. Host can receive these management frames in an order
different from that in which the frames were received over the air. This
problem could be there in single-SoC MLD case also due to race
between FW threads that are handling management frames.

Consider a scenario of client sending deauth followed back-to-back by
auth req. If they are sent on different links, deauth could arrive at AP
host after auth req though deauth ACK’ed by AP HW first, and then auth
req ACK’ed by AP HW. This can lead to issues. We need to process the
management frames received across all the links in the order in which HW
ACK’ed them.

The reordering of frames is done based on the packet sequence number and
timestamp of a frame, this info is sent by hw through
WMI_MGMT_RX_EVENTID in tag WMI_TAG_MLO_MGMT_RX_REO_PARAMS.
To enable this feature execute below command
insmod ath12k mgmt_rx_reorder=1

Signed-off-by: Karthikeyan Kathirvel <quic_kathirve@quicinc.com>
---
 drivers/net/wireless/ath/ath12k/core.c  |  249 +++
 drivers/net/wireless/ath/ath12k/core.h  |    2 +
 drivers/net/wireless/ath/ath12k/debug.h |    1 +
 drivers/net/wireless/ath/ath12k/pci.c   |   11 +
 drivers/net/wireless/ath/ath12k/wmi.c   | 2665 +++++++++++++++++++++--
 drivers/net/wireless/ath/ath12k/wmi.h   |  742 +++++++
 6 files changed, 3515 insertions(+), 155 deletions(-)

--- a/drivers/net/wireless/ath/ath12k/core.c
+++ b/drivers/net/wireless/ath/ath12k/core.c
@@ -47,6 +47,10 @@ MODULE_PARM_DESC(en_fwlog, "fwlog: 0-dis
 
 static unsigned int ath12k_recovery_mode = ATH12K_MLO_RECOVERY_MODE0;
 
+bool ath12k_mgmt_rx_reordering = false;
+module_param_named(mgmt_rx_reorder, ath12k_mgmt_rx_reordering, bool, 0644);
+MODULE_PARM_DESC(mgmt_rx_reorder, "Mgmt Rx Re-Ordering (0 - disable, 1 - enable)");
+
 static DEFINE_MUTEX(ath12k_hw_lock);
 static struct list_head ath12k_hw_groups = LIST_HEAD_INIT(ath12k_hw_groups);
 
@@ -765,6 +769,7 @@ static void ath12k_core_stop(struct ath1
 	if (!test_bit(ATH12K_FLAG_CRASH_FLUSH, &ab->dev_flags))
 		ath12k_qmi_firmware_stop(ab);
 
+	ath12k_mgmt_rx_reo_deinit_context(ab);
 	ath12k_hif_stop(ab);
 	ath12k_wmi_detach(ab);
 	ath12k_dp_rx_pdev_reo_cleanup(ab);
@@ -807,6 +812,248 @@ static void ath12k_core_soc_destroy(stru
 	ath12k_qmi_deinit_service(ab);
 }
 
+/**
+ * ath12k_core_mgmt_rx_reo_init_ss_params() - Initialize a given snapshot
+ * params object
+ * @snapshot_params: Pointer to snapshot params object
+ *
+ * Return: void
+ */
+static void
+ath12k_core_mgmt_rx_reo_init_ss_params(
+	    struct ath12k_mgmt_rx_reo_snapshot_params *snapshot_params)
+{
+	snapshot_params->valid = false;
+	snapshot_params->mgmt_pkt_ctr = 0;
+	snapshot_params->global_timestamp = 0;
+}
+
+/**
+ * ath12k_core_rx_reo_init_ss_value() - Initialize management Rx reorder
+ * snapshot values for a given pdev
+ * @pdev: pointer to pdev object
+ *
+ * Return: 0 for success, non-zero for failure
+ */
+static int
+ath12k_core_rx_reo_init_ss_value(struct ath12k *ar)
+{
+	enum ath12k_mgmt_rx_reo_shared_snapshot_id snapshot_id;
+	struct ath12k_mgmt_rx_reo_pdev_info *mgmt_rx_reo_pdev_ctx;
+
+	mgmt_rx_reo_pdev_ctx = &ar->rx_reo_pdev_ctx;
+	snapshot_id = 0;
+	while (snapshot_id < ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX) {
+		ath12k_core_mgmt_rx_reo_init_ss_params
+			(&mgmt_rx_reo_pdev_ctx->last_valid_shared_snapshot
+			 [snapshot_id]);
+		snapshot_id++;
+	}
+
+	/* Initialize Host snapshot params */
+	ath12k_core_mgmt_rx_reo_init_ss_params
+		(&mgmt_rx_reo_pdev_ctx->host_snapshot);
+
+	return 0;
+}
+
+void *ath12k_core_mgmt_rx_reo_get_ss_address(
+		struct ath12k_base *ab,
+	u8 link_id,
+	enum ath12k_mgmt_rx_reo_shared_snapshot_id snapshot_id)
+{
+	struct ath12k_host_mlo_mem_arena *mlomem_arena_ctx;
+	struct ath12k_host_mlo_glb_rx_reo_snapshot_info *snapshot_info;
+	struct ath12k_host_mlo_glb_rx_reo_per_link_info *snapshot_link_info;
+	u8 link;
+
+	if (snapshot_id >= ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX) {
+		ath12k_err(ab, "Invalid snapshot ID: %d\n", snapshot_id);
+		return NULL;
+	}
+
+	mlomem_arena_ctx = &ab->ag->mlomem_arena;
+	snapshot_info = &mlomem_arena_ctx->rx_reo_snapshot_info;
+
+	for (link = 0; link < snapshot_info->num_links; ++link) {
+		snapshot_link_info = &snapshot_info->link_info[link];
+
+		if (link_id == snapshot_link_info->link_id)
+			break;
+	}
+
+	if (link == snapshot_info->num_links) {
+		ath12k_err(ab, "Couldn't find the snapshot link info corresponding to the link %d\n",
+			   link_id);
+		return NULL;
+	}
+
+	switch (snapshot_id) {
+	case ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAC_HW:
+		return snapshot_link_info->hw_forwarded;
+
+	case ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_CONSUMED:
+		return snapshot_link_info->fw_consumed;
+
+	case ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_FORWARDED:
+		return snapshot_link_info->fw_forwarded;
+
+	default:
+		WARN_ON(1);
+	}
+
+	return NULL;
+}
+
+static int ath12k_mgmt_rx_reo_get_snapshot_version(struct ath12k_base *ab,
+						   enum ath12k_mgmt_rx_reo_shared_snapshot_id id)
+{
+	struct ath12k_host_mlo_mem_arena *mlomem_arena_ctx;
+	struct ath12k_host_mlo_glb_rx_reo_snapshot_info *snapshot_info;
+	int snapshot_version;
+
+	if (id >= ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX) {
+		ath12k_err(ab, "Invalid snapshot ID: %d\n", id);
+		return ATH12K_MGMT_RX_REO_INVALID_SNAPSHOT_VERSION;
+	}
+
+	mlomem_arena_ctx = &ab->ag->mlomem_arena;
+	snapshot_info = &mlomem_arena_ctx->rx_reo_snapshot_info;
+
+	switch (id) {
+	case ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAC_HW:
+		snapshot_version = snapshot_info->hw_forwarded_snapshot_ver;
+		break;
+
+	case ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_CONSUMED:
+		snapshot_version = snapshot_info->fw_consumed_snapshot_ver;
+		break;
+
+	case ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_FORWARDED:
+		snapshot_version = snapshot_info->fw_forwarded_snapshot_ver;
+		break;
+
+	default:
+		snapshot_version = ATH12K_MGMT_RX_REO_INVALID_SNAPSHOT_VERSION;
+		break;
+	}
+
+	return snapshot_version;
+}
+
+static int
+ath12k_core_mgmt_rx_reo_get_snapshot_info
+	    (struct ath12k *ar,
+	     enum ath12k_mgmt_rx_reo_shared_snapshot_id id,
+	     struct ath12k_mgmt_rx_reo_snapshot_info *snapshot_info)
+{
+	u8 link_id;
+	u8 snapshot_version;
+	struct ath12k_base *ab;
+
+	ab = ar->ab;
+
+	if (id >= ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX) {
+		ath12k_err(ab, "Mgmt RX REO snapshot id invalid %d\n", id);
+		return -EINVAL;
+	}
+
+	if (!snapshot_info) {
+		ath12k_err(ab, "Ref to mgmt RX REO snapshot info is null\n");
+		return -EINVAL;
+	}
+
+	link_id = ar->pdev->hw_link_id;
+
+	snapshot_info->address =
+		ath12k_core_mgmt_rx_reo_get_ss_address(ab, link_id, id);
+	if (!snapshot_info->address) {
+		ath12k_err(ab, "NULL snapshot address\n");
+		return -EINVAL;
+	}
+
+	snapshot_version = ath12k_mgmt_rx_reo_get_snapshot_version(ab, id);
+	if (snapshot_version < 0) {
+		ath12k_err(ab, "Invalid snapshot version %d\n",
+			   snapshot_version);
+		return -EINVAL;
+	}
+
+	snapshot_info->version = snapshot_version;
+
+	return 0;
+}
+
+/**
+ * ath12k_mgmt_rx_reo_initialize_snapshot_address() - Initialize management Rx reorder
+ * snapshot addresses for a given pdev
+ * @pdev: pointer to pdev object
+ *
+ * Return: 0 for success, non-zero for failure
+ */
+static int
+ath12k_core_mgmt_rx_reo_init_ss_address(struct ath12k *ar)
+{
+	enum ath12k_mgmt_rx_reo_shared_snapshot_id snapshot_id;
+	struct ath12k_mgmt_rx_reo_pdev_info *mgmt_rx_reo_pdev_ctx;
+	int status;
+
+	mgmt_rx_reo_pdev_ctx = &ar->rx_reo_pdev_ctx;
+	snapshot_id = 0;
+	while (snapshot_id < ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX) {
+		struct ath12k_mgmt_rx_reo_snapshot_info *snapshot_info;
+
+		snapshot_info =
+			&mgmt_rx_reo_pdev_ctx->host_target_shared_snapshot_info
+			[snapshot_id];
+		status = ath12k_core_mgmt_rx_reo_get_snapshot_info
+			(ar, snapshot_id, snapshot_info);
+		if (status) {
+			ath12k_err(ar->ab, "Get snapshot info failed, id = %u\n",
+				   snapshot_id);
+			return status;
+		}
+
+		snapshot_id++;
+	}
+
+	return 0;
+}
+
+static int ath12k_core_mgmt_rx_reordering_init(struct ath12k_base *ab)
+{
+	int i, ret;
+	struct ath12k *ar;
+	struct ath12k_pdev *pdev;
+	struct ath12k_hw_group *ag = ab->ag;
+
+	if (!(ag->mlo_mem.is_mlo_mem_avail && ag->mgmt_rx_reorder))
+		return 0;
+
+	for (i = 0; i < ab->num_radios; i++) {
+		pdev = &ab->pdevs[i];
+		ar = pdev->ar;
+		if (!ar)
+			continue;
+		ret = ath12k_core_rx_reo_init_ss_value(ar);
+		if (ret) {
+			ath12k_err(ab, "Failed to initialize snapshot value\n");
+			return ret;
+		}
+
+		ret = ath12k_core_mgmt_rx_reo_init_ss_address(ar);
+		if (ret) {
+			ath12k_err(ab, "Failed to initialize snapshot address\n");
+			return ret;
+		}
+
+		ar->rx_reo_pdev_ctx.init_complete = true;
+	}
+
+
+	return 0;
+}
+
 static int ath12k_core_pdev_init(struct ath12k_base *ab)
 {
 	int ret;
@@ -824,8 +1071,16 @@ static int ath12k_core_pdev_init(struct
 		goto err_thermal_unregister;
 	}
 
+	ret = ath12k_core_mgmt_rx_reordering_init(ab);
+	if (ret) {
+		ath12k_err(ab, "failed to rx reo reordering %d\n", ret);
+		goto err_spectral_deinit;
+	}
+
 	return 0;
 
+err_spectral_deinit:
+	ath12k_spectral_deinit(ab);
 err_thermal_unregister:
 	ath12k_thermal_unregister(ab);
 	return ret;
@@ -1270,6 +1525,10 @@ int ath12k_core_qmi_firmware_ready(struc
 
 	mutex_unlock(&ag->mutex_lock);
 
+	/* initialize the mgmt rx re-order after
+	 * mlo mem is available
+	 */
+	ath12k_mgmt_rx_reo_init_context(ab);
 	ath12k_qmi_mlo_global_snapshot_mem_init(ab);
 
 	/* Add code here carefully */
@@ -1821,6 +2080,8 @@ int ath12k_core_init(struct ath12k_base
 		}
 	}
 
+	ag->mgmt_rx_reorder = ath12k_mgmt_rx_reordering;
+
 	return 0;
 
 err_hw_group:
--- a/drivers/net/wireless/ath/ath12k/core.h
+++ b/drivers/net/wireless/ath/ath12k/core.h
@@ -959,6 +959,7 @@ struct ath12k {
 
 	struct completion mvr_complete;
 	struct cfg80211_chan_def agile_chandef;
+	struct ath12k_mgmt_rx_reo_pdev_info rx_reo_pdev_ctx;
 };
 
 struct ath12k_band_cap {
@@ -1140,12 +1141,14 @@ struct ath12k_hw_group {
 	u8 num_hw;
 	bool mlo_capable;
 	bool hw_queues_stopped;
+	bool mgmt_rx_reorder;
 	unsigned long dev_flags;
 	struct ath12k_hw *ah[ATH12K_GROUP_MAX_RADIO];
 	struct ath12k_base *ab[ATH12K_MAX_SOCS];
 	struct ath12k __rcu *hw_links[ATH12K_GROUP_MAX_RADIO];
 	struct ath12k_mlo_memory mlo_mem;
 	struct mutex mutex_lock;
+	struct ath12k_mgmt_rx_reo_context rx_reo;
 	struct ath12k_host_mlo_mem_arena mlomem_arena;
 };
 
@@ -1264,7 +1267,6 @@ struct ath12k_base {
 		u32 fw_crash_counter;
 		u32 last_recovery_time;
 	} stats;
-	bool mgmt_rx_reorder;
 	bool ftm_segment_handler;
 	struct ath12k_ftm_event_obj ftm_event_obj;
 	u32 pktlog_defs_checksum;
--- a/drivers/net/wireless/ath/ath12k/debug.h
+++ b/drivers/net/wireless/ath/ath12k/debug.h
@@ -27,6 +27,7 @@ enum ath12k_debug_mask {
 	ATH12K_DBG_DP_TX	= 0x00002000,
 	ATH12K_DBG_DP_RX	= 0x00004000,
 	ATH12K_DBG_OFFSET	= 0x00008000,
+	ATH12K_DBG_RX_REO	= 0x00010000,
 	ATH12K_DBG_ANY		= 0xffffffff,
 };
 
--- a/drivers/net/wireless/ath/ath12k/wmi.c
+++ b/drivers/net/wireless/ath/ath12k/wmi.c
@@ -6722,7 +6722,57 @@ static int ath12k_wmi_tlv_mgmt_rx_parse(
 			parse->frame_buf_done = true;
 		}
 		break;
+	case WMI_TAG_MLO_MGMT_RX_REO_PARAMS:
+		parse->reo_params = (struct ath12k_wmi_mgmt_rx_reo_params *)ptr;
+		break;
+	case WMI_TAG_MLO_MGMT_RX_FW_CONSUMED_HDR:
+		parse->fw_consumed_reo_params = (struct ath12k_wmi_mgmt_rx_fw_consumed_hdr *)ptr;
+		break;
+	}
+	return 0;
+}
+
+static int ath12k_pull_fw_consumed_mgmt_rx_params_tlv(struct ath12k_base *ab,
+					  struct sk_buff *skb,
+					  struct mgmt_rx_event_params *hdr)
+{
+	struct wmi_tlv_mgmt_rx_parse parse = { };
+	int ret;
+	struct ath12k_wmi_mgmt_rx_fw_consumed_hdr *fw_con_reo_params_tlv = NULL;
+
+	ret = ath12k_wmi_tlv_iter(ab, skb->data, skb->len,
+			ath12k_wmi_tlv_mgmt_rx_parse,
+			&parse);
+	if (ret) {
+		ath12k_warn(ab, "failed to parse mgmt rx tlv %d\n", ret);
+		return ret;
 	}
+
+
+	fw_con_reo_params_tlv = parse.fw_consumed_reo_params;
+
+	if (!fw_con_reo_params_tlv) {
+		ret = -EINVAL;
+		ath12k_warn(ab, "no fw_consumed_reo_params\n");
+		return ret;
+	}
+
+	hdr->pdev_id = fw_con_reo_params_tlv->pdev_id;
+	hdr->reo_params.valid = FIELD_GET(WMI_MGMT_RX_FW_CONSUMED_PARAM_MGMT_PKT_CTR_VALID_GET,
+			fw_con_reo_params_tlv->mgmt_pkt_ctr_info);
+	hdr->reo_params.global_timestamp = fw_con_reo_params_tlv->global_timestamp;
+	hdr->reo_params.mgmt_pkt_ctr = FIELD_GET(WMI_MGMT_RX_FW_CONSUMED_PARAM_MGMT_PKT_CTR_GET,
+			fw_con_reo_params_tlv->mgmt_pkt_ctr_info);
+	hdr->reo_params.duration_us = fw_con_reo_params_tlv->rx_ppdu_duration_us;
+	hdr->reo_params.start_timestamp = hdr->reo_params.global_timestamp;
+	hdr->reo_params.end_timestamp = hdr->reo_params.start_timestamp +
+		hdr->reo_params.duration_us;
+
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "FW Consumed : Mgmt Re-order ingress: valid %u global_ts %u pkt_ctr %u\n",
+			hdr->reo_params.valid,
+			hdr->reo_params.global_timestamp,
+			hdr->reo_params.mgmt_pkt_ctr);
+
 	return 0;
 }
 
@@ -6734,6 +6784,7 @@ static int ath12k_pull_mgmt_rx_params_tl
 	const struct wmi_mgmt_rx_hdr *ev;
 	const u8 *frame;
 	int ret;
+	struct ath12k_wmi_mgmt_rx_reo_params *reo_params_tlv = NULL;
 
 	ret = ath12k_wmi_tlv_iter(ab, skb->data, skb->len,
 				  ath12k_wmi_tlv_mgmt_rx_parse,
@@ -6746,6 +6797,13 @@ static int ath12k_pull_mgmt_rx_params_tl
 	ev = parse.fixed;
 	frame = parse.frame_buf;
 
+	reo_params_tlv = parse.reo_params;
+	if (!reo_params_tlv) {
+		ret = -EINVAL;
+		ath12k_warn(ab, "no reo_params_tlv\n");
+		return ret;
+	}
+
 	if (!ev || !frame) {
 		ath12k_warn(ab, "failed to fetch mgmt rx hdr");
 		return -EPROTO;
@@ -6764,6 +6822,24 @@ static int ath12k_pull_mgmt_rx_params_tl
 	hdr->tsf_delta =  ev->tsf_delta;
 	memcpy(hdr->rssi_ctl, ev->rssi_ctl, sizeof(hdr->rssi_ctl));
 
+	hdr->reo_params.pdev_id = ev->pdev_id;
+
+	hdr->reo_params.valid = FIELD_GET(WMI_MGMT_RX_REO_PARAM_MGMT_PKT_CTR_VALID_GET,
+			reo_params_tlv->mgmt_pkt_ctr_link_info);
+	hdr->reo_params.global_timestamp = reo_params_tlv->global_timestamp;
+	hdr->reo_params.mgmt_pkt_ctr = FIELD_GET(WMI_MGMT_RX_REO_PARAM_MGMT_PKT_CTR_GET,
+			reo_params_tlv->mgmt_pkt_ctr_link_info);
+	hdr->reo_params.duration_us = reo_params_tlv->rx_ppdu_duration_us;
+	hdr->reo_params.start_timestamp = hdr->reo_params.global_timestamp;
+	hdr->reo_params.end_timestamp = hdr->reo_params.start_timestamp +
+		hdr->reo_params.duration_us;
+
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "Mgmt Re-order ingress: channel %d valid %u global_ts %u pkt_ctr %u\n",
+			hdr->channel,
+			hdr->reo_params.valid,
+			hdr->reo_params.global_timestamp,
+			hdr->reo_params.mgmt_pkt_ctr);
+
 	if (skb->len < (frame - skb->data) + hdr->buf_len) {
 		ath12k_warn(ab, "invalid length in mgmt rx hdr ev");
 		return -EPROTO;
@@ -8026,207 +8102,2527 @@ static int ath12k_wmi_tlv_rdy_parse(stru
 		rdy_parse->num_extra_mac_addr =
 			fixed_param.ready_event_min.num_extra_mac_addr;
 
-		ether_addr_copy(ab->mac_addr,
-				fixed_param.ready_event_min.mac_addr.addr);
-		ab->pktlog_defs_checksum = fixed_param.pktlog_defs_checksum;
-		ab->wmi_ready = true;
-		break;
-	case WMI_TAG_ARRAY_FIXED_STRUCT:
-		addr_list = (struct wmi_mac_addr *)ptr;
-		num_mac_addr = rdy_parse->num_extra_mac_addr;
+		ether_addr_copy(ab->mac_addr,
+				fixed_param.ready_event_min.mac_addr.addr);
+		ab->pktlog_defs_checksum = fixed_param.pktlog_defs_checksum;
+		ab->wmi_ready = true;
+		break;
+	case WMI_TAG_ARRAY_FIXED_STRUCT:
+		addr_list = (struct wmi_mac_addr *)ptr;
+		num_mac_addr = rdy_parse->num_extra_mac_addr;
+
+		if (!(ab->num_radios > 1 && num_mac_addr >= ab->num_radios))
+			break;
+
+		for (i = 0; i < ab->num_radios; i++) {
+			pdev = &ab->pdevs[i];
+			ether_addr_copy(pdev->mac_addr, addr_list[i].addr);
+		}
+		ab->pdevs_macaddr_valid = true;
+		break;
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+static int ath12k_ready_event(struct ath12k_base *ab, struct sk_buff *skb)
+{
+	struct wmi_tlv_rdy_parse rdy_parse = { };
+	int ret;
+
+	ret = ath12k_wmi_tlv_iter(ab, skb->data, skb->len,
+				  ath12k_wmi_tlv_rdy_parse, &rdy_parse);
+	if (ret) {
+		ath12k_warn(ab, "failed to parse tlv %d\n", ret);
+		return ret;
+	}
+
+	complete(&ab->wmi_ab.unified_ready);
+	return 0;
+}
+
+static void ath12k_peer_delete_resp_event(struct ath12k_base *ab, struct sk_buff *skb)
+{
+	struct wmi_peer_delete_resp_event peer_del_resp;
+	struct ath12k *ar;
+
+	if (ath12k_pull_peer_del_resp_ev(ab, skb, &peer_del_resp) != 0) {
+		ath12k_warn(ab, "failed to extract peer delete resp");
+		return;
+	}
+
+	rcu_read_lock();
+	ar = ath12k_mac_get_ar_by_vdev_id(ab, peer_del_resp.vdev_id);
+	if (!ar) {
+		ath12k_warn(ab, "invalid vdev id in peer delete resp ev %d",
+			    peer_del_resp.vdev_id);
+		rcu_read_unlock();
+		return;
+	}
+
+	complete(&ar->peer_delete_done);
+	rcu_read_unlock();
+	ath12k_dbg(ab, ATH12K_DBG_WMI, "peer delete resp for vdev id %d addr %pM\n",
+		   peer_del_resp.vdev_id, peer_del_resp.peer_macaddr.addr);
+}
+
+static void ath12k_vdev_delete_resp_event(struct ath12k_base *ab,
+					  struct sk_buff *skb)
+{
+	struct ath12k *ar;
+	u32 vdev_id = 0;
+
+	if (ath12k_pull_vdev_del_resp_ev(ab, skb, &vdev_id) != 0) {
+		ath12k_warn(ab, "failed to extract vdev delete resp");
+		return;
+	}
+
+	rcu_read_lock();
+	ar = ath12k_mac_get_ar_by_vdev_id(ab, vdev_id);
+	if (!ar) {
+		ath12k_warn(ab, "invalid vdev id in vdev delete resp ev %d",
+			    vdev_id);
+		rcu_read_unlock();
+		return;
+	}
+
+	complete(&ar->vdev_delete_done);
+
+	rcu_read_unlock();
+
+	ath12k_dbg(ab, ATH12K_DBG_WMI, "vdev delete resp for vdev id %d\n",
+		   vdev_id);
+}
+
+static const char *ath12k_wmi_vdev_resp_print(u32 vdev_resp_status)
+{
+	switch (vdev_resp_status) {
+	case WMI_VDEV_START_RESPONSE_INVALID_VDEVID:
+		return "invalid vdev id";
+	case WMI_VDEV_START_RESPONSE_NOT_SUPPORTED:
+		return "not supported";
+	case WMI_VDEV_START_RESPONSE_DFS_VIOLATION:
+		return "dfs violation";
+	case WMI_VDEV_START_RESPONSE_INVALID_REGDOMAIN:
+		return "invalid regdomain";
+	default:
+		return "unknown";
+	}
+}
+
+static void ath12k_vdev_start_resp_event(struct ath12k_base *ab, struct sk_buff *skb)
+{
+	struct wmi_vdev_start_resp_event vdev_start_resp;
+	struct ath12k *ar;
+	u32 status;
+
+	if (ath12k_pull_vdev_start_resp_tlv(ab, skb, &vdev_start_resp) != 0) {
+		ath12k_warn(ab, "failed to extract vdev start resp");
+		return;
+	}
+
+	rcu_read_lock();
+	ar = ath12k_mac_get_ar_by_vdev_id(ab, vdev_start_resp.vdev_id);
+	if (!ar) {
+		ath12k_warn(ab, "invalid vdev id in vdev start resp ev %d",
+			    vdev_start_resp.vdev_id);
+		rcu_read_unlock();
+		return;
+	}
+
+	ar->last_wmi_vdev_start_status = 0;
+	ar->max_allowed_tx_power = vdev_start_resp.max_allowed_tx_power;
+
+	status = vdev_start_resp.status;
+
+	if (WARN_ON_ONCE(status)) {
+		ath12k_warn(ab, "vdev start resp error status %d (%s)\n",
+			    status, ath12k_wmi_vdev_resp_print(status));
+		ar->last_wmi_vdev_start_status = status;
+	}
+
+	complete(&ar->vdev_setup_done);
+
+	rcu_read_unlock();
+
+	ath12k_dbg(ab, ATH12K_DBG_WMI, "vdev start resp for vdev id %d",
+		   vdev_start_resp.vdev_id);
+}
+
+static void ath12k_bcn_tx_status_event(struct ath12k_base *ab, struct sk_buff *skb)
+{
+	struct ath12k_link_vif *arvif;
+	u32 vdev_id, tx_status;
+
+	if (ath12k_pull_bcn_tx_status_ev(ab, skb->data, skb->len,
+					 &vdev_id, &tx_status) != 0) {
+		ath12k_warn(ab, "failed to extract bcn tx status");
+		return;
+	}
+
+	rcu_read_lock();
+	arvif = ath12k_mac_get_arvif_by_vdev_id(ab, vdev_id);
+	if (!arvif) {
+		ath12k_warn(ab, "invalid vdev id %d in bcn_tx_status",
+			    vdev_id);
+		rcu_read_unlock();
+		return;
+	}
+	ath12k_mac_bcn_tx_event(arvif);
+	rcu_read_unlock();
+}
+
+static void ath12k_vdev_stopped_event(struct ath12k_base *ab, struct sk_buff *skb)
+{
+	struct ath12k *ar;
+	u32 vdev_id = 0;
+
+	if (ath12k_pull_vdev_stopped_param_tlv(ab, skb, &vdev_id) != 0) {
+		ath12k_warn(ab, "failed to extract vdev stopped event");
+		return;
+	}
+
+	rcu_read_lock();
+	ar = ath12k_mac_get_ar_by_vdev_id(ab, vdev_id);
+	if (!ar) {
+		ath12k_warn(ab, "invalid vdev id in vdev stopped ev %d",
+			    vdev_id);
+		rcu_read_unlock();
+		return;
+	}
+
+	complete(&ar->vdev_setup_done);
+
+	rcu_read_unlock();
+
+	ath12k_dbg(ab, ATH12K_DBG_WMI, "vdev stopped for vdev id %d", vdev_id);
+}
+
+/**
+ * ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte() - Compare given mgmt packet counters
+ * @ctr1: Management packet counter1
+ * @ctr2: Management packet counter2
+ *
+ * We can't directly use the comparison operator here because the counters can
+ * overflow. But these counters have a property that the difference between
+ * them can never be greater than half the range of the data type.
+ * We can make use of this condition to detect which one is actually greater.
+ *
+ * Return: true if @ctr1 is greater than or equal to @ctr2, else false
+ */
+static inline bool
+ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte(u16 ctr1, u16 ctr2)
+{
+	u16 delta = ctr1 - ctr2;
+
+	return delta <= ATH12K_MGMT_RX_REO_PKT_CTR_HALF_RANGE;
+}
+
+/**
+ * ath12k_mgmt_rx_reo_subtract_pkt_ctrs() - Subtract given mgmt packet counters
+ * @ctr1: Management packet counter1
+ * @ctr2: Management packet counter2
+ *
+ * We can't directly use the subtract operator here because the counters can
+ * overflow. But these counters have a property that the difference between
+ * them can never be greater than half the range of the data type.
+ * We can make use of this condition to detect whichone is actually greater and
+ * return the difference accordingly.
+ *
+ * Return: Difference between @ctr1 and @crt2
+ */
+static inline int
+ath12k_mgmt_rx_reo_subtract_pkt_ctrs(u16 ctr1, u16 ctr2)
+{
+	u16 delta = ctr1 - ctr2;
+
+	/**
+	 * if delta is greater than half the range (i.e, ctr1 is actually
+	 * smaller than ctr2), then the result should be a negative number.
+	 * subtracting the entire range should give the correct value.
+	 */
+	if (delta > ATH12K_MGMT_RX_REO_PKT_CTR_HALF_RANGE)
+		return delta - ATH12K_MGMT_RX_REO_PKT_CTR_FULL_RANGE;
+
+	return delta;
+}
+
+#define ATH12K_MGMT_RX_REO_GLOBAL_TS_HALF_RANGE (0x80000000)
+/**
+ * ath12k_mgmt_rx_reo_compare_global_timestamps_gte()-Compare given global timestamps
+ * @ts1: Global timestamp1
+ * @ts2: Global timestamp2
+ *
+ * We can't directly use the comparison operator here because the timestamps can
+ * overflow. But these timestamps have a property that the difference between
+ * them can never be greater than half the range of the data type.
+ * We can make use of this condition to detect which one is actually greater.
+ *
+ * Return: true if @ts1 is greater than or equal to @ts2, else false
+ */
+static inline bool
+ath12k_mgmt_rx_reo_compare_global_timestamps_gte(u32 ts1, u32 ts2)
+{
+	u32 delta = ts1 - ts2;
+
+	return delta <= ATH12K_MGMT_RX_REO_GLOBAL_TS_HALF_RANGE;
+}
+
+#define ATH12K_RX_REO_REORD_MAX_DELTA	0xFFFF
+/**
+ * ath12k_wlan_mgmt_rx_reo_update_host_snapshot() - Update Host snapshot with the MGMT
+ * Rx REO parameters.
+ * @desc: pointer to frame descriptor
+ *
+ * Return: 0 on Success, Error value on failure of operation
+ */
+static int
+ath12k_wlan_mgmt_rx_reo_update_host_snapshot(struct ath12k *ar,
+					     struct ath12k_mgmt_rx_reo_frame_descriptor *desc)
+{
+	struct ath12k_base *ab = ar->ab;
+	struct ath12k_mgmt_rx_reo_pdev_info *rx_reo_pdev_ctx;
+	struct ath12k_mgmt_rx_reo_snapshot_params *host_ss;
+	struct ath12k_mgmt_rx_reo_params *reo_params;
+	int pkt_ctr_delta;
+
+	if (!desc) {
+		ath12k_err(ab, "Mgmt Rx REO frame descriptor null\n");
+		return -EINVAL;
+	}
+
+	if (!desc->rx_params) {
+		ath12k_err(ab, "Mgmt Rx params NULL\n");
+		return -EINVAL;
+	}
+
+	reo_params = &desc->rx_params->reo_params;
+
+	rx_reo_pdev_ctx = &ar->rx_reo_pdev_ctx;
+	if (!rx_reo_pdev_ctx) {
+		ath12k_err(ab, "Mgmt Rx REO context empty for pdev\n");
+		return -EINVAL;
+	}
+
+	/* FW should send valid REO parameters */
+	if (!reo_params->valid) {
+		ath12k_err(ab, "Mgmt Rx REO params is invalid\n");
+		return -EINVAL;
+	}
+
+	host_ss = &rx_reo_pdev_ctx->host_snapshot;
+
+	if (!host_ss->valid) {
+		desc->pkt_ctr_delta = 1;
+		goto update_host_ss;
+	}
+
+	if (ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte(host_ss->mgmt_pkt_ctr,
+						    reo_params->mgmt_pkt_ctr)) {
+		ath12k_err(ab, "Cur frame ctr < last frame ctr for link = %u",
+			   reo_params->link_id);
+		goto failure_debug;
+	}
+
+	pkt_ctr_delta = ath12k_mgmt_rx_reo_subtract_pkt_ctrs(reo_params->mgmt_pkt_ctr,
+							     host_ss->mgmt_pkt_ctr);
+	WARN_ON(!(pkt_ctr_delta > 0));
+	desc->pkt_ctr_delta = pkt_ctr_delta;
+
+	if (pkt_ctr_delta == 1)
+		goto update_host_ss;
+
+	/* Under back pressure scenarios, FW may drop management Rx frame
+	 * WMI events. So holes in the management packet counter is expected.
+	 * Add a debug print and optional assert to track the holes.
+	 */
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "---- Rx reo reordering(this info is seen since pkt_ctr_delta > 0) ----\n");
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "pkt_ctr_delta = %u\n", pkt_ctr_delta);
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "Cur frame valid = %u, pkt_ctr = %u, ts =%u\n",
+		   reo_params->valid, reo_params->mgmt_pkt_ctr,
+			reo_params->global_timestamp);
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "Last frame valid = %u, pkt_ctr = %u, ts =%u\n",
+		   host_ss->valid, host_ss->mgmt_pkt_ctr,
+			host_ss->global_timestamp);
+
+	if (pkt_ctr_delta > ATH12K_RX_REO_REORD_MAX_DELTA) {
+		ath12k_err(ab, "pkt ctr delta %u > thresh %u for link %u",
+			   pkt_ctr_delta, ATH12K_RX_REO_REORD_MAX_DELTA,
+				reo_params->link_id);
+		goto failure_debug;
+	}
+
+update_host_ss:
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "Last frame valid = %u, pkt_ctr = %u, ts =%u\n",
+		   host_ss->valid, host_ss->mgmt_pkt_ctr,
+			host_ss->global_timestamp);
+
+	host_ss->valid = true;
+	host_ss->global_timestamp = reo_params->global_timestamp;
+	host_ss->mgmt_pkt_ctr = reo_params->mgmt_pkt_ctr;
+
+	return 0;
+
+failure_debug:
+	ath12k_err(ab, "Cur frame valid = %u, pkt_ctr = %u, ts =%u\n",
+		   reo_params->valid, reo_params->mgmt_pkt_ctr,
+			reo_params->global_timestamp);
+	ath12k_err(ab, "Last frame valid = %u, pkt_ctr = %u, ts =%u\n",
+		   host_ss->valid, host_ss->mgmt_pkt_ctr,
+			host_ss->global_timestamp);
+	WARN_ON(1);
+
+	return -EINVAL;
+}
+
+/**
+ * ath12k_wmi_mgmt_rx_reo_read_snapshot_raw() - Read raw value of management
+ * rx-reorder snapshot
+ * @snapshot_address: snapshot address
+ * @ath12k_mgmt_rx_reo_snapshot_low: Pointer to lower 32 bits of snapshot value
+ * @ath12k_mgmt_rx_reo_snapshot_high: Pointer to higher 32 bits of snapshot value
+ * @snapshot_version: snapshot version
+ *
+ * Read raw value of management rx-reorder snapshots.
+ *
+ * Return: 0 on Success, Error value on failure
+ */
+static int
+ath12k_wmi_mgmt_rx_reo_read_snapshot_raw
+			(struct ath12k_mgmt_rx_reo_shared_snapshot *snapshot_address,
+			 u32 *ath12k_mgmt_rx_reo_snapshot_low,
+			 u32 *ath12k_mgmt_rx_reo_snapshot_high,
+			 u8 snapshot_version,
+			 struct ath12k_mgmt_rx_reo_shared_snapshot *raw_snapshot)
+{
+	u32 prev_snapshot_low;
+	u32 prev_snapshot_high;
+	u32 cur_snapshot_low;
+	u32 cur_snapshot_high;
+	u8 retry_count = 0;
+
+	if (snapshot_version == 1) {
+		*ath12k_mgmt_rx_reo_snapshot_low =
+			snapshot_address->u_low.ath12k_mgmt_rx_reo_snapshot_low;
+		*ath12k_mgmt_rx_reo_snapshot_high =
+			snapshot_address->u_high.ath12k_mgmt_rx_reo_snapshot_high;
+		raw_snapshot->u_low.ath12k_mgmt_rx_reo_snapshot_low =
+			*ath12k_mgmt_rx_reo_snapshot_low;
+		raw_snapshot->u_high.ath12k_mgmt_rx_reo_snapshot_high =
+			*ath12k_mgmt_rx_reo_snapshot_high;
+		return 0;
+	}
+
+	prev_snapshot_low = snapshot_address->u_low.ath12k_mgmt_rx_reo_snapshot_low;
+	prev_snapshot_high = snapshot_address->u_high.ath12k_mgmt_rx_reo_snapshot_high;
+	raw_snapshot->u_low.ath12k_mgmt_rx_reo_snapshot_low = prev_snapshot_low;
+	raw_snapshot->u_high.ath12k_mgmt_rx_reo_snapshot_high = prev_snapshot_high;
+
+	for (; retry_count < (ATH12K_MGMT_RX_REO_SNAPSHOT_B2B_READ_SWAR_RETRY_LIMIT - 1);
+			retry_count++) {
+		cur_snapshot_low = snapshot_address->u_low.ath12k_mgmt_rx_reo_snapshot_low;
+		cur_snapshot_high = snapshot_address->u_high.ath12k_mgmt_rx_reo_snapshot_high;
+
+		raw_snapshot[retry_count + 1].u_low.ath12k_mgmt_rx_reo_snapshot_low =
+			cur_snapshot_low;
+		raw_snapshot[retry_count + 1].u_high.ath12k_mgmt_rx_reo_snapshot_high =
+			cur_snapshot_high;
+
+		if (prev_snapshot_low == cur_snapshot_low &&
+		    prev_snapshot_high == cur_snapshot_high)
+			break;
+
+		prev_snapshot_low = cur_snapshot_low;
+		prev_snapshot_high = cur_snapshot_high;
+	}
+
+	WARN_ON(retry_count ==
+			(ATH12K_MGMT_RX_REO_SNAPSHOT_B2B_READ_SWAR_RETRY_LIMIT - 1));
+
+	*ath12k_mgmt_rx_reo_snapshot_low = cur_snapshot_low;
+	*ath12k_mgmt_rx_reo_snapshot_high = cur_snapshot_high;
+
+	return 0;
+}
+
+/**
+ * Helper macros/functions for params GET/SET of different hw version
+ * of the mgmt_rx_reo_snapshot
+ */
+
+static inline u8
+ath12k_mlo_shmem_mgmt_rx_reo_snapshot_valid_get(struct ath12k_base *ab,
+						u32 ath12k_mgmt_rx_reo_snapshot_low,
+						u8 snapshot_ver)
+{
+	if ((snapshot_ver != ATH12K_MGMT_RX_REO_SNAPSHOT_VERSION_TIMESTAMP_REDUNDANCY) &&
+	    (snapshot_ver != ATH12K_MGMT_RX_REO_SNAPSHOT_VERSION_PKT_CTR_REDUNDANCY)) {
+		ath12k_err(ab, "Check this error snapshot ver %d\n", snapshot_ver);
+		WARN_ON(1);
+	}
+
+	return ATH12K_MLO_SHMEM_GET_BITS(ath12k_mgmt_rx_reo_snapshot_low, 31, 1);
+}
+
+static inline u32
+ath12k_mlo_shmem_mgmt_rx_reo_snapshot_global_timestamp_get(
+		struct ath12k_base *ab,
+	u32 ath12k_mgmt_rx_reo_snapshot_low,
+	u32 ath12k_mgmt_rx_reo_snapshot_high,
+	u8 snapshot_ver)
+{
+	if (snapshot_ver == ATH12K_MGMT_RX_REO_SNAPSHOT_VERSION_TIMESTAMP_REDUNDANCY) {
+		return ath12k_mgmt_rx_reo_snapshot_high;
+	} else if (snapshot_ver == ATH12K_MGMT_RX_REO_SNAPSHOT_VERSION_PKT_CTR_REDUNDANCY) {
+		return
+			((ATH12K_MLO_SHMEM_GET_BITS(ath12k_mgmt_rx_reo_snapshot_high, 15, 17) << 15) |
+			 ATH12K_MLO_SHMEM_GET_BITS(ath12k_mgmt_rx_reo_snapshot_low, 0, 15));
+	} else {
+		ath12k_err(ab, "Check this error snapshot ver %d\n", snapshot_ver);
+		WARN_ON(1);
+		return 0;
+	}
+}
+
+static inline u16
+mlo_shmem_mgmt_rx_reo_snapshot_mgmt_pkt_ctr_get(
+		struct ath12k_base *ab,
+	u32 ath12k_mgmt_rx_reo_snapshot_low, u8 snapshot_ver)
+{
+	if (snapshot_ver == ATH12K_MGMT_RX_REO_SNAPSHOT_VERSION_TIMESTAMP_REDUNDANCY) {
+		return ATH12K_MLO_SHMEM_GET_BITS(ath12k_mgmt_rx_reo_snapshot_low, 0, 16);
+	} else if (snapshot_ver == ATH12K_MGMT_RX_REO_SNAPSHOT_VERSION_PKT_CTR_REDUNDANCY) {
+		return ATH12K_MLO_SHMEM_GET_BITS(ath12k_mgmt_rx_reo_snapshot_low, 15, 16);
+	} else {
+		ath12k_err(ab, "Check this error snapshot ver %d\n", snapshot_ver);
+		WARN_ON(1);
+		return 0;
+	}
+}
+
+#define ATH12K_MLO_SHMEM_MGMT_RX_REO_SNAPSHOT_MGMT_PKT_CTR_REDUNDANT_GET( \
+	ath12k_mgmt_rx_reo_snapshot_high) \
+	ATH12K_MLO_SHMEM_GET_BITS(ath12k_mgmt_rx_reo_snapshot_high, 0, 15)
+
+#define ATH12K_MLO_SHMEM_MGMT_RX_REO_SNAPSHOT_GLOBAL_TIMESTAMP_REDUNDANT_GET( \
+	ath12k_mgmt_rx_reo_snapshot_low) \
+	ATH12K_MLO_SHMEM_GET_BITS(ath12k_mgmt_rx_reo_snapshot_low, 16, 15)
+
+static inline bool
+ath12k_mlo_shmem_mgmt_rx_reo_snapshot_check_consistency(
+		struct ath12k_base *ab,
+		u32 ath12k_mgmt_rx_reo_snapshot_low,
+		u32 ath12k_mgmt_rx_reo_snapshot_high,
+		u8 snapshot_ver)
+{
+	if (snapshot_ver == ATH12K_MGMT_RX_REO_SNAPSHOT_VERSION_TIMESTAMP_REDUNDANCY) {
+		u32 global_timestamp;
+		u32 global_timestamp_redundant;
+
+		global_timestamp =
+			ath12k_mlo_shmem_mgmt_rx_reo_snapshot_global_timestamp_get(ab,
+					ath12k_mgmt_rx_reo_snapshot_low,
+					ath12k_mgmt_rx_reo_snapshot_high,
+					snapshot_ver);
+		global_timestamp_redundant =
+			ATH12K_MLO_SHMEM_MGMT_RX_REO_SNAPSHOT_GLOBAL_TIMESTAMP_REDUNDANT_GET(
+					ath12k_mgmt_rx_reo_snapshot_low);
+
+		return
+			(ATH12K_MLO_SHMEM_GET_BITS(global_timestamp, 0, 15) ==
+			 ATH12K_MLO_SHMEM_GET_BITS(global_timestamp_redundant, 0, 15));
+	} else if (snapshot_ver == ATH12K_MGMT_RX_REO_SNAPSHOT_VERSION_PKT_CTR_REDUNDANCY) {
+		u16 mgmt_pkt_ctr;
+		u16 mgmt_pkt_ctr_redundant;
+
+		mgmt_pkt_ctr = mlo_shmem_mgmt_rx_reo_snapshot_mgmt_pkt_ctr_get(ab,
+				ath12k_mgmt_rx_reo_snapshot_low, snapshot_ver);
+		mgmt_pkt_ctr_redundant =
+			ATH12K_MLO_SHMEM_MGMT_RX_REO_SNAPSHOT_MGMT_PKT_CTR_REDUNDANT_GET(
+					ath12k_mgmt_rx_reo_snapshot_high);
+
+		return
+			(ATH12K_MLO_SHMEM_GET_BITS(mgmt_pkt_ctr, 0, 15) ==
+			 ATH12K_MLO_SHMEM_GET_BITS(mgmt_pkt_ctr_redundant, 0, 15));
+	} else {
+		ath12k_err(ab, "Check this error snapshot ver %d\n", snapshot_ver);
+		WARN_ON(1);
+		return 0;
+	}
+}
+
+/**
+ * ath12k_mgmt_rx_reo_snapshot_get_mgmt_pkt_ctr() - Get the management packet counter
+ * from an MGMT Rx REO snapshot
+ * @snapshot_low: lower 32-bits of the snapshot
+ * @snapshot_version: snapshot version
+ *
+ * Return: Management packet counter of the snapshot
+ */
+static u16 ath12k_mgmt_rx_reo_snapshot_get_mgmt_pkt_ctr(
+		struct ath12k_base *ab,
+		u32 mgmt_rx_reo_snapshot_low,
+			      u8 snapshot_version)
+{
+	if (snapshot_version == MGMT_RX_REO_SNAPSHOT_VERSION_TIMESTAMP_REDUNDANCY) {
+		return ATH12K_MLO_SHMEM_GET_BITS(mgmt_rx_reo_snapshot_low, 0, 16);
+	} else if (snapshot_version == MGMT_RX_REO_SNAPSHOT_VERSION_PKT_CTR_REDUNDANCY) {
+		return ATH12K_MLO_SHMEM_GET_BITS(mgmt_rx_reo_snapshot_low, 15, 16);
+	} else {
+		ath12k_err(ab, "Check this error snapshot ver %d\n", snapshot_version);
+		WARN_ON(1);
+		return 0;
+	}
+}
+
+/**
+ * ath12k_wmi_mgmt_rx_reo_read_snapshot() - Read management rx-reorder snapshot
+ * @pdev: pdev pointer
+ * @snapshot_info: Snapshot info
+ * @id: Snapshot ID
+ * @snapshot_value: Pointer to snapshot value
+ *
+ * Read management rx-reorder snapshots from target.
+ *
+ * Return: 0 on Success, Error value on failure
+ */
+static int
+ath12k_wmi_mgmt_rx_reo_read_snapshot(
+			struct ath12k *ar,
+			struct ath12k_mgmt_rx_reo_snapshot_info *snapshot_info,
+			enum ath12k_mgmt_rx_reo_shared_snapshot_id id,
+			struct ath12k_mgmt_rx_reo_snapshot_params *snapshot_value,
+			struct ath12k_mgmt_rx_reo_shared_snapshot (*raw_snapshot)
+			[ATH12K_MGMT_RX_REO_SNAPSHOT_B2B_READ_SWAR_RETRY_LIMIT])
+{
+	struct ath12k_base *ab = ar->ab;
+	bool snapshot_valid;
+	u16 mgmt_pkt_ctr;
+	u32 global_timestamp;
+	u32 ath12k_mgmt_rx_reo_snapshot_low;
+	u32 ath12k_mgmt_rx_reo_snapshot_high;
+	u8 retry_count;
+	int status;
+	struct ath12k_mgmt_rx_reo_shared_snapshot *snapshot_address;
+	u8 snapshot_version;
+
+	if (!snapshot_info) {
+		ath12k_err(ab, "Mgmt Rx REO snapshot info null\n");
+		return -EINVAL;
+	}
+
+	snapshot_address = snapshot_info->address;
+	if (!snapshot_address) {
+		ath12k_err(ab, "Mgmt Rx REO snapshot address null\n");
+		return -EINVAL;
+	}
+
+	snapshot_version = snapshot_info->version;
+
+	if (!snapshot_value) {
+		ath12k_err(ab, "Mgmt Rx REO snapshot null\n");
+		return -EINVAL;
+	}
+
+	memset(snapshot_value, 0, sizeof(*snapshot_value));
+
+	switch (id) {
+	case ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAC_HW:
+	case ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_CONSUMED:
+	case ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_FORWARDED:
+		retry_count = 0;
+		for (; retry_count < ATH12K_MGMT_RX_REO_SNAPSHOT_READ_RETRY_LIMIT;
+				retry_count++) {
+			status = ath12k_wmi_mgmt_rx_reo_read_snapshot_raw
+				(snapshot_address,
+				 &ath12k_mgmt_rx_reo_snapshot_low,
+				 &ath12k_mgmt_rx_reo_snapshot_high,
+				 snapshot_version,
+				 raw_snapshot[retry_count]);
+
+			if (status) {
+				ath12k_err(ab, "Failed to read snapshot %d status %d\n",
+						id, status);
+				return -EINVAL;
+			}
+
+			snapshot_valid = ath12k_mlo_shmem_mgmt_rx_reo_snapshot_valid_get(ab,
+					ath12k_mgmt_rx_reo_snapshot_low,
+					snapshot_version);
+
+			if (!snapshot_valid) {
+				ath12k_dbg(ab, ATH12K_DBG_RX_REO, "Invalid REO snapshot value");
+				snapshot_value->valid = false;
+				snapshot_value->mgmt_pkt_ctr =
+					ath12k_mgmt_rx_reo_snapshot_get_mgmt_pkt_ctr
+					(ab, ath12k_mgmt_rx_reo_snapshot_low,
+					 snapshot_version);
+				snapshot_value->global_timestamp =
+					ath12k_mlo_shmem_mgmt_rx_reo_snapshot_global_timestamp_get
+					(ab, ath12k_mgmt_rx_reo_snapshot_low,
+					 ath12k_mgmt_rx_reo_snapshot_high,
+					 snapshot_version);
+				snapshot_value->retry_count = retry_count + 1;
+				return 0;
+			}
+
+			if (ath12k_mlo_shmem_mgmt_rx_reo_snapshot_check_consistency
+					(ar->ab, ath12k_mgmt_rx_reo_snapshot_low,
+					 ath12k_mgmt_rx_reo_snapshot_high,
+					 snapshot_version)) {
+				global_timestamp =
+					ath12k_mlo_shmem_mgmt_rx_reo_snapshot_global_timestamp_get
+					(ab, ath12k_mgmt_rx_reo_snapshot_low,
+					 ath12k_mgmt_rx_reo_snapshot_high,
+					 snapshot_version);
+				mgmt_pkt_ctr =
+					mlo_shmem_mgmt_rx_reo_snapshot_mgmt_pkt_ctr_get
+					(ab, ath12k_mgmt_rx_reo_snapshot_low,
+					 snapshot_version);
+				break;
+			}
+			ath12k_info(ab, "Inconsistent snapshot %d, version=%u, low=0x%x, high=0x%x, retry=%u\n",
+					id, snapshot_version,
+					ath12k_mgmt_rx_reo_snapshot_low,
+					ath12k_mgmt_rx_reo_snapshot_high,
+					retry_count);
+		}
+
+		if (retry_count == ATH12K_MGMT_RX_REO_SNAPSHOT_READ_RETRY_LIMIT) {
+			ath12k_err(ab, "Read retry limit, id = %d, ver = %u\n",
+					id, snapshot_version);
+			WARN_ON(1);
+			return -EINVAL;
+		}
+
+		snapshot_value->valid = true;
+		snapshot_value->mgmt_pkt_ctr = mgmt_pkt_ctr;
+		snapshot_value->global_timestamp = global_timestamp;
+		snapshot_value->retry_count = retry_count + 1;
+		status = 0;
+		break;
+
+	default:
+		ath12k_err(ab, "Invalid snapshot id %d\n", id);
+		status = -EINVAL;
+		break;
+	}
+
+	return status;
+}
+
+/**
+ * ath12k_mgmt_rx_reo_print_snapshots() - Print all snapshots related
+ * to management Rx reorder module
+ * @mac_hw_ss: MAC HW snapshot
+ * @fw_forwarded_ss: FW forwarded snapshot
+ * @fw_consumed_ss: FW consumed snapshot
+ * @host_ss: Host snapshot
+ *
+ * return: int
+ */
+static int
+ath12k_mgmt_rx_reo_print_snapshots
+	    (struct ath12k_base *ab,
+			 struct ath12k_mgmt_rx_reo_snapshot_params *mac_hw_ss,
+	     struct ath12k_mgmt_rx_reo_snapshot_params *fw_forwarded_ss,
+	     struct ath12k_mgmt_rx_reo_snapshot_params *fw_consumed_ss,
+	     struct ath12k_mgmt_rx_reo_snapshot_params *host_ss)
+{
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "HW SS: valid = %u, ctr = %u, ts = %u\n",
+		   mac_hw_ss->valid, mac_hw_ss->mgmt_pkt_ctr,
+			mac_hw_ss->global_timestamp);
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "FW forwarded SS: valid = %u, ctr = %u, ts = %u\n",
+		   fw_forwarded_ss->valid,
+			fw_forwarded_ss->mgmt_pkt_ctr,
+			fw_forwarded_ss->global_timestamp);
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "FW consumed SS: valid = %u, ctr = %u, ts = %u\n",
+		   fw_consumed_ss->valid,
+			fw_consumed_ss->mgmt_pkt_ctr,
+			fw_consumed_ss->global_timestamp);
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "HOST SS: valid = %u, ctr = %u, ts = %u\n",
+		   host_ss->valid, host_ss->mgmt_pkt_ctr,
+			host_ss->global_timestamp);
+
+	return 0;
+}
+
+/**
+ * ath12k_wmi_mgmt_rx_reo_invalidate_stale_snapshots() - Invalidate stale management
+ * Rx REO snapshots
+ * @mac_hw_ss: MAC HW snapshot
+ * @fw_forwarded_ss: FW forwarded snapshot
+ * @fw_consumed_ss: FW consumed snapshot
+ * @host_ss: Host snapshot
+ * @link: link ID
+ *
+ * return: int
+ */
+static int
+ath12k_wmi_mgmt_rx_reo_invalidate_stale_snapshots
+	    (struct ath12k *ar,
+			 struct ath12k_mgmt_rx_reo_snapshot_params *mac_hw_ss,
+	     struct ath12k_mgmt_rx_reo_snapshot_params *fw_forwarded_ss,
+	     struct ath12k_mgmt_rx_reo_snapshot_params *fw_consumed_ss,
+	     struct ath12k_mgmt_rx_reo_snapshot_params *host_ss,
+	     u8 link)
+{
+	struct ath12k_base *ab = ar->ab;
+
+	if (!mac_hw_ss->valid)
+		return 0;
+
+	if (host_ss->valid) {
+		if (!ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+				(mac_hw_ss->global_timestamp,
+				 host_ss->global_timestamp) ||
+				!ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte
+				(mac_hw_ss->mgmt_pkt_ctr,
+				 host_ss->mgmt_pkt_ctr)) {
+			ath12k_mgmt_rx_reo_print_snapshots(ab, mac_hw_ss, fw_forwarded_ss,
+							   fw_consumed_ss, host_ss);
+			ath12k_dbg(ab, ATH12K_DBG_RX_REO, "Invalidate host snapshot, link %u",
+				   link);
+			host_ss->valid = false;
+		}
+	}
+
+	if (fw_forwarded_ss->valid) {
+		if (!ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+				(mac_hw_ss->global_timestamp,
+				 fw_forwarded_ss->global_timestamp) ||
+				!ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte
+				(mac_hw_ss->mgmt_pkt_ctr,
+				 fw_forwarded_ss->mgmt_pkt_ctr)) {
+			ath12k_mgmt_rx_reo_print_snapshots(ab, mac_hw_ss, fw_forwarded_ss,
+							   fw_consumed_ss, host_ss);
+			ath12k_dbg(ab, ATH12K_DBG_RX_REO, "Invalidate FW forwarded SS, link %u\n",
+				   link);
+			fw_forwarded_ss->valid = false;
+		}
+	}
+
+	if (host_ss->valid && fw_forwarded_ss->valid &&
+	    (ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+			 (host_ss->global_timestamp,
+			  fw_forwarded_ss->global_timestamp) !=
+			 ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte
+			 (host_ss->mgmt_pkt_ctr,
+			  fw_forwarded_ss->mgmt_pkt_ctr))) {
+		ath12k_mgmt_rx_reo_print_snapshots(ab, mac_hw_ss, fw_forwarded_ss,
+						   fw_consumed_ss, host_ss);
+		ath12k_dbg(ab, ATH12K_DBG_RX_REO, "Invalidate FW forwarded SS, link %u",
+			   link);
+		fw_forwarded_ss->valid = false;
+	}
+
+	if (fw_consumed_ss->valid) {
+		if (!ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+				(mac_hw_ss->global_timestamp,
+				 fw_consumed_ss->global_timestamp) ||
+				!ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte
+				(mac_hw_ss->mgmt_pkt_ctr,
+				 fw_consumed_ss->mgmt_pkt_ctr)) {
+			ath12k_mgmt_rx_reo_print_snapshots(ab, mac_hw_ss, fw_forwarded_ss,
+							   fw_consumed_ss, host_ss);
+			ath12k_dbg(ab, ATH12K_DBG_RX_REO, "Invalidate FW consumed SS, link %u\n",
+				   link);
+			fw_consumed_ss->valid = false;
+		}
+		if (host_ss->valid && fw_consumed_ss->valid &&
+		    (ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+				 (host_ss->global_timestamp,
+				  fw_consumed_ss->global_timestamp) !=
+				 ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte
+				 (host_ss->mgmt_pkt_ctr,
+				  fw_consumed_ss->mgmt_pkt_ctr))) {
+			ath12k_mgmt_rx_reo_print_snapshots(ab, mac_hw_ss, fw_forwarded_ss,
+							   fw_consumed_ss, host_ss);
+			ath12k_dbg(ab, ATH12K_DBG_RX_REO, "Invalidate FW consumed SS, link %u",
+				   link);
+			fw_consumed_ss->valid = false;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * mgmt_rx_reo_snapshots_check_sanity() - Check the sanity of management
+ * Rx REO snapshots
+ * @mac_hw_ss: MAC HW snapshot
+ * @fw_forwarded_ss: FW forwarded snapshot
+ * @fw_consumed_ss: FW consumed snapshot
+ * @host_ss: Host snapshot
+ *
+ * return: int
+ */
+static int
+mgmt_rx_reo_snapshots_check_sanity
+			(struct ath12k *ar,
+			struct ath12k_mgmt_rx_reo_snapshot_params *mac_hw_ss,
+			struct ath12k_mgmt_rx_reo_snapshot_params *fw_forwarded_ss,
+			struct ath12k_mgmt_rx_reo_snapshot_params *fw_consumed_ss,
+			struct ath12k_mgmt_rx_reo_snapshot_params *host_ss)
+{
+	int status;
+	struct ath12k_base *ab = ar->ab;
+
+	if (!mac_hw_ss->valid) {
+		if (fw_forwarded_ss->valid || fw_consumed_ss->valid ||
+		    host_ss->valid) {
+			ath12k_err(ab, "MAC HW SS is invalid\n");
+			status = -EINVAL;
+			goto fail;
+		}
+
+		return 0;
+	}
+
+	if (!fw_forwarded_ss->valid && !fw_consumed_ss->valid) {
+		if (host_ss->valid) {
+			ath12k_err(ab, "FW forwarded and consumed SS invalid\n");
+			status = -EINVAL;
+			goto fail;
+		}
+
+		return 0;
+	}
+
+	if (fw_forwarded_ss->valid) {
+		if (!ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+				(mac_hw_ss->global_timestamp,
+				 fw_forwarded_ss->global_timestamp)) {
+			ath12k_err(ab, "TS: MAC HW SS < FW forwarded SS\n");
+			status = -EINVAL;
+			goto fail;
+		}
+
+		if (!ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte
+				(mac_hw_ss->mgmt_pkt_ctr,
+				 fw_forwarded_ss->mgmt_pkt_ctr)) {
+			ath12k_err(ab, "PKT CTR: MAC HW SS < FW forwarded SS\n");
+			status = -EINVAL;
+			goto fail;
+		}
+	}
+
+	if (fw_consumed_ss->valid) {
+		if (!ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+				(mac_hw_ss->global_timestamp,
+				 fw_consumed_ss->global_timestamp)) {
+			ath12k_err(ab, "TS: MAC HW SS < FW consumed SS\n");
+			status = -EINVAL;
+			goto fail;
+		}
+
+		if (!ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte
+				(mac_hw_ss->mgmt_pkt_ctr,
+				 fw_consumed_ss->mgmt_pkt_ctr)) {
+			ath12k_err(ab, "PKT CTR: MAC HW SS < FW consumed SS\n");
+			status = -EINVAL;
+			goto fail;
+		}
+	}
+
+	if (host_ss->valid) {
+		if (!ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+				(mac_hw_ss->global_timestamp,
+				 host_ss->global_timestamp)) {
+			ath12k_err(ab, "TS: MAC HW SS < host SS\n");
+			status = -EINVAL;
+			goto fail;
+		}
+
+		if (!ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte
+				(mac_hw_ss->mgmt_pkt_ctr,
+				 host_ss->mgmt_pkt_ctr)) {
+			ath12k_err(ab, "PKT CTR: MAC HW SS < host SS\n");
+			status = -EINVAL;
+			goto fail;
+		}
+
+		if (fw_forwarded_ss->valid && !fw_consumed_ss->valid) {
+			if (!ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+					(fw_forwarded_ss->global_timestamp,
+					 host_ss->global_timestamp)) {
+				ath12k_err(ab, "TS: FW forwarded < host SS\n");
+				status = -EINVAL;
+				goto fail;
+			}
+
+			if (!ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte
+					(fw_forwarded_ss->mgmt_pkt_ctr,
+					 host_ss->mgmt_pkt_ctr)) {
+				ath12k_err(ab, "CTR: FW forwarded < host SS\n");
+				status = -EINVAL;
+				goto fail;
+			}
+		}
+
+		if (fw_consumed_ss->valid && !fw_forwarded_ss->valid) {
+			if (!ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+					(fw_consumed_ss->global_timestamp,
+					 host_ss->global_timestamp)) {
+				ath12k_err(ab, "TS: FW consumed < host SS\n");
+				status = -EINVAL;
+				goto fail;
+			}
+
+			if (!ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte
+					(fw_consumed_ss->mgmt_pkt_ctr,
+					 host_ss->mgmt_pkt_ctr)) {
+				ath12k_err(ab, "CTR: FW consumed < host SS\n");
+				status = -EINVAL;
+				goto fail;
+			}
+		}
+
+		if (fw_forwarded_ss->valid && fw_consumed_ss->valid) {
+			if (!ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+					(fw_consumed_ss->global_timestamp,
+					 host_ss->global_timestamp) &&
+					!ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+					(fw_forwarded_ss->global_timestamp,
+					 host_ss->global_timestamp)) {
+				ath12k_err(ab, "TS: FW consumed/forwarded < host\n");
+				status = -EINVAL;
+				goto fail;
+			}
+
+			if (!ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte
+					(fw_consumed_ss->mgmt_pkt_ctr,
+					 host_ss->mgmt_pkt_ctr) &&
+					!ath12k_mgmt_rx_reo_compare_pkt_ctrs_gte
+					(fw_forwarded_ss->mgmt_pkt_ctr,
+					 host_ss->mgmt_pkt_ctr)) {
+				ath12k_err(ab, "CTR: FW consumed/forwarded < host\n");
+				status = -EINVAL;
+				goto fail;
+			}
+		}
+	}
+
+	return 0;
+
+fail:
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "HW SS: valid = %u, ctr = %u, ts = %u",
+		   mac_hw_ss->valid, mac_hw_ss->mgmt_pkt_ctr,
+			mac_hw_ss->global_timestamp);
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "FW forwarded SS: valid = %u, ctr = %u, ts = %u",
+		   fw_forwarded_ss->valid,
+			fw_forwarded_ss->mgmt_pkt_ctr,
+			fw_forwarded_ss->global_timestamp);
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "FW consumed SS: valid = %u, ctr = %u, ts = %u",
+		   fw_consumed_ss->valid,
+			fw_consumed_ss->mgmt_pkt_ctr,
+			fw_consumed_ss->global_timestamp);
+	ath12k_dbg(ab, ATH12K_DBG_RX_REO, "HOST SS: valid = %u, ctr = %u, ts = %u",
+		   host_ss->valid, host_ss->mgmt_pkt_ctr,
+			host_ss->global_timestamp);
+
+	return status;
+}
+
+/**
+ * ath12k_wmi_mgmt_rx_reorder_process_calculate_wait_count() - Calculates the number of
+ * frames an incoming frame should wait for before it gets delivered.
+ * @in_frame_pdev: pdev on which this frame is received
+ * @desc: frame Descriptor
+ *
+ * Each frame carrys a MGMT pkt number which is local to that link, and a
+ * timestamp which is global across all the links. MAC HW and FW also captures
+ * the same details of the last frame that they have seen. Host also maintains
+ * the details of the last frame it has seen. In total, there are 4 snapshots.
+ * 1. MAC HW snapshot - latest frame seen at MAC HW
+ * 2. FW forwarded snapshot- latest frame forwarded to the Host
+ * 3. FW consumed snapshot - latest frame consumed by the FW
+ * 4. Host/FW consumed snapshot - latest frame seen by the Host
+ * By using all these snapshots, this function tries to compute the wait count
+ * for a given incoming frame on all links.
+ *
+ * Return: 1 on success otherwise -1 on failure
+ */
+static int
+ath12k_wmi_mgmt_rx_reorder_process_calculate_wait_count(
+		struct ath12k *ar,
+		struct ath12k_mgmt_rx_reo_frame_descriptor *desc)
+{
+	int status;
+	u8 hw_link_id;
+	s8 in_frame_link;
+	int frames_pending, delta_fwd_host;
+	u8 snapshot_id;
+	struct ath12k_base *ab = ar->ab;
+	struct ath12k *hw_link;
+	struct ath12k_mgmt_rx_reo_pdev_info *rx_reo_pdev_ctx;
+	struct ath12k_mgmt_rx_reo_pdev_info *in_frame_rx_reo_pdev_ctx;
+	struct ath12k_mgmt_rx_reo_snapshot_info *snapshot_info;
+	struct ath12k_mgmt_rx_reo_snapshot_params snapshot_params
+				[ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX];
+	struct ath12k_mgmt_rx_reo_snapshot_params *mac_hw_ss, *fw_forwarded_ss,
+					    *fw_consumed_ss, *host_ss;
+	struct ath12k_mgmt_rx_reo_params *in_frame_params;
+	struct ath12k_mgmt_rx_reo_wait_count *wait_count;
+
+	in_frame_params = &desc->rx_params->reo_params;
+
+	wait_count = &desc->wait_count;
+
+	/* Get the hw link ID of incoming frame */
+	in_frame_link = ar->pdev->hw_link_id;
+	in_frame_rx_reo_pdev_ctx = &ar->rx_reo_pdev_ctx;
+
+	if (!in_frame_rx_reo_pdev_ctx) {
+		ath12k_err(ab, "Reo context null for incoming frame\n");
+		return -EINVAL;
+	}
+	memset(in_frame_rx_reo_pdev_ctx->raw_snapshots, 0,
+	       sizeof(in_frame_rx_reo_pdev_ctx->raw_snapshots));
+
+	/* Iterate over all the valid hw links */
+	for (hw_link_id = 0; hw_link_id < ATH12K_WMI_MLO_MAX_LINKS; hw_link_id++) {
+		/* No need wait for any frames on an invalid hw_link_id */
+		if (!ab->ag->hw_links[hw_link_id]) {
+			frames_pending = 0;
+			goto update_pending_frames;
+		}
+
+		hw_link = ab->ag->hw_links[hw_link_id];
+
+		/* No need to wait for any frames if the pdev is not found */
+		if (!hw_link) {
+			ath12k_dbg(ab, ATH12K_DBG_RX_REO, "pdev is null for hw_link_id %d\n", hw_link_id);
+			frames_pending = 0;
+			goto update_pending_frames;
+		}
+
+		rx_reo_pdev_ctx = &hw_link->rx_reo_pdev_ctx;
+		if (!rx_reo_pdev_ctx) {
+			ath12k_err(ab, "Mgmt reo context empty for hw_link %pK\n",
+				   hw_link);
+			return -EINVAL;
+		}
+
+		if (!rx_reo_pdev_ctx->init_complete) {
+			ath12k_dbg(ab, ATH12K_DBG_RX_REO, "REO init in progress for hw_link_id %d",
+				   hw_link_id);
+			frames_pending = 0;
+			goto update_pending_frames;
+		}
+
+		host_ss = &rx_reo_pdev_ctx->host_snapshot;
+
+		desc->host_snapshot[hw_link_id] = rx_reo_pdev_ctx->host_snapshot;
+
+		ath12k_dbg(ab, ATH12K_DBG_RX_REO,
+			   "hw_link_id = %u HOST SS: valid = %u, ctr = %u, ts = %u",
+				 hw_link_id, host_ss->valid, host_ss->mgmt_pkt_ctr,
+				 host_ss->global_timestamp);
+
+		snapshot_id = 0;
+		/* Read all the shared snapshots */
+		while (snapshot_id <
+			ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX) {
+			snapshot_info = &rx_reo_pdev_ctx->
+				host_target_shared_snapshot_info[snapshot_id];
+
+			memset(&snapshot_params[snapshot_id], 0,
+			       sizeof(snapshot_params[snapshot_id]));
+
+			status = ath12k_wmi_mgmt_rx_reo_read_snapshot(
+					hw_link, snapshot_info, snapshot_id,
+					&snapshot_params[snapshot_id],
+					in_frame_rx_reo_pdev_ctx->raw_snapshots
+					[hw_link_id][snapshot_id]);
+
+			/* Read operation shouldn't fail */
+			if (status) {
+				ath12k_err(ab, "snapshot(%d) read failed on hw_link_id (%d) status %d\n",
+					   snapshot_id, hw_link_id, status);
+				return status;
+			}
+
+			/* If snapshot is valid, save it in the pdev context */
+			if (snapshot_params[snapshot_id].valid) {
+				rx_reo_pdev_ctx->
+				   last_valid_shared_snapshot[snapshot_id] =
+				   snapshot_params[snapshot_id];
+			}
+			desc->shared_snapshots[hw_link_id][snapshot_id] =
+						snapshot_params[snapshot_id];
+
+			snapshot_id++;
+		}
+
+		mac_hw_ss = &snapshot_params
+				[ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAC_HW];
+		fw_forwarded_ss = &snapshot_params
+				[ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_FORWARDED];
+		fw_consumed_ss = &snapshot_params
+				[ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_CONSUMED];
+
+		status = ath12k_wmi_mgmt_rx_reo_invalidate_stale_snapshots(ar, mac_hw_ss,
+									   fw_forwarded_ss,
+								fw_consumed_ss,
+								host_ss, hw_link_id);
+		if (status) {
+			ath12k_err(ab, "Failed to invalidate SS for hw_link_id %u\n",
+				   hw_link_id);
+			return status;
+		}
+
+		desc->shared_snapshots[hw_link_id][ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAC_HW] =
+								*mac_hw_ss;
+		desc->shared_snapshots[hw_link_id][ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_FORWARDED] =
+								*fw_forwarded_ss;
+		desc->shared_snapshots[hw_link_id][ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_CONSUMED] =
+								*fw_consumed_ss;
+		desc->host_snapshot[hw_link_id] = *host_ss;
+
+		status = mgmt_rx_reo_snapshots_check_sanity
+			(ar, mac_hw_ss, fw_forwarded_ss, fw_consumed_ss, host_ss);
+		if (status) {
+			ath12k_err(ab, "Snapshot sanity for hw_link_id %u failed\n",
+				   hw_link_id);
+			return status;
+		}
+
+		ath12k_dbg(ab, ATH12K_DBG_RX_REO, "hw_link_id = %u HW SS: valid = %u, ctr = %u, ts = %u",
+			   hw_link_id, mac_hw_ss->valid,
+				 mac_hw_ss->mgmt_pkt_ctr,
+				 mac_hw_ss->global_timestamp);
+		ath12k_dbg(ab, ATH12K_DBG_RX_REO, "hw_link_id = %u FW forwarded SS: valid = %u, ctr = %u, ts = %u",
+			   hw_link_id, fw_forwarded_ss->valid,
+				 fw_forwarded_ss->mgmt_pkt_ctr,
+				 fw_forwarded_ss->global_timestamp);
+		ath12k_dbg(ab, ATH12K_DBG_RX_REO, "hw_link_id = %u FW consumed SS: valid = %u, ctr = %u, ts = %u",
+			   hw_link_id, fw_consumed_ss->valid,
+				 fw_consumed_ss->mgmt_pkt_ctr,
+				 fw_consumed_ss->global_timestamp);
+
+		/* No need wait for any frames on the same hw_link_id */
+		if (hw_link_id == in_frame_link) {
+			frames_pending = 0;
+			goto update_pending_frames;
+		}
+
+		/**
+		 * If MAC HW snapshot is invalid, the hw_link_id has not started
+		 * receiving management frames. Set wait count to zero.
+		 */
+		if (!mac_hw_ss->valid) {
+			frames_pending = 0;
+			goto update_pending_frames;
+		}
+
+		/**
+		 * If host snapshot is invalid, wait for MAX number of frames.
+		 * When any frame in this hw_link_id arrives at host, actual wait
+		 * counts will be updated.
+		 */
+		if (!host_ss->valid) {
+			wait_count->per_link_count[hw_link_id] = UINT_MAX;
+			wait_count->total_count += UINT_MAX;
+			goto print_wait_count;
+		}
+
+		/**
+		 * If MAC HW snapshot sequence number and host snapshot
+		 * sequence number are same, all the frames received by
+		 * this hw_link_id are processed by host. No need to wait for
+		 * any frames from this hw_link_id.
+		 */
+		if (!ath12k_mgmt_rx_reo_subtract_pkt_ctrs(mac_hw_ss->mgmt_pkt_ctr,
+							  host_ss->mgmt_pkt_ctr)) {
+			frames_pending = 0;
+			goto update_pending_frames;
+		}
+
+		/**
+		 * Ideally, the incoming frame has to wait for only those frames
+		 * (on other links) which meet all the below criterion.
+		 * 1. Frame's timestamp is less than incoming frame's
+		 * 2. Frame is supposed to be consumed by the Host
+		 * 3. Frame is not yet seen by the Host.
+		 * We may not be able to compute the exact optimal wait count
+		 * because HW/FW provides a limited assist.
+		 * This reorder process tries to get the best estimate of wait
+		 * count by not waiting for those frames where we have a conclusive
+		 * evidence that we don't have to wait for those frames.
+		 */
+
+		/**
+		 * If this link has already seen a frame whose timestamp is
+		 * greater than or equal to incoming frame's timestamp,
+		 * then no need to wait for any frames on this link.
+		 * If the total wait count becomes zero, then the policy on
+		 * whether to deliver such a frame to upper layers is handled
+		 * separately.
+		 */
+		if (ath12k_mgmt_rx_reo_compare_global_timestamps_gte(
+				host_ss->global_timestamp,
+				in_frame_params->global_timestamp)) {
+			frames_pending = 0;
+			goto update_pending_frames;
+		}
+
+		/**
+		 * For starters, we only have to wait for the frames that are
+		 * seen by MAC HW but not yet seen by Host. The frames which
+		 * reach MAC HW later are guaranteed to have a timestamp
+		 * greater than incoming frame's timestamp.
+		 */
+		frames_pending = ath12k_mgmt_rx_reo_subtract_pkt_ctrs(
+					mac_hw_ss->mgmt_pkt_ctr,
+					host_ss->mgmt_pkt_ctr);
+		WARN_ON(!(frames_pending >= 0));
+
+		if (frames_pending &&
+		    ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+					(mac_hw_ss->global_timestamp,
+					 in_frame_params->global_timestamp)) {
+			/**
+			 * Last frame seen at MAC HW has timestamp greater than
+			 * or equal to incoming frame's timestamp. So no need to
+			 * wait for that last frame, but we can't conclusively
+			 * say anything about timestamp of frames before the
+			 * last frame, so try to wait for all of those frames.
+			 */
+			frames_pending--;
+			WARN_ON(!(frames_pending >= 0));
+
+			if (fw_consumed_ss->valid &&
+			    ath12k_mgmt_rx_reo_compare_global_timestamps_gte(
+				fw_consumed_ss->global_timestamp,
+				in_frame_params->global_timestamp)) {
+				/**
+				 * Last frame consumed by the FW has timestamp
+				 * greater than or equal to incoming frame's.
+				 * That means all the frames from
+				 * fw_consumed_ss->mgmt_pkt_ctr to
+				 * mac_hw->mgmt_pkt_ctr will have timestamp
+				 * greater than or equal to incoming frame's and
+				 * hence, no need to wait for those frames.
+				 * We just need to wait for frames from
+				 * host_ss->mgmt_pkt_ctr to
+				 * fw_consumed_ss->mgmt_pkt_ctr-1. This is a
+				 * better estimate over the above estimate,
+				 * so update frames_pending.
+				 */
+				frames_pending =
+				  ath12k_mgmt_rx_reo_subtract_pkt_ctrs(
+				      fw_consumed_ss->mgmt_pkt_ctr,
+				      host_ss->mgmt_pkt_ctr) - 1;
+
+				WARN_ON(!(frames_pending >= 0));
+
+				/**
+				 * Last frame forwarded to Host has timestamp
+				 * less than incoming frame's. That means all
+				 * the frames starting from
+				 * fw_forwarded_ss->mgmt_pkt_ctr+1 to
+				 * fw_consumed_ss->mgmt_pkt_ctr are consumed by
+				 * the FW and hence, no need to wait for those
+				 * frames. We just need to wait for frames
+				 * from host_ss->mgmt_pkt_ctr to
+				 * fw_forwarded_ss->mgmt_pkt_ctr. This is a
+				 * better estimate over the above estimate,
+				 * so update frames_pending.
+				 */
+				if (fw_forwarded_ss->valid &&
+				    !ath12k_mgmt_rx_reo_compare_global_timestamps_gte(
+					fw_forwarded_ss->global_timestamp,
+					in_frame_params->global_timestamp)) {
+					frames_pending =
+					  ath12k_mgmt_rx_reo_subtract_pkt_ctrs(
+					      fw_forwarded_ss->mgmt_pkt_ctr,
+					      host_ss->mgmt_pkt_ctr);
+
+					/**
+					 * frames_pending can be negative in
+					 * cases whene there are no frames
+					 * getting forwarded to the Host. No
+					 * need to wait for any frames in that
+					 * case.
+					 */
+					if (frames_pending < 0)
+						frames_pending = 0;
+				}
+			}
+
+			/**
+			 * Last frame forwarded to Host has timestamp greater
+			 * than or equal to incoming frame's. That means all the
+			 * frames from fw_forwarded->mgmt_pkt_ctr to
+			 * mac_hw->mgmt_pkt_ctr will have timestamp greater than
+			 * or equal to incoming frame's and hence, no need to
+			 * wait for those frames. We may have to just wait for
+			 * frames from host_ss->mgmt_pkt_ctr to
+			 * fw_forwarded_ss->mgmt_pkt_ctr-1
+			 */
+			if (fw_forwarded_ss->valid &&
+			    ath12k_mgmt_rx_reo_compare_global_timestamps_gte(
+				fw_forwarded_ss->global_timestamp,
+				in_frame_params->global_timestamp)) {
+				delta_fwd_host =
+				  ath12k_mgmt_rx_reo_subtract_pkt_ctrs(
+				    fw_forwarded_ss->mgmt_pkt_ctr,
+				    host_ss->mgmt_pkt_ctr) - 1;
+
+				WARN_ON(!(delta_fwd_host >= 0));
+
+				/**
+				 * This will be a better estimate over the one
+				 * we computed using mac_hw_ss but this may or
+				 * may not be a better estimate over the
+				 * one we computed using fw_consumed_ss.
+				 * When timestamps of both fw_consumed_ss and
+				 * fw_forwarded_ss are greater than incoming
+				 * frame's but timestamp of fw_consumed_ss is
+				 * smaller than fw_forwarded_ss, then
+				 * frames_pending will be smaller than
+				 * delta_fwd_host, the reverse will be true in
+				 * other cases. Instead of checking for all
+				 * those cases, just waiting for the minimum
+				 * among these two should be sufficient.
+				 */
+				frames_pending = min(frames_pending,
+						     delta_fwd_host);
+				WARN_ON(!(frames_pending >= 0));
+			}
+		}
+
+update_pending_frames:
+		WARN_ON(!(frames_pending >= 0));
+
+		wait_count->per_link_count[hw_link_id] = frames_pending;
+		wait_count->total_count += frames_pending;
+
+print_wait_count:
+		ath12k_dbg(ab, ATH12K_DBG_RX_REO,
+			   "hw_link_id = %u wait count: per link = 0x%x, total = 0x%llx",
+			   hw_link_id, wait_count->per_link_count[hw_link_id],
+			   wait_count->total_count);
+	}
+	return 0;
+}
+
+/**
+ * mgmt_rx_reo_prepare_list_entry() - Prepare a list entry from the management
+ * frame received.
+ * @frame_desc: Pointer to the frame descriptor
+ * @entry: Pointer to the list entry
+ *
+ * This API prepares the reorder list entry corresponding to a management frame
+ * to be consumed by host. This entry would be inserted at the appropriate
+ * position in the reorder list.
+ *
+ * Return: 0 on success, non-zero on failure
+ */
+static int
+ath12k_mgmt_rx_reo_prepare_list_entry(struct ath12k_base *ab,
+				      const struct ath12k_mgmt_rx_reo_frame_descriptor *frame_desc,
+	struct mgmt_rx_reo_list_entry **entry)
+{
+	struct mgmt_rx_reo_list_entry *list_entry;
+	struct ath12k_mgmt_rx_reo_params *reo_params =
+				&frame_desc->rx_params->reo_params;
+	struct ath12k *ar;
+	u8 link_id;
+
+	link_id = reo_params->link_id;
+
+	if (link_id >= ATH12K_GROUP_MAX_RADIO) {
+		ath12k_warn(ab, "%s:invalid hw link id %d\n", __func__, link_id);
+		return -EINVAL;
+	}
+
+	ar = rcu_dereference(ab->ag->hw_links[link_id]);
+	if (!ar) {
+		ath12k_err(ab, "ar corresponding to link %u is null\n",
+			   link_id);
+		return -EINVAL;
+	}
+
+	list_entry = kmalloc(sizeof(*list_entry), GFP_ATOMIC);
+	if (!list_entry)
+		return -ENOMEM;
+
+	list_entry->ar = ar;
+	list_entry->nbuf = frame_desc->nbuf;
+	list_entry->rx_params = frame_desc->rx_params;
+	list_entry->wait_count = frame_desc->wait_count;
+	list_entry->initial_wait_count = frame_desc->wait_count;
+	memcpy(list_entry->shared_snapshots, frame_desc->shared_snapshots,
+	       min(sizeof(list_entry->shared_snapshots),
+		   sizeof(frame_desc->shared_snapshots)));
+	memcpy(list_entry->host_snapshot, frame_desc->host_snapshot,
+	       min(sizeof(list_entry->host_snapshot),
+		   sizeof(frame_desc->host_snapshot)));
+	list_entry->status = 0;
+	if (list_entry->wait_count.total_count)
+		list_entry->status |=
+			ATH12K_MGMT_RX_REO_STATUS_WAIT_FOR_FRAME_ON_OTHER_LINKS;
+
+	*entry = list_entry;
+
+	return 0;
+}
+
+/**
+ * ath12k_mgmt_rx_reo_is_stale_frame()- API to check whether the given management frame
+ * is stale
+ * @ts_last_released_frame: pointer to global time stamp of the last frame
+ * removed from the reorder list
+ * @frame_desc: pointer to frame descriptor
+ *
+ * This API checks whether the current management frame under processing is
+ * stale. Any frame older than the last frame delivered to upper layer is a
+ * stale frame. This could happen when we have to deliver frames out of order
+ * due to time out or list size limit. The frames which arrive late at host and
+ * with time stamp lesser than the last delivered frame are stale frames and
+ * they need to be handled differently.
+ *
+ * Return: 0 on success, non-zero on failure. On success "is_stale" and
+ * "is_parallel_rx" members of
+ * @frame_desc will be filled with proper values.
+ */
+static int
+ath12k_mgmt_rx_reo_is_stale_frame(struct ath12k_base *ab,
+				  struct ath12k_mgmt_rx_reo_global_ts_info *ts_last_released_frame,
+	struct ath12k_mgmt_rx_reo_frame_descriptor *frame_desc)
+{
+	struct ath12k_mgmt_rx_reo_params *reo_params
+				= &frame_desc->rx_params->reo_params;
+	u32 cur_frame_start_ts;
+	u32 cur_frame_end_ts;
+
+	if (!ts_last_released_frame) {
+		ath12k_err(ab, "Last released frame time stamp info is null\n");
+		return -EINVAL;
+	}
+
+	if (!frame_desc) {
+		ath12k_err(ab, "Frame descriptor is null\n");
+		return -EINVAL;
+	}
+
+	frame_desc->is_stale = false;
+	frame_desc->is_parallel_rx = false;
+
+	if (!ts_last_released_frame->valid)
+		return 0;
+
+	cur_frame_start_ts = reo_params->start_timestamp;
+	cur_frame_end_ts = reo_params->end_timestamp;
+
+	frame_desc->is_stale =
+		!ath12k_mgmt_rx_reo_compare_global_timestamps_gte(cur_frame_start_ts,
+				ts_last_released_frame->start_ts);
+
+	if (ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+			(ts_last_released_frame->start_ts, cur_frame_start_ts) &&
+			ath12k_mgmt_rx_reo_compare_global_timestamps_gte
+			(cur_frame_end_ts, ts_last_released_frame->end_ts)) {
+		frame_desc->is_parallel_rx = true;
+		frame_desc->is_stale = false;
+	}
+
+	return 0;
+}
+
+/**
+ * ath12k_mgmt_rx_reo_update_wait_count() - Update the wait count for a frame based
+ * on the wait count of a frame received after that on air.
+ * @wait_count_old_frame: Pointer to the wait count structure for the old frame.
+ * @wait_count_new_frame: Pointer to the wait count structure for the new frame.
+ *
+ * This API optimizes the wait count of a frame based on the wait count of
+ * a frame received after that on air. Old frame refers to the frame received
+ * first on the air and new frame refers to the frame received after that.
+ * We use the following fundamental idea. Wait counts for old frames can't be
+ * more than wait counts for the new frame. Use this to optimize the wait count
+ * for the old frames. Per link wait count of an old frame is minimum of the
+ * per link wait count of the old frame and new frame.
+ *
+ * Return: 0 on success, non-zero on failure
+ */
+static int
+ath12k_mgmt_rx_reo_update_wait_count(
+	struct ath12k_mgmt_rx_reo_wait_count *wait_count_old_frame,
+	const struct ath12k_mgmt_rx_reo_wait_count *wait_count_new_frame)
+{
+	u8 link_id;
+
+	for (link_id = 0; link_id < ATH12K_WMI_MLO_MAX_LINKS; link_id++) {
+		if (wait_count_old_frame->per_link_count[link_id]) {
+			u32 temp_wait_count;
+			u32 wait_count_diff;
+
+			temp_wait_count =
+				wait_count_old_frame->per_link_count[link_id];
+			wait_count_old_frame->per_link_count[link_id] =
+				min(wait_count_old_frame->
+						per_link_count[link_id],
+						wait_count_new_frame->
+						per_link_count[link_id]);
+			wait_count_diff = temp_wait_count -
+				wait_count_old_frame->per_link_count[link_id];
+
+			wait_count_old_frame->total_count -= wait_count_diff;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * ath12k_mgmt_rx_reo_update_list() - Modify the reorder list when a frame is received
+ * @reo_list: Pointer to reorder list
+ * @frame_desc: Pointer to frame descriptor
+ * @is_queued: Whether this frame is queued in the REO list
+ *
+ * API to update the reorder list on every management frame reception.
+ * This API does the following things.
+ *   a) Update the wait counts for all the frames in the reorder list with
+ *      global time stamp <= current frame's global time stamp. We use the
+ *      following principle for updating the wait count in this case.
+ *      Let A and B be two management frames with global time stamp of A <=
+ *      global time stamp of B. Let WAi and WBi be the wait count of A and B
+ *      for link i, then WAi <= WBi. Hence we can optimize WAi as
+ *      min(WAi, WBi).
+ *   b) If the current frame is to be consumed by host, insert it in the
+ *      reorder list such that the list is always sorted in the increasing order
+ *      of global time stamp. Update the wait count of the current frame based
+ *      on the frame next to it in the reorder list (if any).
+ *   c) Update the wait count of the frames in the reorder list with global
+ *      time stamp > current frame's global time stamp. Let the current frame
+ *      belong to link "l". Then link "l"'s wait count can be reduced by one for
+ *      all the frames in the reorder list with global time stamp > current
+ *      frame's global time stamp.
+ *
+ * Return: 0 on success, non-zero on failure
+ */
+static int
+ath12k_mgmt_rx_reo_update_list(
+			struct ath12k_base *ab,
+			struct mgmt_rx_reo_list *reo_list,
+	    struct ath12k_mgmt_rx_reo_frame_descriptor *frame_desc,
+	    bool *is_queued)
+{
+	struct mgmt_rx_reo_list_entry *cur_entry;
+	struct mgmt_rx_reo_list_entry *least_greater_entry = NULL;
+	bool least_greater_entry_found = false;
+	int status;
+	u32 new_frame_global_ts;
+	struct mgmt_rx_reo_list_entry *new_entry = NULL;
+	u16 list_insertion_pos = 0;
+	struct ath12k_mgmt_rx_reo_params *reo_params;
+
+	if (!is_queued)
+		return -EINVAL;
+
+	*is_queued = false;
+
+	if (!reo_list) {
+		ath12k_err(ab, "Mgmt Rx reo list is null\n");
+		return -EINVAL;
+	}
+
+	if (!frame_desc) {
+		ath12k_err(ab, "Mgmt frame descriptor is null\n");
+		return -EINVAL;
+	}
+
+	if (frame_desc->rx_params) {
+		reo_params = &frame_desc->rx_params->reo_params;
+		new_frame_global_ts = reo_params->global_timestamp;
+	} else {
+		ath12k_err(ab, "null rx_param %p\n", frame_desc->rx_params);
+		return -EINVAL;
+	}
+
+	/* Prepare the list entry before acquiring lock */
+	if (frame_desc->type == ATH12K_MGMT_RX_REO_FRAME_DESC_HOST_CONSUMED_FRAME &&
+	    frame_desc->reo_required) {
+		status = ath12k_mgmt_rx_reo_prepare_list_entry(ab, frame_desc, &new_entry);
+		if (status) {
+			ath12k_err(ab, "Failed to prepare list entry %d\n", status);
+			return -EINVAL;
+		}
+	}
+
+	spin_lock_bh(&reo_list->list_lock);
+
+	frame_desc->list_size_rx = reo_list->count;
+
+	status = ath12k_mgmt_rx_reo_is_stale_frame(ab, &reo_list->ts_last_released_frame,
+						   frame_desc);
+	if (status)
+		goto exit_free_entry;
+
+	list_for_each_entry(cur_entry, &reo_list->list, node) {
+		u32 cur_entry_global_ts;
+		struct ath12k_mgmt_rx_reo_params *cur_reo_params =
+				&cur_entry->rx_params->reo_params;
+
+		cur_entry_global_ts = cur_reo_params->global_timestamp;
+
+		if (!ath12k_mgmt_rx_reo_compare_global_timestamps_gte(
+					new_frame_global_ts, cur_entry_global_ts)) {
+			least_greater_entry = cur_entry;
+			least_greater_entry_found = true;
+			break;
+		}
+
+		/* if it is stale entry at this point and gts seems to be valid
+		 * then the reordering didn't properly happened please check the
+		 * reordering
+		 */
+		WARN_ON(!(!frame_desc->is_stale || cur_entry->is_parallel_rx));
+
+		list_insertion_pos++;
+
+		status = ath12k_mgmt_rx_reo_update_wait_count(
+				&cur_entry->wait_count,
+				&frame_desc->wait_count);
+		if (status)
+			goto exit_free_entry;
+
+		if (cur_entry->wait_count.total_count == 0)
+			cur_entry->status &=
+				~ATH12K_MGMT_RX_REO_STATUS_WAIT_FOR_FRAME_ON_OTHER_LINKS;
+	}
+
+	if (frame_desc->type == ATH12K_MGMT_RX_REO_FRAME_DESC_HOST_CONSUMED_FRAME &&
+	    !frame_desc->is_stale && frame_desc->reo_required) {
+		if (least_greater_entry_found) {
+			status = ath12k_mgmt_rx_reo_update_wait_count(
+					&new_entry->wait_count,
+					&least_greater_entry->wait_count);
+
+			if (status)
+				goto exit_free_entry;
+
+			frame_desc->wait_count = new_entry->wait_count;
+
+			if (new_entry->wait_count.total_count == 0)
+				new_entry->status &=
+					~ATH12K_MGMT_RX_REO_STATUS_WAIT_FOR_FRAME_ON_OTHER_LINKS;
+		}
+
+		new_entry->insertion_ts = ktime_to_us(ktime_get());
+		new_entry->ingress_timestamp = frame_desc->ingress_timestamp;
+		new_entry->is_parallel_rx = frame_desc->is_parallel_rx;
+		frame_desc->list_insertion_pos = list_insertion_pos;
+
+		if (least_greater_entry_found)
+			list_add_tail(&new_entry->node, &least_greater_entry->node);
+		else
+			list_add_tail(&new_entry->node, &reo_list->list);
+
+		reo_list->count++;
+
+		*is_queued = true;
+
+		if (new_entry->wait_count.total_count == 0)
+			frame_desc->zero_wait_count_rx = true;
+
+		if (frame_desc->zero_wait_count_rx &&
+		    list_first_entry_or_null(&reo_list->list,
+					     struct mgmt_rx_reo_list_entry,
+					node) == new_entry)
+			frame_desc->immediate_delivery = true;
+	}
+
+	if (least_greater_entry_found) {
+		cur_entry = least_greater_entry;
+
+		list_for_each_entry_from(cur_entry, &reo_list->list, node) {
+			u8 frame_link_id;
+			struct ath12k_mgmt_rx_reo_wait_count *wait_count;
+
+			frame_link_id = reo_params->link_id;
+			wait_count = &cur_entry->wait_count;
+			if (wait_count->per_link_count[frame_link_id]) {
+				u32 old_wait_count;
+				u32 new_wait_count;
+				u32 wait_count_diff;
+				u16 pkt_ctr_delta;
+
+				pkt_ctr_delta = frame_desc->pkt_ctr_delta;
+				old_wait_count =
+					wait_count->per_link_count[frame_link_id];
+
+				if (old_wait_count >= pkt_ctr_delta)
+					new_wait_count = old_wait_count -
+						pkt_ctr_delta;
+				else
+					new_wait_count = 0;
+
+				wait_count_diff = old_wait_count -
+					new_wait_count;
+
+				wait_count->per_link_count[frame_link_id] =
+					new_wait_count;
+				wait_count->total_count -= wait_count_diff;
+
+				if (wait_count->total_count == 0)
+					cur_entry->status &=
+						~ATH12K_MGMT_RX_REO_STATUS_WAIT_FOR_FRAME_ON_OTHER_LINKS;
+			}
+		}
+	}
+
+	status = 0;
+
+exit_free_entry:
+	/* Cleanup the entry if it is not queued */
+	if (new_entry && !*is_queued) {
+		/**
+		 * New entry created is not inserted to reorder list, free
+		 * the entry and release the reference
+		 */
+		kfree(new_entry);
+	}
+
+	spin_unlock_bh(&reo_list->list_lock);
+
+	return status;
+}
+
+/**
+ * ath12k_mgmt_rx_reo_list_is_ready_to_send_up_entry() - API to check whether the
+ * list entry can be send to upper layers.
+ * @reo_list: Pointer to reorder list
+ * @entry: List entry
+ *
+ * Return: 0 on success, non-zero on failure
+ */
+static bool
+ath12k_mgmt_rx_reo_list_is_ready_to_send_up_entry(struct mgmt_rx_reo_list *reo_list,
+						  struct mgmt_rx_reo_list_entry *entry)
+{
+	if (!reo_list || !entry)
+		return false;
+
+	return (reo_list->count > reo_list->max_list_size) ||
+		!ATH12K_MGMT_RX_REO_LIST_ENTRY_IS_WAITING_FOR_FRAME_ON_OTHER_LINK(
+				entry) || ATH12K_MGMT_RX_REO_LIST_ENTRY_IS_AGED_OUT(entry) ||
+		ATH12K_MGMT_RX_REO_LIST_ENTRY_IS_OLDER_THAN_LATEST_AGED_OUT_FRAME
+		(entry);
+}
+
+/**
+ * ath12k_mgmt_rx_reo_list_entry_get_release_reason() - Helper API to get the reason
+ * for releasing the reorder list entry to upper layer.
+ * reorder list.
+ * @entry: List entry
+ *
+ * This API expects the caller to acquire the spin lock protecting the reorder
+ * list.
+ *
+ * Return: Reason for releasing the frame.
+ */
+static u8
+ath12k_mgmt_rx_reo_list_entry_get_release_reason(struct mgmt_rx_reo_list_entry *entry)
+{
+	u8 release_reason = 0;
+
+	if (!entry)
+		return 0;
+
+	if (ATH12K_MGMT_RX_REO_LIST_ENTRY_IS_MAX_SIZE_EXCEEDED(entry))
+		release_reason |=
+			MGMT_RX_REO_LIST_ENTRY_RELEASE_REASON_LIST_MAX_SIZE_EXCEEDED;
+
+	if (!ATH12K_MGMT_RX_REO_LIST_ENTRY_IS_WAITING_FOR_FRAME_ON_OTHER_LINK(entry))
+		release_reason |=
+			MGMT_RX_REO_LIST_ENTRY_RELEASE_REASON_ZERO_WAIT_COUNT;
+
+	if (ATH12K_MGMT_RX_REO_LIST_ENTRY_IS_AGED_OUT(entry))
+		release_reason |=
+			MGMT_RX_REO_LIST_ENTRY_RELEASE_REASON_AGED_OUT;
+
+	if (ATH12K_MGMT_RX_REO_LIST_ENTRY_IS_OLDER_THAN_LATEST_AGED_OUT_FRAME(entry))
+		release_reason |=
+			MGMT_RX_REO_LIST_ENTRY_RELEASE_REASON_OLDER_THAN_AGED_OUT_FRAME;
+
+	return release_reason;
+}
+
+/**
+ * mgmt_rx_reo_is_potential_premature_delivery() - Helper API to check
+ * whether the current frame getting delivered to upper layer is a premature
+ * delivery
+ * @release_reason: release reason
+ *
+ * Return: true for a premature delivery
+ */
+static bool
+ath12k_mgmt_rx_reo_is_potential_premature_delivery(u8 release_reason)
+{
+	return !(release_reason &
+			MGMT_RX_REO_LIST_ENTRY_RELEASE_REASON_ZERO_WAIT_COUNT);
+}
+
+/**
+ * ath12k_mgmt_rx_reo_list_entry_send_up() - API to send the frame to the upper layer.
+ * @reo_list: Pointer to reorder list
+ * @entry: List entry
+ *
+ * API to send the frame to the upper layer. This API has to be called only
+ * for entries which can be released to upper layer. It is the caller's
+ * responsibility to ensure that entry can be released (by using API
+ * mgmt_rx_reo_list_is_ready_to_send_up_entry). This API is called after
+ * acquiring the lock which serializes the frame delivery to the upper layers.
+ *
+ * Return: 0 on success, non-zero on failure
+ */
+static int
+ath12k_mgmt_rx_reo_list_entry_send_up(struct mgmt_rx_reo_list *reo_list,
+			       struct mgmt_rx_reo_list_entry *entry)
+{
+	struct ath12k *ar = entry->ar;
+	u8 release_reason;
+	u8 link_id;
+	u32 entry_global_ts;
+	int status;
+	struct ath12k_mgmt_rx_reo_context *reo_context;
+	struct ath12k_mgmt_rx_reo_params *reo_params =
+		&entry->rx_params->reo_params;
+
+	if (unlikely(!rcu_access_pointer(ar->ab->pdevs_active[ar->pdev_idx]))) {
+		dev_kfree_skb(entry->nbuf);
+		goto free;
+	}
+
+	reo_context = container_of(reo_list, struct ath12k_mgmt_rx_reo_context,
+				   reo_list);
+
+	link_id = reo_params->link_id;
+	entry_global_ts = reo_params->global_timestamp;
+
+	release_reason = ath12k_mgmt_rx_reo_list_entry_get_release_reason(entry);
+
+	//TODO remove WARN_ON once the implementation is stablized
+	WARN_ON(!release_reason != 0);
+
+	entry->is_delivered = false;
+	entry->is_premature_delivery = false;
+	entry->release_reason = release_reason;
+
+	if (ath12k_mgmt_rx_reo_is_potential_premature_delivery(release_reason))
+		entry->is_premature_delivery = true;
+
+	ath12k_dbg(ar->ab, ATH12K_DBG_RX_REO,
+			"Mgmt Re-order egress: channel %d valid %u global_ts %u pkt_ctr %u is_parallel_rx %d\n",
+			entry->rx_params->channel,
+			entry->rx_params->reo_params.valid,
+			entry->rx_params->reo_params.global_timestamp,
+			entry->rx_params->reo_params.mgmt_pkt_ctr,
+			entry->is_parallel_rx);
+
+	ieee80211_rx_ni(entry->ar->ah->hw, entry->nbuf);
+	/* believing rx stack takes care of skb */
+	entry->is_delivered = true;
+
+free:
+	kfree(entry->rx_params);
+	/*Above ieee80211_rx_ni call frees nbuf and rx_params, make it null explicitly */
+	entry->nbuf = NULL;
+	entry->rx_params = NULL;
+	status = 0;
+
+	return status;
+}
+
+/**
+ * ath12k_mgmt_rx_reo_list_release_entries() - Release entries from the reorder list
+ * @reo_context: Pointer to management Rx reorder context
+ *
+ * This API releases the entries from the reorder list based on the following
+ * conditions.
+ *   a) Entries with total wait count equal to 0
+ *   b) Entries which are timed out or entries with global time stamp <= global
+ *      time stamp of the latest frame which is timed out. We can only release
+ *      the entries in the increasing order of the global time stamp.
+ *      So all the entries with global time stamp <= global time stamp of the
+ *      latest timed out frame has to be released.
+ *
+ * Return: 0 on success, non-zero on failure
+ */
+static int
+ath12k_mgmt_rx_reo_list_release_entries(struct ath12k_base *ab,
+					struct ath12k_mgmt_rx_reo_context *reo_context)
+{
+	struct mgmt_rx_reo_list *reo_list;
+	struct ath12k_mgmt_rx_reo_params *reo_params;
+	int status;
+
+	if (!reo_context) {
+		ath12k_err(ab, "reo context is null\n");
+		return -EINVAL;
+	}
+
+	reo_list = &reo_context->reo_list;
+
+	spin_lock(&reo_context->frame_release_lock);
+
+	while (1) {
+		struct mgmt_rx_reo_list_entry *first_entry;
+		/* TODO yield if release_count > THRESHOLD */
+		u16 release_count = 0;
+		struct ath12k_mgmt_rx_reo_global_ts_info *ts_last_released_frame =
+			&reo_list->ts_last_released_frame;
+		u32 entry_global_ts;
+
+		spin_lock_bh(&reo_list->list_lock);
+
+		first_entry = list_first_entry_or_null(
+				&reo_list->list, struct mgmt_rx_reo_list_entry, node);
+
+		if (!first_entry) {
+			status = 0;
+			goto exit_unlock_list_lock;
+		}
+
+		if (!ath12k_mgmt_rx_reo_list_is_ready_to_send_up_entry(reo_list,
+								       first_entry)) {
+			status = 0;
+			goto exit_unlock_list_lock;
+		}
+
+		if (reo_list->count > reo_list->max_list_size)
+			first_entry->status |=
+				ATH12K_MGMT_RX_REO_STATUS_LIST_MAX_SIZE_EXCEEDED;
+
+		list_del_init(&first_entry->node);
+		reo_list->count--;
+
+		if (status) {
+			status = -EINVAL;
+			goto exit_unlock_list_lock;
+		}
+		first_entry->removal_ts = ktime_to_us(ktime_get());
+
+		/**
+		 * Last released frame global time stamp is invalid means that
+		 * current frame is the first frame to be released to the
+		 * upper layer from the reorder list. Blindly update the last
+		 * released frame global time stamp to the current frame's
+		 * global time stamp and set the valid to true.
+		 * If the last released frame global time stamp is valid and
+		 * current frame's global time stamp is >= last released frame
+		 * global time stamp, deliver the current frame to upper layer
+		 * and update the last released frame global time stamp.
+		 */
+		reo_params = &first_entry->rx_params->reo_params;
+		entry_global_ts = reo_params->global_timestamp;
 
-		if (!(ab->num_radios > 1 && num_mac_addr >= ab->num_radios))
-			break;
+		if (!ts_last_released_frame->valid ||
+		    ath12k_mgmt_rx_reo_compare_global_timestamps_gte(
+					entry_global_ts, ts_last_released_frame->global_ts)) {
+			ts_last_released_frame->global_ts = entry_global_ts;
+			ts_last_released_frame->start_ts =
+				reo_params->start_timestamp;
+			ts_last_released_frame->end_ts =
+				reo_params->end_timestamp;
+			ts_last_released_frame->valid = true;
+			ts_last_released_frame->expiry_time = jiffies + ATH12K_MGMT_RX_REO_GLOBAL_MGMT_RX_INACTIVITY_TIMEOUT;
+
+		} else {
+			/**
+			 * This should never happen. All the frames older than
+			 * the last frame released from the reorder list will be
+			 * discarded at the entry to reorder process itself.
+			 */
+			WARN_ON(!first_entry->is_parallel_rx);
+		}
 
-		for (i = 0; i < ab->num_radios; i++) {
-			pdev = &ab->pdevs[i];
-			ether_addr_copy(pdev->mac_addr, addr_list[i].addr);
+		spin_unlock_bh(&reo_list->list_lock);
+
+		status = ath12k_mgmt_rx_reo_list_entry_send_up(reo_list,
+							first_entry);
+		if (status) {
+			status = -EINVAL;
+			kfree(first_entry);
+			goto exit_unlock_frame_release_lock;
 		}
-		ab->pdevs_macaddr_valid = true;
-		break;
-	default:
-		break;
+
+		kfree(first_entry);
+		release_count++;
 	}
 
-	return 0;
+	status = 0;
+	goto exit_unlock_frame_release_lock;
+
+exit_unlock_list_lock:
+	spin_unlock_bh(&reo_list->list_lock);
+exit_unlock_frame_release_lock:
+	spin_unlock(&reo_context->frame_release_lock);
+
+	return status;
 }
 
-static int ath12k_ready_event(struct ath12k_base *ab, struct sk_buff *skb)
+static int
+ath12k_wmi_mgmt_rx_reorder_process_entry(struct ath12k *ar,
+					 struct ath12k_mgmt_rx_reo_frame_descriptor *desc,
+			    bool *is_queued)
 {
-	struct wmi_tlv_rdy_parse rdy_parse = { };
+	struct ath12k_base *ab = ar->ab;
+	struct ath12k_mgmt_rx_reo_context *reo_ctx;
 	int ret;
 
-	ret = ath12k_wmi_tlv_iter(ab, skb->data, skb->len,
-				  ath12k_wmi_tlv_rdy_parse, &rdy_parse);
-	if (ret) {
-		ath12k_warn(ab, "failed to parse tlv %d\n", ret);
-		return ret;
+	if (!is_queued)
+		return -EINVAL;
+
+	*is_queued = false;
+
+	if (!desc || !desc->rx_params) {
+		ath12k_err(ab, "MGMT Rx REO descriptor or rx params are null\n");
+		return -EINVAL;
 	}
 
-	complete(&ab->wmi_ab.unified_ready);
-	return 0;
-}
+	reo_ctx = &ab->ag->rx_reo;
+	if (!reo_ctx) {
+		ath12k_err(ab, "REO context is NULL\n");
+		return -EINVAL;
+	}
 
-static void ath12k_peer_delete_resp_event(struct ath12k_base *ab, struct sk_buff *skb)
-{
-	struct wmi_peer_delete_resp_event peer_del_resp;
-	struct ath12k *ar;
+	if (!reo_ctx->init_done || !reo_ctx->timer_init_done) {
+		ath12k_err(ab, "MGMT Rx REO timer is not initialised\n");
+		return -EINVAL;
+	}
 
-	if (ath12k_pull_peer_del_resp_ev(ab, skb, &peer_del_resp) != 0) {
-		ath12k_warn(ab, "failed to extract peer delete resp");
-		return;
+	/**
+	 * Critical Section = Host snapshot update + Calculation of wait
+	 * counts + Update reorder list. Following section describes the
+	 * motivation for making this a critical section.
+	 * Lets take an example of 2 links (Link A & B) and each has received
+	 * a management frame A1 and B1 such that MLO global time stamp of A1 <
+	 * MLO global time stamp of B1. Host is concurrently executing
+	 * "ath12k_wmi_mgmt_rx_reorder_process_entry" for A1 and B1 in 2 different CPUs.
+	 *
+	 * A lock less version of this API("ath12k_wmi_mgmt_rx_reorder_process_entry_v1") is
+	 * as follows.
+	 *
+	 * ath12k_wmi_mgmt_rx_reorder_process_entry()
+	 * {
+	 *     Host snapshot update
+	 *     Calculation of wait counts
+	 *     Update reorder list
+	 *     Release to upper layer
+	 * }
+	 *
+	 * We may run into race conditions under the following sequence of
+	 * operations.
+	 *
+	 * 1. Host snapshot update for link A in context of frame A1
+	 * 2. Host snapshot update for link B in context of frame B1
+	 * 3. Calculation of wait count for frame B1
+	 *        link A wait count =  0
+	 *        link B wait count =  0
+	 * 4. Update reorder list with frame B1
+	 * 5. Release B1 to upper layer
+	 * 6. Calculation of wait count for frame A1
+	 *        link A wait count =  0
+	 *        link B wait count =  0
+	 * 7. Update reorder list with frame A1
+	 * 8. Release A1 to upper layer
+	 *
+	 * This leads to incorrect behaviour as B1 goes to upper layer before
+	 * A1.
+	 *
+	 * To prevent this lets make Host snapshot update + Calculate wait count
+	 * a critical section by adding locks. The updated version of the API
+	 * ("ath12k_wmi_mgmt_rx_reorder_process_entry_v2") is as follows.
+	 *
+	 * ath12k_wmi_mgmt_rx_reorder_process_entry()
+	 * {
+	 *     LOCK
+	 *         Host snapshot update
+	 *         Calculation of wait counts
+	 *     UNLOCK
+	 *     Update reorder list
+	 *     Release to upper layer
+	 * }
+	 *
+	 * With this API also We may run into race conditions under the
+	 * following sequence of operations.
+	 *
+	 * 1. Host snapshot update for link A in context of frame A1 +
+	 *    Calculation of wait count for frame A1
+	 *        link A wait count =  0
+	 *        link B wait count =  0
+	 * 2. Host snapshot update for link B in context of frame B1 +
+	 *    Calculation of wait count for frame B1
+	 *        link A wait count =  0
+	 *        link B wait count =  0
+	 * 4. Update reorder list with frame B1
+	 * 5. Release B1 to upper layer
+	 * 7. Update reorder list with frame A1
+	 * 8. Release A1 to upper layer
+	 *
+	 * This also leads to incorrect behaviour as B1 goes to upper layer
+	 * before A1.
+	 *
+	 *
+	 * To prevent this, let's make Host snapshot update + Calculate wait
+	 * count + Update reorder list a critical section by adding locks.
+	 * The updated version of the API ("ath12k_wmi_mgmt_rx_reorder_process_entry_final")
+	 * is as follows.
+	 *
+	 * ath12k_wmi_mgmt_rx_reorder_process_entry()
+	 * {
+	 *     LOCK
+	 *         Host snapshot update
+	 *         Calculation of wait counts
+	 *         Update reorder list
+	 *     UNLOCK
+	 *     Release to upper layer
+	 * }
+	 */
+	spin_lock_bh(&reo_ctx->rx_reorder_entry_lock);
+
+	if (unlikely(!desc->rx_params->reo_params.valid)) {
+		ath12k_warn(ab, "Valid bit is not set for mgmt frame\n");
+		goto failure;
 	}
 
-	rcu_read_lock();
-	ar = ath12k_mac_get_ar_by_vdev_id(ab, peer_del_resp.vdev_id);
-	if (!ar) {
-		ath12k_warn(ab, "invalid vdev id in peer delete resp ev %d",
-			    peer_del_resp.vdev_id);
-		rcu_read_unlock();
-		return;
+	if ((desc->type == ATH12K_MGMT_RX_REO_FRAME_DESC_HOST_CONSUMED_FRAME ||
+	     desc->type == ATH12K_MGMT_RX_REO_FRAME_DESC_FW_CONSUMED_FRAME) &&
+			!desc->rx_params->reo_params.duration_us) {
+		ath12k_warn(ab, "Rx reo duration of the frame is not valid %d\n",
+			    desc->rx_params->reo_params.duration_us);
+		goto failure;
 	}
 
-	complete(&ar->peer_delete_done);
-	rcu_read_unlock();
-	ath12k_dbg(ab, ATH12K_DBG_WMI, "peer delete resp for vdev id %d addr %pM\n",
-		   peer_del_resp.vdev_id, peer_del_resp.peer_macaddr.addr);
+	/* Update the Host snapshot */
+	ret = ath12k_wlan_mgmt_rx_reo_update_host_snapshot(ar, desc);
+	if (ret)
+		goto failure;
+
+	/* Compute wait count for this frame/event */
+	ret = ath12k_wmi_mgmt_rx_reorder_process_calculate_wait_count(ar, desc);
+	if (ret)
+		goto failure;
+
+	/* Update the REO list */
+	ret = ath12k_mgmt_rx_reo_update_list(ab, &reo_ctx->reo_list, desc, is_queued);
+	if (ret)
+		goto failure;
+
+	spin_unlock_bh(&reo_ctx->rx_reorder_entry_lock);
+	/* Finally, release the entries for which pending frame is received */
+	return ath12k_mgmt_rx_reo_list_release_entries(ab, reo_ctx);
+
+failure:
+	spin_unlock_bh(&reo_ctx->rx_reorder_entry_lock);
+	return ret;
 }
 
-static void ath12k_vdev_delete_resp_event(struct ath12k_base *ab,
-					  struct sk_buff *skb)
+/**
+ * ath12k_mgmt_rx_reo_list_ageout_timer_handler() - Periodic ageout timer handler
+ * @arg: Argument to timer handler
+ *
+ * This is the handler for periodic ageout timer used to timeout entries in the
+ * reorder list.
+ *
+ * Return: void
+ */
+static void
+ath12k_mgmt_rx_reo_list_ageout_timer_handler(struct timer_list *timer)
 {
-	struct ath12k *ar;
-	u32 vdev_id = 0;
+	struct mgmt_rx_reo_list *reo_list = from_timer(reo_list, timer, ageout_timer);
+	struct ath12k_base *ab = reo_list->ab;
+	struct mgmt_rx_reo_list_entry *cur_entry;
+	u64 cur_ts;
+	int status;
+	struct mgmt_rx_reo_context *reo_context = &ab->ag->rx_reo;
+	/**
+	 * Stores the pointer to the entry in reorder list for the latest aged
+	 * out frame. Latest aged out frame is the aged out frame in reorder
+	 * list which has the largest global time stamp value.
+	 */
+	struct mgmt_rx_reo_list_entry *latest_aged_out_entry = NULL;
 
-	if (ath12k_pull_vdev_del_resp_ev(ab, skb, &vdev_id) != 0) {
-		ath12k_warn(ab, "failed to extract vdev delete resp");
-		return;
+	spin_lock_bh(&reo_list->list_lock);
+
+	cur_ts = ktime_to_us(ktime_get());
+
+	if (list_empty(&reo_list->list))
+		goto out;
+
+	list_for_each_entry(cur_entry, &reo_list->list, node) {
+		if (cur_ts - cur_entry->insertion_ts >=
+				reo_list->list_entry_timeout_us) {
+			latest_aged_out_entry = cur_entry;
+			cur_entry->status |= ATH12K_MGMT_RX_REO_STATUS_AGED_OUT;
+		}
 	}
 
-	rcu_read_lock();
-	ar = ath12k_mac_get_ar_by_vdev_id(ab, vdev_id);
-	if (!ar) {
-		ath12k_warn(ab, "invalid vdev id in vdev delete resp ev %d",
-			    vdev_id);
-		rcu_read_unlock();
-		return;
+	if (latest_aged_out_entry) {
+		list_for_each_entry(cur_entry, &reo_list->list, node) {
+			if (cur_entry == latest_aged_out_entry)
+				break;
+			cur_entry->status |= ATH12K_MGMT_RX_REO_STATUS_OLDER_THAN_LATEST_AGED_OUT_FRAME;
+		}
 	}
 
-	complete(&ar->vdev_delete_done);
+out:
+	spin_unlock_bh(&reo_list->list_lock);
 
-	rcu_read_unlock();
+	if (latest_aged_out_entry) {
+		status = ath12k_mgmt_rx_reo_list_release_entries(ab, reo_context);
+		if (status) {
+			ath12k_err(ab, "Failed to release entries, ret = %d\n",
+				   status);
+			return;
+		}
+	}
 
-	ath12k_dbg(ab, ATH12K_DBG_WMI, "vdev delete resp for vdev id %d\n",
-		   vdev_id);
+	mod_timer(&reo_list->ageout_timer, jiffies +
+			msecs_to_jiffies(ATH12K_MGMT_RX_REO_AGEOUT_TIMER_PERIOD_MS));
 }
 
-static const char *ath12k_wmi_vdev_resp_print(u32 vdev_resp_status)
+/**
+ * ath12k_mgmt_rx_reo_global_mgmt_rx_inactivity_timer_handler() - Timer handler
+ * for global management Rx inactivity timer
+ * @arg: Argument to timer handler
+ *
+ * This is the timer handler for tracking management Rx inactivity across
+ * links.
+ *
+ * Return: void
+ */
+static void
+ath12k_mgmt_rx_reo_global_mgmt_rx_inactivity_timer_handler(struct timer_list *timer)
 {
-	switch (vdev_resp_status) {
-	case WMI_VDEV_START_RESPONSE_INVALID_VDEVID:
-		return "invalid vdev id";
-	case WMI_VDEV_START_RESPONSE_NOT_SUPPORTED:
-		return "not supported";
-	case WMI_VDEV_START_RESPONSE_DFS_VIOLATION:
-		return "dfs violation";
-	case WMI_VDEV_START_RESPONSE_INVALID_REGDOMAIN:
-		return "invalid regdomain";
-	default:
-		return "unknown";
-	}
+	struct mgmt_rx_reo_list *reo_list = from_timer(reo_list, timer, global_mgmt_rx_inactivity_timer);
+	struct ath12k_base *ab = reo_list->ab;
+	struct ath12k_mgmt_rx_reo_context *reo_context = &ab->ag->rx_reo;
+	struct ath12k_mgmt_rx_reo_global_ts_info *ts_last_released_frame;
+
+	ts_last_released_frame = &reo_list->ts_last_released_frame;
+
+	spin_lock(&reo_context->frame_release_lock);
+	spin_lock_bh(&reo_list->list_lock);
+
+	if (time_after(jiffies, ts_last_released_frame->expiry_time))
+		memset(ts_last_released_frame, 0, sizeof(*ts_last_released_frame));
+
+	spin_unlock_bh(&reo_list->list_lock);
+	spin_unlock(&reo_context->frame_release_lock);
+
+	mod_timer(&reo_list->global_mgmt_rx_inactivity_timer, jiffies +
+		 ATH12K_MGMT_RX_REO_GLOBAL_MGMT_RX_INACTIVITY_TIMEOUT);
 }
 
-static void ath12k_vdev_start_resp_event(struct ath12k_base *ab, struct sk_buff *skb)
+/**
+ * mgmt_rx_reo_list_init() - Initialize the management rx-reorder list
+ * @reo_list: Pointer to reorder list
+ *
+ * API to initialize the management rx-reorder list.
+ *
+ * Return: 0 on success, non-zero on failure
+ */
+static int
+ath12k_mgmt_rx_reo_list_init(struct ath12k_base *ab, struct mgmt_rx_reo_list *reo_list)
 {
-	struct wmi_vdev_start_resp_event vdev_start_resp;
-	struct ath12k *ar;
-	u32 status;
+	reo_list->max_list_size = ATH12K_MGMT_RX_REO_LIST_MAX_SIZE;
+	reo_list->list_entry_timeout_us = ATH12K_MGMT_RX_REO_LIST_TIMEOUT_US;
+	reo_list->count = 0;
+	reo_list->ab = ab;
+	INIT_LIST_HEAD(&reo_list->list);
+	spin_lock_init(&reo_list->list_lock);
 
-	if (ath12k_pull_vdev_start_resp_tlv(ab, skb, &vdev_start_resp) != 0) {
-		ath12k_warn(ab, "failed to extract vdev start resp");
-		return;
+	timer_setup(&reo_list->ageout_timer,
+		    ath12k_mgmt_rx_reo_list_ageout_timer_handler, 0);
+
+	reo_list->ts_last_released_frame.valid = false;
+
+	timer_setup(&reo_list->global_mgmt_rx_inactivity_timer,
+		    ath12k_mgmt_rx_reo_global_mgmt_rx_inactivity_timer_handler, 0);
+
+	return 0;
+}
+
+/**
+ * ath12k_mgmt_rx_reo_flush_reorder_list() - Flush all entries in the reorder list
+ * @reo_list: Pointer to reorder list
+ *
+ * API to flush all the entries of the reorder list. This API would acquire
+ * the lock protecting the list.
+ *
+ * Return: 0 on success, non-zero on failure
+ */
+static int
+ath12k_mgmt_rx_reo_flush_reorder_list(struct ath12k_base *ab, struct mgmt_rx_reo_list *reo_list)
+{
+	struct mgmt_rx_reo_list_entry *cur_entry, *temp;
+
+	if (!reo_list) {
+		ath12k_err(ab, "reorder list is null\n");
+		return -EINVAL;
 	}
 
-	rcu_read_lock();
-	ar = ath12k_mac_get_ar_by_vdev_id(ab, vdev_start_resp.vdev_id);
-	if (!ar) {
-		ath12k_warn(ab, "invalid vdev id in vdev start resp ev %d",
-			    vdev_start_resp.vdev_id);
-		rcu_read_unlock();
-		return;
+	spin_lock_bh(&reo_list->list_lock);
+
+	list_for_each_entry_safe(cur_entry, temp, &reo_list->list, node) {
+		list_del(&cur_entry->node);
+		dev_kfree_skb(cur_entry->nbuf);
+		kfree(cur_entry->rx_params);
+		kfree(cur_entry);
 	}
 
-	ar->last_wmi_vdev_start_status = 0;
-	ar->max_allowed_tx_power = vdev_start_resp.max_allowed_tx_power;
+	spin_unlock_bh(&reo_list->list_lock);
 
-	status = vdev_start_resp.status;
+	return 0;
+}
 
-	if (WARN_ON_ONCE(status)) {
-		ath12k_warn(ab, "vdev start resp error status %d (%s)\n",
-			    status, ath12k_wmi_vdev_resp_print(status));
-		ar->last_wmi_vdev_start_status = status;
+/**
+ * mgmt_rx_reo_list_deinit() - De initialize the management rx-reorder list
+ * @reo_list: Pointer to reorder list
+ *
+ * API to de initialize the management rx-reorder list.
+ *
+ * Return: 0 on success, non-zero on failure
+ */
+static int
+ath12k_mgmt_rx_reo_list_deinit(struct ath12k_base *ab, struct mgmt_rx_reo_list *reo_list)
+{
+	int status;
+
+	status = ath12k_mgmt_rx_reo_flush_reorder_list(ab, reo_list);
+	if (status) {
+		ath12k_err(ab, "Failed to flush the reorder list\n");
+		return status;
 	}
 
-	complete(&ar->vdev_setup_done);
+	return 0;
+}
 
-	rcu_read_unlock();
+int ath12k_mgmt_rx_reo_init_context(struct ath12k_base *ab)
+{
+	struct ath12k_hw_group *ag = ab->ag;
+	struct ath12k_mgmt_rx_reo_context *reo_context = &ag->rx_reo;
 
-	ath12k_dbg(ab, ATH12K_DBG_WMI, "vdev start resp for vdev id %d",
-		   vdev_start_resp.vdev_id);
+	if (!(ag->mlo_mem.is_mlo_mem_avail && ag->mgmt_rx_reorder))
+		return 0;
+
+	if (reo_context->init_done)
+		return 0;
+
+	spin_lock_init(&reo_context->rx_reorder_entry_lock);
+	spin_lock_init(&reo_context->frame_release_lock);
+
+	ath12k_mgmt_rx_reo_list_init(ab, &reo_context->reo_list);
+
+	reo_context->init_done = true;
+
+	return 0;
 }
 
-static void ath12k_bcn_tx_status_event(struct ath12k_base *ab, struct sk_buff *skb)
+int
+ath12k_mgmt_rx_reo_deinit_context(struct ath12k_base *ab)
 {
-	struct ath12k_link_vif *arvif;
-	u32 vdev_id, tx_status;
+	int status;
+	struct ath12k_mgmt_rx_reo_context *reo_context = &ab->ag->rx_reo;
+	struct ath12k_hw_group *ag = ab->ag;
 
-	if (ath12k_pull_bcn_tx_status_ev(ab, skb->data, skb->len,
-					 &vdev_id, &tx_status) != 0) {
-		ath12k_warn(ab, "failed to extract bcn tx status");
-		return;
-	}
+	if (!(ag->mlo_mem.is_mlo_mem_avail && ag->mgmt_rx_reorder))
+		return 0;
 
-	rcu_read_lock();
-	arvif = ath12k_mac_get_arvif_by_vdev_id(ab, vdev_id);
-	if (!arvif) {
-		ath12k_warn(ab, "invalid vdev id %d in bcn_tx_status",
-			    vdev_id);
-		rcu_read_unlock();
-		return;
+	if (!reo_context->init_done)
+		return 0;
+
+	status = ath12k_mgmt_rx_reo_list_deinit(ab, &reo_context->reo_list);
+	if (status) {
+		ath12k_err(ab, "Failed to de-initialize mgmt Rx reo list\n");
+		return status;
 	}
-	ath12k_mac_bcn_tx_event(arvif);
-	rcu_read_unlock();
+
+	reo_context->init_done = false;
+
+	return status;
 }
 
-static void ath12k_vdev_stopped_event(struct ath12k_base *ab, struct sk_buff *skb)
+static void ath12k_fw_consumed_mgmt_rx_event(struct ath12k_base *ab, struct sk_buff *skb)
 {
+	struct ath12k_hw_group *ag = ab->ag;
+	struct mgmt_rx_event_params *rx_ev;
 	struct ath12k *ar;
-	u32 vdev_id = 0;
+	struct ath12k_mgmt_rx_reo_frame_descriptor desc = {0};
+	bool is_queued = false;
+	int ret;
 
-	if (ath12k_pull_vdev_stopped_param_tlv(ab, skb, &vdev_id) != 0) {
-		ath12k_warn(ab, "failed to extract vdev stopped event");
+	if (!(ag->mlo_mem.is_mlo_mem_avail && ag->mgmt_rx_reorder))
+		return;
+
+	rx_ev = kmalloc(sizeof(*rx_ev), GFP_ATOMIC);
+	if (!rx_ev) {
+		ath12k_err(ab, "failed to allocate rx event\n");
+		return;
+	}
+
+	if (ath12k_pull_fw_consumed_mgmt_rx_params_tlv(ab, skb, rx_ev) != 0) {
+		ath12k_warn(ab, "failed to extract mgmt rx event");
+		kfree(rx_ev);
 		return;
 	}
 
 	rcu_read_lock();
-	ar = ath12k_mac_get_ar_by_vdev_id(ab, vdev_id);
+	ar = ath12k_mac_get_ar_by_pdev_id(ab, rx_ev->pdev_id);
+
 	if (!ar) {
-		ath12k_warn(ab, "invalid vdev id in vdev stopped ev %d",
-			    vdev_id);
-		rcu_read_unlock();
-		return;
+		ath12k_warn(ab, "invalid pdev_id %d in mgmt_rx_event\n",
+			    rx_ev->pdev_id);
+		goto exit;
 	}
 
-	complete(&ar->vdev_setup_done);
+	rx_ev->reo_params.link_id = rx_ev->pdev_id;
 
-	rcu_read_unlock();
+	/* Populate frame descriptor */
+	desc.type = ATH12K_MGMT_RX_REO_FRAME_DESC_HOST_CONSUMED_FRAME;
+	desc.nbuf = NULL; /* No frame buffer */
+	desc.rx_params = rx_ev;
+	desc.ingress_timestamp = ktime_to_us(ktime_get());
+	desc.list_size_rx = -1;
+	desc.list_insertion_pos = -1;
 
-	ath12k_dbg(ab, ATH12K_DBG_WMI, "vdev stopped for vdev id %d", vdev_id);
+	desc.frame_type = IEEE80211_FTYPE_MGMT;
+	desc.frame_subtype = 0xFF;
+
+	desc.reo_required = false;
+
+	ret = ath12k_wmi_mgmt_rx_reorder_process_entry(ar, &desc, &is_queued);
+	if (ret) {
+		ath12k_warn(ab, "Failed to execute MGMT REO reorder process\n");
+		goto exit;
+	}
+
+	if (is_queued)
+		WARN_ON(1);
+
+exit:
+	/* if the frame is queued dont free it here
+	 * it will be taken care by the mgmt rx reorder
+	 * process
+	 */
+	if (!is_queued)
+		kfree(rx_ev);
+
+	rcu_read_unlock();
 }
 
 static void ath12k_mgmt_rx_event(struct ath12k_base *ab, struct sk_buff *skb)
 {
-	struct mgmt_rx_event_params rx_ev = {0};
+	struct ath12k_hw_group *ag = ab->ag;
+	struct mgmt_rx_event_params *rx_ev;
 	struct ath12k *ar;
 	struct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);
 	struct ieee80211_hdr *hdr;
@@ -8237,45 +10633,56 @@ static void ath12k_mgmt_rx_event(struct
 	struct ath12k_vif *ahvif;
 	struct ath12k_mgmt_frame_stats *mgmt_stats;
 	u16 frm_type = 0;
+	struct ath12k_mgmt_rx_reo_frame_descriptor desc = {0};
+	bool is_queued = false;
+	int ret;
+
+	rx_ev = kmalloc(sizeof(*rx_ev), GFP_ATOMIC);
+	if (!rx_ev) {
+		dev_kfree_skb(skb);
+		ath12k_err(ab, "failed to allocate rx event\n");
+		return;
+	}
 
-	if (ath12k_pull_mgmt_rx_params_tlv(ab, skb, &rx_ev) != 0) {
+	if (ath12k_pull_mgmt_rx_params_tlv(ab, skb, rx_ev) != 0) {
 		ath12k_warn(ab, "failed to extract mgmt rx event");
 		dev_kfree_skb(skb);
+		kfree(rx_ev);
 		return;
 	}
 
 	memset(status, 0, sizeof(*status));
 
 	ath12k_dbg(ab, ATH12K_DBG_MGMT, "mgmt rx event status %08x\n",
-		   rx_ev.status);
+		   rx_ev->status);
 
 	rcu_read_lock();
-	ar = ath12k_mac_get_ar_by_pdev_id(ab, rx_ev.pdev_id);
+	ar = ath12k_mac_get_ar_by_pdev_id(ab, rx_ev->pdev_id);
 
 	if (!ar) {
 		ath12k_warn(ab, "invalid pdev_id %d in mgmt_rx_event\n",
-			    rx_ev.pdev_id);
+			    rx_ev->pdev_id);
 		dev_kfree_skb(skb);
 		goto exit;
 	}
 
 	if ((test_bit(ATH12K_CAC_RUNNING, &ar->dev_flags)) ||
-	    (rx_ev.status & (WMI_RX_STATUS_ERR_DECRYPT |
+	    (rx_ev->status & (WMI_RX_STATUS_ERR_DECRYPT |
 	    WMI_RX_STATUS_ERR_KEY_CACHE_MISS | WMI_RX_STATUS_ERR_CRC))) {
 		dev_kfree_skb(skb);
 		goto exit;
 	}
 
-	if (rx_ev.status & WMI_RX_STATUS_ERR_MIC)
+	if (rx_ev->status & WMI_RX_STATUS_ERR_MIC)
 		status->flag |= RX_FLAG_MMIC_ERROR;
 
-	if (rx_ev.chan_freq >= ATH12K_MIN_6G_FREQ &&
-	    rx_ev.chan_freq <= ATH12K_MAX_6G_FREQ) {
+	if (rx_ev->chan_freq >= ATH12K_MIN_6G_FREQ &&
+	    rx_ev->chan_freq <= ATH12K_MAX_6G_FREQ) {
 		status->band = NL80211_BAND_6GHZ;
-		status->freq = rx_ev.chan_freq;
-	} else if (rx_ev.channel >= 1 && rx_ev.channel <= 14) {
+		status->freq = rx_ev->chan_freq;
+	} else if (rx_ev->channel >= 1 && rx_ev->channel <= 14) {
 		status->band = NL80211_BAND_2GHZ;
-	} else if (rx_ev.channel >= 36 && rx_ev.channel <= ATH12K_MAX_5G_CHAN) {
+	} else if (rx_ev->channel >= 36 && rx_ev->channel <= ATH12K_MAX_5G_CHAN) {
 		status->band = NL80211_BAND_5GHZ;
 	} else {
 		/* Shouldn't happen unless list of advertised channels to
@@ -8286,7 +10693,7 @@ static void ath12k_mgmt_rx_event(struct
 		goto exit;
 	}
 
-	if (rx_ev.phy_mode == MODE_11B &&
+	if (rx_ev->phy_mode == MODE_11B &&
 	    (status->band == NL80211_BAND_5GHZ || status->band == NL80211_BAND_6GHZ))
 		ath12k_dbg(ab, ATH12K_DBG_WMI,
 			   "wmi mgmt rx 11b (CCK) on 5/6GHz, band = %d\n", status->band);
@@ -8294,11 +10701,11 @@ static void ath12k_mgmt_rx_event(struct
 	sband = &ar->mac.sbands[status->band];
 
 	if (status->band != NL80211_BAND_6GHZ)
-		status->freq = ieee80211_channel_to_frequency(rx_ev.channel,
+		status->freq = ieee80211_channel_to_frequency(rx_ev->channel,
 							      status->band);
 
-	status->signal = rx_ev.snr + ATH12K_DEFAULT_NOISE_FLOOR;
-	status->rate_idx = ath12k_mac_bitrate_to_idx(sband, rx_ev.rate / 100);
+	status->signal = rx_ev->snr + ATH12K_DEFAULT_NOISE_FLOOR;
+	status->rate_idx = ath12k_mac_bitrate_to_idx(sband, rx_ev->rate / 100);
 
 	hdr = (struct ieee80211_hdr *)skb->data;
 	fc = le16_to_cpu(hdr->frame_control);
@@ -8356,6 +10763,43 @@ skip_mgmt_stats:
 	 *	ath12k_mac_handle_beacon(ar, skb);
 	 */
 
+	if (!(ag->mlo_mem.is_mlo_mem_avail && ag->mgmt_rx_reorder))
+		goto pass_up;
+
+	if (!rx_ev->reo_params.valid) {
+		ath12k_warn(ab, "Invalid MGMT rx REO param for link %u\n",
+			    ar->pdev->hw_link_id);
+		goto pass_up;
+	}
+
+	rx_ev->reo_params.link_id = ar->pdev->hw_link_id;
+
+	/* Populate frame descriptor */
+	desc.type = ATH12K_MGMT_RX_REO_FRAME_DESC_HOST_CONSUMED_FRAME;
+	desc.nbuf = skb;
+	desc.rx_params = rx_ev;
+	desc.ingress_timestamp = ktime_to_us(ktime_get());
+	desc.list_size_rx = -1;
+	desc.list_insertion_pos = -1;
+
+	desc.frame_type = FIELD_GET(IEEE80211_FCTL_FTYPE, fc);
+	desc.frame_subtype = frm_type;
+
+	desc.reo_required = true;
+	ret = ath12k_wmi_mgmt_rx_reorder_process_entry(ar, &desc, &is_queued);
+	if (ret) {
+		ath12k_warn(ab, "Failed to execute MGMT REO reorder process\n");
+		dev_kfree_skb(skb);
+		goto exit;
+	}
+
+	/**
+	 *  If frame is queued, we shouldn't free up rx params
+	 */
+	if (is_queued)
+		goto exit;
+
+pass_up:
 	ath12k_dbg(ab, ATH12K_DBG_MGMT,
 		   "event mgmt rx skb %pK len %d ftype %02x stype %02x\n",
 		   skb, skb->len,
@@ -8369,6 +10813,13 @@ skip_mgmt_stats:
 	ieee80211_rx_ni(ar->ah->hw, skb);
 
 exit:
+	/* if the frame is queued dont free it here
+	 * it will be taken care by the mgmt rx reorder
+	 * process
+	 */
+	if (!is_queued)
+		kfree(rx_ev);
+
 	rcu_read_unlock();
 }
 
@@ -10947,6 +13398,9 @@ static void ath12k_wmi_tlv_op_rx(struct
 	case WMI_VDEV_ADFS_OCAC_COMPLETE_EVENTID:
 		ath12k_process_ocac_complete_event(ab, skb);
 		break;
+	case WMI_MGMT_RX_FW_CONSUMED_EVENTID:
+		ath12k_fw_consumed_mgmt_rx_event(ab, skb);
+		break;
 	/* TODO: Add remaining events */
 	default:
 		ath12k_dbg(ab, ATH12K_DBG_WMI, "Unknown eventid: 0x%x\n", id);
@@ -11573,4 +14027,3 @@ int ath12k_wmi_vdev_adfs_ocac_abort_cmd_
 	}
 	return ret;
 }
-
--- a/drivers/net/wireless/ath/ath12k/wmi.h
+++ b/drivers/net/wireless/ath/ath12k/wmi.h
@@ -428,6 +428,9 @@ enum wmi_tlv_cmd_id {
 	WMI_BSS_COLOR_CHANGE_ENABLE_CMDID,
 	WMI_VDEV_BCN_OFFLOAD_QUIET_CONFIG_CMDID,
 	WMI_FILS_DISCOVERY_TMPL_CMDID,
+	WMI_QOS_NULL_FRAME_TX_SEND_CMDID,
+	/** WMI CMD to receive the management filter criteria from the host for RX REO */
+	WMI_MGMT_RX_REO_FILTER_CONFIGURATION_CMDID,
 	WMI_ADDBA_CLEAR_RESP_CMDID = WMI_TLV_CMD(WMI_GRP_BA_NEG),
 	WMI_ADDBA_SEND_CMDID,
 	WMI_ADDBA_STATUS_CMDID,
@@ -776,6 +779,9 @@ enum wmi_tlv_event_id {
 	WMI_TBTTOFFSET_EXT_UPDATE_EVENTID,
 	WMI_OFFCHAN_DATA_TX_COMPLETION_EVENTID,
 	WMI_HOST_FILS_DISCOVERY_EVENTID,
+	WMI_HOST_FILS_V2_DISCOVERY_EVENTID,
+	WMI_QOS_NULL_FRAME_TX_COMPLETION_EVENTID,
+	WMI_MGMT_RX_FW_CONSUMED_EVENTID,
 	WMI_TX_DELBA_COMPLETE_EVENTID = WMI_TLV_CMD(WMI_GRP_BA_NEG),
 	WMI_TX_ADDBA_COMPLETE_EVENTID,
 	WMI_BA_RSP_SSN_EVENTID,
@@ -2006,6 +2012,9 @@ enum wmi_tlv_tag {
 	WMI_TAG_MLO_TEARDOWN_CMD,
 	WMI_TAG_MLO_TEARDOWN_COMPLETE,
 	WMI_TAG_MLO_PEER_ASSOC_PARAMS = 0x3D0,
+	WMI_TAG_MLO_MGMT_RX_REO_PARAMS = 0x3D2,
+	WMI_TAG_MLO_MGMT_RX_FW_CONSUMED_HDR = 0x3D3,
+	WMI_TAG_MLO_MGMT_RX_REO_FILTER_CFG_CMD = 0x3D4,
 	WMI_TAG_MLO_PEER_CREATE_PARAMS = 0x3D5,
 	WMI_TAG_MLO_VDEV_START_PARAMS = 0x3D6,
 	WMI_TAG_MLO_VDEV_CREATE_PARAMS = 0x3D7,
@@ -5442,6 +5451,676 @@ struct mlo_glb_rx_reo_per_link_snapshot_
 	struct mgmt_rx_reo_snapshot hw_forwarded;
 };
 
+/**
+ * Enum which defines different versions of management Rx reorder snapshots.
+ */
+enum {
+	/**
+	 * DWORD Lower:
+	 * [15:0]  : Management packet counter
+	 * [30:16] : Redundant global time stamp = Global time stamp[14:0]
+	 * [31]    : Valid
+	 *
+	 * DWORD Upper:
+	 * [31:0]  : Global time stamp
+	 *
+	 */
+	ATH12K_MGMT_RX_REO_SNAPSHOT_VERSION_TIMESTAMP_REDUNDANCY = 0,
+
+	/**
+	 * DWORD Lower:
+	 * [14:0]  : Global time stamp[14:0]
+	 * [30:15] : Management packet counter
+	 * [31]    : Valid
+	 *
+	 * DWORD Upper:
+	 * [14:0]  : Redundant management packet counter = Management packet
+	 *           counter[14:0]
+	 * [31:15] : Global time stamp[31:15]
+	 */
+	ATH12K_MGMT_RX_REO_SNAPSHOT_VERSION_PKT_CTR_REDUNDANCY = 1,
+};
+
+/**
+ * Macros for getting and setting the required number of bits
+ * from the TLV params.
+ */
+#define ATH12K_MLO_SHMEM_GET_BITS(_val, _index, _num_bits) \
+	(((_val) >> (_index)) & ((1 << (_num_bits)) - 1))
+
+/* WMI CMD to receive the management filter criteria from the host */
+struct ath12k_wmi_mgmt_rx_reo_filter_config_cmd_fixed_param {
+	u32 tlv_header;
+	u32 pdev_id; /* pdev_id for identifying the MAC */
+	/* filter:
+	 * Each bit represents the possible combination of frame type (2 bits)
+	 * and subtype (4 bits)
+	 * There would be 64 such combinations as per the 802.11 standard
+	 * For Exp : We have beacon frame, we will take the type and subtype
+	 *           of this frame and concatenate the bits, it will give 6 bits
+	 *           number. We need to go to that bit position in the below
+	 *           2 filter_low and filter_high bitmap and set the bit.
+	 */
+	u32 filter_low;
+	u32 filter_high;
+};
+
+#define WMI_MGMT_RX_FW_CONSUMED_PARAM_MGMT_PKT_CTR_VALID_GET GENMASK(15, 15)
+#define WMI_MGMT_RX_FW_CONSUMED_PARAM_MGMT_PKT_CTR_GET	GENMASK(31, 16)
+
+struct ath12k_wmi_mgmt_rx_fw_consumed_hdr {
+	u32 rx_tsf_l32; /* h/w assigned timestamp of the rx frame in micro sec */
+	u32 rx_tsf_u32 ;/* h/w assigned timestamp of the rx frame in micro sec */
+	u32 pdev_id; /* pdev_id for identifying the MAC the rx mgmt frame was received by */
+	/**
+	 * peer_info_subtype
+	 *
+	 * [15:0]:  ml_peer_id, ML peer_id unique across chips
+	 * [18:16]: ieee_link_id, protocol link id on which the rx frame is received
+	 * [27:19]: reserved
+	 * [31:28]: subtype, subtype of the received MGMT frame
+	 */
+	u32 peer_info_subtype;
+	u32 chan_freq; /* frequency in MHz of the channel on which this frame was received */
+	/* Timestamp (in micro sec) of the last fw consumed/dropped mgmt. frame, same across chips */
+	u32 global_timestamp;
+	/**
+	 * mgmt_pkt_ctr_info
+	 *
+	 * [14:0]:  reserved
+	 * [15]:    mgmt_pkt_ctr_valid
+	 * [31:16]: mgmt_pkt_ctr, Sequence number of the last fw consumed mgmt frame
+	 */
+	u32 mgmt_pkt_ctr_info;
+	u32 rx_ppdu_duration_us; /* receive duration in us */
+	u32 mpdu_end_timestamp; /* mpdu end timestamp in us (based on HWMLO timer) */
+};
+
+#define WMI_MGMT_RX_REO_PARAM_IEEE_LINK_ID_GET	GENMASK(14, 12)
+#define WMI_MGMT_RX_REO_PARAM_MGMT_PKT_CTR_VALID_GET	GENMASK(15, 15)
+#define WMI_MGMT_RX_REO_PARAM_MGMT_PKT_CTR_GET	GENMASK(31, 16)
+
+/** Data structure of the TLV to add in RX EVENTID for providing REO params
+ *  like global_timestamp and mgmt_pkt_ctr
+ */
+struct ath12k_wmi_mgmt_rx_reo_params {
+	/* Timestamp (in micro sec) of the last fw forwarded mgmt. frame, same across chips */
+	u32 global_timestamp;
+	/**
+	 * mgmt_pkt_ctr_link_info
+	 *
+	 * [11:0]:  reserved
+	 * [14:12]: ieee_link_id, protocol link id on which the rx frame is received
+	 * [15]:    mgmt_pkt_ctr_valid
+	 * [31:16]: mgmt_pkt_ctr, Sequence number of the last fw forwarded mgmt frame
+	 */
+
+	u32 mgmt_pkt_ctr_link_info;
+	u32 rx_ppdu_duration_us; /* receive duration in us */
+	u32 mpdu_end_timestamp; /* mpdu end timestamp in us (based on HWMLO timer) */
+};
+
+/* struct ath12k_mgmt_rx_reo_params - MGMT Rx REO parameters
+ * @valid: Whether these params are valid
+ * @pdev_id: pdev ID for which FW consumed event is received
+ * @link_id: link ID for which FW consumed event is received
+ * @mgmt_pkt_ctr: MGMT packet counter of the frame that is consumed
+ * @global_timestamp: Global timestamp of the frame that is consumed
+ * @duration_us: duration in us
+ * @start_timestamp: start time stamp
+ * @end_timestamp: end time stamp
+ */
+struct ath12k_mgmt_rx_reo_params {
+	bool valid;
+	u8 pdev_id;
+	u8 link_id;
+	u8 mlo_grp_id;
+	u16 mgmt_pkt_ctr;
+	u32 global_timestamp;
+	u16 duration_us;
+	u32 start_timestamp;
+	u32 end_timestamp;
+};
+
+/* struct ath12k_mgmt_rx_reo_filter - MGMT Rx REO filter
+ * @filter_low: Least significant 32-bits of the filter
+ * @filter_high: Most significant 32-bits of the filter
+ */
+struct ath12k_mgmt_rx_reo_filter {
+	u32 low;
+	u32 high;
+};
+
+/* struct ath12k_mgmt_rx_reo_wait_count - Wait count for a mgmt frame
+ * @per_link_count: Array of wait counts for all MLO links. Each array entry
+ * holds the number of frames this mgmt frame should wait for on that
+ * particular link.
+ * @total_count: Sum of entries in @per_link_count
+ */
+struct ath12k_mgmt_rx_reo_wait_count {
+	unsigned int per_link_count[ATH12K_WMI_MLO_MAX_LINKS];
+	unsigned long long total_count;
+};
+
+/* struct ath12k_mgmt_rx_reo_snapshot_params - Represents the simplified version of
+ * Management Rx Frame snapshot for Host use. Note that this is different from
+ * the structure shared between the Host and FW/HW
+ * @valid: Whether this snapshot is valid
+ * @retry_count: snapshot read retry count
+ * @mgmt_pkt_ctr: MGMT packet counter. This will be local to a particular
+ * HW link
+ * @global_timestamp: Global timestamp.This is taken from a clock which is
+ * common across all the HW links
+ */
+struct ath12k_mgmt_rx_reo_snapshot_params {
+	bool valid;
+	u8 retry_count;
+	u16 mgmt_pkt_ctr;
+	u32 global_timestamp;
+};
+
+/* struct ath12k_mgmt_rx_reo_shared_snapshot - Represents the management rx-reorder
+ * shared snapshots
+ * @ath12k_mgmt_rx_reo_snapshot_low: Lower 32 bits of the reo snapshot
+ * @ath12k_mgmt_rx_reo_snapshot_high: Higher 32 bits of the reo snapshot
+ */
+struct ath12k_mgmt_rx_reo_shared_snapshot {
+	union {
+		u32 ath12k_mgmt_rx_reo_snapshot_low;
+		u32 mgmt_pkt_ctr_ver_a:16,
+			global_timestamp_redundant_ver_a:15,
+			valid_ver_a:1;
+		u32 global_timestamp_low_ver_b:15,
+			mgmt_pkt_ctr_ver_b:16,
+			valid_ver_b:1;
+	} u_low;
+
+	union {
+		u32 ath12k_mgmt_rx_reo_snapshot_high;
+		u32 global_timestamp_ver_a;
+		u32 mgmt_pkt_ctr_redundant_ver_b:15,
+			global_timestamp_high_ver_b:17;
+	} u_high;
+};
+
+/* struct ath12k_mgmt_rx_reo_snapshot_info - Information related to management Rx
+ * reorder snapshot
+ * @address: Snapshot address
+ * @version: Snapshot version
+ */
+struct ath12k_mgmt_rx_reo_snapshot_info {
+	struct ath12k_mgmt_rx_reo_shared_snapshot *address;
+	u8 version;
+};
+
+#define ATH12K_MGMT_RX_REO_SNAPSHOT_B2B_READ_SWAR_RETRY_LIMIT     (11)
+#define ATH12K_MGMT_RX_REO_SNAPSHOT_READ_RETRY_LIMIT              (25)
+
+/* enum ath12k_mgmt_rx_reo_shared_snapshot_id - Represents the management
+ * rx-reorder snapshots shared between host and target in the host DDR.
+ * These snapshots are written by FW/HW and read by Host.
+ * @ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_CONSUMED: FW consumed snapshot
+ * @ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_FORWARDED: FW forwarded snapshot
+ * @ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAC_HW: MAC HW snapshot
+ * @ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX: Max number of snapshots
+ * @ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_INVALID: Invalid snapshot
+ */
+enum ath12k_mgmt_rx_reo_shared_snapshot_id {
+	ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_CONSUMED = 0,
+	ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_FW_FORWARDED = 1,
+	ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAC_HW = 2,
+	ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX = 3,
+	ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_INVALID,
+};
+
+/* struct ath12k_mgmt_rx_reo_pdev_info - Pdev information required by the Management
+ * Rx REO module
+ * @host_snapshot: Latest snapshot seen at the Host.
+ * It considers both MGMT Rx and MGMT FW consumed.
+ * @last_valid_shared_snapshot: Array of last valid snapshots(for snapshots
+ * shared between host and target)
+ * @host_target_shared_snapshot_info: Array of meta information related to
+ * snapshots(for snapshots shared between host and target)
+ * @filter: MGMT Rx REO filter
+ * @init_complete: Flag to indicate initialization completion of the
+ * ath12k_mgmt_rx_reo_pdev_info object
+ */
+struct ath12k_mgmt_rx_reo_pdev_info {
+	struct ath12k_mgmt_rx_reo_snapshot_params host_snapshot;
+	struct ath12k_mgmt_rx_reo_snapshot_params last_valid_shared_snapshot
+		[ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX];
+	struct ath12k_mgmt_rx_reo_snapshot_info host_target_shared_snapshot_info
+		[ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX];
+	struct ath12k_mgmt_rx_reo_filter filter;
+	struct ath12k_mgmt_rx_reo_shared_snapshot raw_snapshots[ATH12K_WMI_MLO_MAX_LINKS]
+		[ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX]
+		[ATH12K_MGMT_RX_REO_SNAPSHOT_READ_RETRY_LIMIT]
+			[ATH12K_MGMT_RX_REO_SNAPSHOT_B2B_READ_SWAR_RETRY_LIMIT];
+	bool init_complete;
+};
+
+/* enum ath12k_mgmt_rx_reo_frame_descriptor_type - Enumeration for management frame
+ * descriptor type.
+ * @ATH12K_MGMT_RX_REO_FRAME_DESC_HOST_CONSUMED_FRAME: Management frame to be consumed
+ * by host.
+ * @ATH12K_MGMT_RX_REO_FRAME_DESC_FW_CONSUMED_FRAME: Management frame consumed by FW
+ * @ATH12K_MGMT_RX_REO_FRAME_DESC_ERROR_FRAME: Management frame which got dropped
+ * at host due to any error
+ * @ATH12K_MGMT_RX_REO_FRAME_DESC_TYPE_MAX: Maximum number of frame types
+ */
+enum ath12k_mgmt_rx_reo_frame_descriptor_type {
+	ATH12K_MGMT_RX_REO_FRAME_DESC_HOST_CONSUMED_FRAME = 0,
+	ATH12K_MGMT_RX_REO_FRAME_DESC_FW_CONSUMED_FRAME,
+	ATH12K_MGMT_RX_REO_FRAME_DESC_ERROR_FRAME,
+	ATH12K_MGMT_RX_REO_FRAME_DESC_TYPE_MAX,
+};
+
+/* struct ath12k_mgmt_rx_reo_global_ts_info - This structure holds the global time
+ * stamp information of a frame.
+ * @valid: Indicates whether global time stamp is valid
+ * @global_ts: Global time stamp value
+ * @start_ts: Start time stamp value
+ * @end_ts: End time stamp value
+ * @expiry_time: information in the structure is valid until expiry_time
+ */
+struct ath12k_mgmt_rx_reo_global_ts_info {
+	bool valid;
+	u32 global_ts;
+	u32 start_ts;
+	u32 end_ts;
+	unsigned long expiry_time;
+};
+
+/* struct ath12k_reo_ingress_debug_frame_info - Debug information about a frame
+ * entering reorder process
+ * @link_id: link id
+ * @mgmt_pkt_ctr: management packet counter
+ * @global_timestamp: MLO global time stamp
+ * @start_timestamp: start time stamp of the frame
+ * @end_timestamp: end time stamp of the frame
+ * @duration_us: duration of the frame in us
+ * @desc_type: Type of the frame descriptor
+ * @frame_type: frame type
+ * @frame_subtype: frame sub type
+ * @ingress_timestamp: Host time stamp when the frames enters the reorder
+ * process
+ * @ingress_duration: Duration in us for processing the incoming frame.
+ * ingress_duration = Time stamp at which reorder list update is done -
+ * Time stamp at which frame has entered the reorder module
+ * @wait_count: Wait count calculated for the current frame
+ * @is_queued: Indicates whether this frame is queued to reorder list
+ * @is_stale: Indicates whether this frame is stale.
+ * @is_parallel_rx: Indicates that this frame is received in parallel to the
+ * last frame which is delivered to the upper layer.
+ * @zero_wait_count_rx: Indicates whether this frame's wait count was
+ * zero when received by host
+ * @immediate_delivery: Indicates whether this frame can be delivered
+ * immediately to the upper layers
+ * @is_error: Indicates whether any error occurred during processing this frame
+ * @ts_last_released_frame: Stores the global time stamp for the last frame
+ * removed from the reorder list
+ * @list_size_rx: Size of the reorder list when this frame is received (before
+ * updating the list based on this frame).
+ * @list_insertion_pos: Position in the reorder list where this frame is going
+ * to get inserted (Applicable for only host consumed frames)
+ * @shared_snapshots: snapshots shared b/w host and target
+ * @host_snapshot: host snapshot
+ * @cpu_id: CPU index
+ * @reo_required: Indicates whether reorder is required for the current frame.
+ * If reorder is not required, current frame will just be used for updating the
+ * wait count of frames already part of the reorder list.
+ */
+struct ath12k_reo_ingress_debug_frame_info {
+	u8 link_id;
+	u16 mgmt_pkt_ctr;
+	u32 global_timestamp;
+	u32 start_timestamp;
+	u32 end_timestamp;
+	u32 duration_us;
+	enum ath12k_mgmt_rx_reo_frame_descriptor_type desc_type;
+	u8 frame_type;
+	u8 frame_subtype;
+	u64 ingress_timestamp;
+	u64 ingress_duration;
+	struct ath12k_mgmt_rx_reo_wait_count wait_count;
+	bool is_queued;
+	bool is_stale;
+	bool is_parallel_rx;
+	bool zero_wait_count_rx;
+	bool immediate_delivery;
+	bool is_error;
+	struct ath12k_mgmt_rx_reo_global_ts_info ts_last_released_frame;
+	s16 list_size_rx;
+	s16 list_insertion_pos;
+	struct ath12k_mgmt_rx_reo_snapshot_params shared_snapshots
+		[ATH12K_WMI_MLO_MAX_LINKS][ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX];
+	struct ath12k_mgmt_rx_reo_snapshot_params host_snapshot[ATH12K_WMI_MLO_MAX_LINKS];
+	int cpu_id;
+	bool reo_required;
+};
+
+/* struct ath12k_reo_ingress_frame_stats - Structure to store statistics related to
+ * incoming frames
+ * @ingress_count: Number of frames entering reo module
+ * @queued_count: Number of frames queued to reorder list
+ * @zero_wait_count_rx_count: Number of frames for which wait count is
+ * zero when received at host
+ * @immediate_delivery_count: Number of frames which can be delivered
+ * immediately to the upper layers without reordering. A frame can be
+ * immediately delivered if it has wait count of zero on reception at host
+ * and the global time stamp is less than or equal to the global time
+ * stamp of all the frames in the reorder list. Such frames would get
+ * inserted to the head of the reorder list and gets delivered immediately
+ * to the upper layers.
+ * @stale_count: Number of stale frames. Any frame older than the
+ * last frame delivered to upper layer is a stale frame.
+ * @error_count: Number of frames dropped due to error occurred
+ * within the reorder module
+ */
+struct ath12k_reo_ingress_frame_stats {
+	u64 ingress_count
+		[ATH12K_WMI_MLO_MAX_LINKS][ATH12K_MGMT_RX_REO_FRAME_DESC_TYPE_MAX];
+	u64 queued_count[ATH12K_WMI_MLO_MAX_LINKS];
+	u64 zero_wait_count_rx_count[ATH12K_WMI_MLO_MAX_LINKS];
+	u64 immediate_delivery_count[ATH12K_WMI_MLO_MAX_LINKS];
+	u64 stale_count[ATH12K_WMI_MLO_MAX_LINKS]
+		[ATH12K_MGMT_RX_REO_FRAME_DESC_TYPE_MAX];
+	u64 error_count[ATH12K_WMI_MLO_MAX_LINKS]
+		[ATH12K_MGMT_RX_REO_FRAME_DESC_TYPE_MAX];
+};
+
+#define ATH12K_MGMT_RX_REO_INGRESS_FRAME_DEBUG_INFO_BOARDER_MAX_SIZE	(785)
+
+/* struct reo_ingress_debug_info - Circular array to store the
+ * debug information about the frames entering the reorder process.
+ * @frame_list: Circular array to store the debug info about frames
+ * @frame_list_size: Size of circular array @frame_list
+ * @next_index: The index at which information about next frame will be logged
+ * @wrap_aroud: Flag to indicate whether wrap around occurred when logging
+ * debug information to @frame_list
+ * @stats: Stats related to incoming frames
+ * @boarder: boarder string
+ */
+struct reo_ingress_debug_info {
+	struct ath12k_reo_ingress_debug_frame_info *frame_list;
+	u16 frame_list_size;
+	int next_index;
+	bool wrap_aroud;
+	struct ath12k_reo_ingress_frame_stats stats;
+	char boarder[ATH12K_MGMT_RX_REO_INGRESS_FRAME_DEBUG_INFO_BOARDER_MAX_SIZE + 1];
+};
+
+/* struct ath12k_reo_egress_debug_frame_info - Debug information about a frame
+ * leaving the reorder module
+ * @is_delivered: Indicates whether the frame is delivered to upper layers
+ * @is_premature_delivery: Indicates whether the frame is delivered
+ * prematurely
+ * @link_id: link id
+ * @mgmt_pkt_ctr: management packet counter
+ * @global_timestamp: MLO global time stamp
+ * @ingress_timestamp: Host time stamp when the frame enters the reorder module
+ * @insertion_ts: Host time stamp when the frame is inserted into the reorder
+ * list
+ * @egress_timestamp: Host time stamp just before delivery of the frame to upper
+ * layer
+ * @egress_duration: Duration in us taken by the upper layer to process
+ * the frame.
+ * @removal_ts: Host time stamp when this entry is removed from the list
+ * @initial_wait_count: Wait count when the frame is queued
+ * @final_wait_count: Wait count when frame is released to upper layer
+ * @release_reason: Reason for delivering the frame to upper layers
+ * @shared_snapshots: snapshots shared b/w host and target
+ * @host_snapshot: host snapshot
+ * @cpu_id: CPU index
+ */
+struct ath12k_reo_egress_debug_frame_info {
+	bool is_delivered;
+	bool is_premature_delivery;
+	u8 link_id;
+	u16 mgmt_pkt_ctr;
+	u32 global_timestamp;
+	u64 ingress_timestamp;
+	u64 insertion_ts;
+	u64 egress_timestamp;
+	u64 egress_duration;
+	u64 removal_ts;
+	struct ath12k_mgmt_rx_reo_wait_count initial_wait_count;
+	struct ath12k_mgmt_rx_reo_wait_count final_wait_count;
+	u8 release_reason;
+	struct ath12k_mgmt_rx_reo_snapshot_params shared_snapshots
+		[ATH12K_WMI_MLO_MAX_LINKS][ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX];
+	struct ath12k_mgmt_rx_reo_snapshot_params host_snapshot[ATH12K_WMI_MLO_MAX_LINKS];
+	int cpu_id;
+};
+
+/* Reason to release an entry from the reorder list */
+#define MGMT_RX_REO_LIST_ENTRY_RELEASE_REASON_ZERO_WAIT_COUNT           (BIT(0))
+#define MGMT_RX_REO_LIST_ENTRY_RELEASE_REASON_AGED_OUT                  (BIT(1))
+#define MGMT_RX_REO_LIST_ENTRY_RELEASE_REASON_OLDER_THAN_AGED_OUT_FRAME (BIT(2))
+#define MGMT_RX_REO_LIST_ENTRY_RELEASE_REASON_LIST_MAX_SIZE_EXCEEDED    (BIT(3))
+#define MGMT_RX_REO_RELEASE_REASON_MAX      \
+    (MGMT_RX_REO_LIST_ENTRY_RELEASE_REASON_LIST_MAX_SIZE_EXCEEDED << 1)
+
+/**
+ * struct reo_egress_frame_stats - Structure to store statistics related to
+ * outgoing frames
+ * @delivery_attempts_count: Number of attempts to deliver management
+ * frames to upper layers
+ * @delivery_success_count: Number of successful management frame
+ * deliveries to upper layer
+ * @premature_delivery_count:  Number of frames delivered
+ * prematurely. Premature delivery is the delivery of a management frame
+ * to the upper layers even before its wait count is reaching zero.
+ * @delivery_count: Number frames delivered successfully for
+ * each link and release  reason.
+ */
+struct reo_egress_frame_stats {
+	u64 delivery_attempts_count[ATH12K_WMI_MLO_MAX_LINKS];
+	u64 delivery_success_count[ATH12K_WMI_MLO_MAX_LINKS];
+	u64 premature_delivery_count[ATH12K_WMI_MLO_MAX_LINKS];
+	u64 delivery_count[ATH12K_WMI_MLO_MAX_LINKS]
+		[MGMT_RX_REO_RELEASE_REASON_MAX];
+};
+
+#define ATH12K_MGMT_RX_REO_EGRESS_FRAME_DEBUG_INFO_BOARDER_MAX_SIZE   (816)
+
+/**
+ * struct ath12k_reo_egress_debug_info - Circular array to store the
+ * debug information about the frames leaving the reorder module.
+ * @frame_list: Circular array to store the debug info
+ * @frame_list_size: Size of circular array @frame_list
+ * @next_index: The index at which information about next frame will be logged
+ * @wrap_aroud: Flag to indicate whether wrap around occurred when logging
+ * debug information to @frame_list
+ * @stats: Stats related to outgoing frames
+ * @boarder: boarder string
+ */
+struct ath12k_reo_egress_debug_info {
+	struct ath12k_reo_egress_debug_frame_info *frame_list;
+	u16 frame_list_size;
+	int next_index;
+	bool wrap_aroud;
+	struct reo_egress_frame_stats stats;
+	char boarder[ATH12K_MGMT_RX_REO_EGRESS_FRAME_DEBUG_INFO_BOARDER_MAX_SIZE + 1];
+};
+
+/**
+ * struct mgmt_rx_reo_list – Linked list used to reorder the management frames
+ * received. Each list entry would correspond to a management frame. List
+ * entries would be sorted in the same order in which they are received by MAC
+ * HW.
+ * @list: List used for reordering
+ * @list_lock: Lock to protect the list
+ * @max_list_size: Maximum size of the reorder list
+ * @list_entry_timeout_us: Time out value(microsecond) for the reorder list
+ * entries
+ * @ageout_timer: Periodic timer to age-out the list entries
+ * @global_mgmt_rx_inactivity_timer: Global management Rx inactivity timer
+ * @ts_last_released_frame: Stores the global time stamp for the last frame
+ * removed from the reorder list
+ */
+struct mgmt_rx_reo_list {
+	struct ath12k_base *ab;
+	struct list_head list;
+	int count;
+	/* protects the list used for reordering */
+	spinlock_t list_lock;
+	u32 max_list_size;
+	u32 list_entry_timeout_us;
+	struct timer_list ageout_timer;
+	struct timer_list global_mgmt_rx_inactivity_timer;
+	struct ath12k_mgmt_rx_reo_global_ts_info ts_last_released_frame;
+};
+
+/**
+ * struct ath12k_mgmt_rx_reo_context - This structure holds the info required for
+ * management rx-reordering. Reordering is done across all the psocs.
+ * So there should be only one instance of this structure defined.
+ * @reo_list: Linked list used for reordering
+ * @rx_reorder_entry_lock: Spin lock to protect rx reorder process entry critical
+ * section execution
+ * @frame_release_lock: Spin lock to serialize the frame delivery to the
+ * upper layers. This could prevent race conditions like the one given in
+ * the following example.
+ * Lets take an example of 2 links (Link A & B) and each has received
+ * a management frame A1(deauth) and B1(auth) such that MLO global time
+ * stamp of A1 < MLO global time stamp of B1. Host is concurrently
+ * executing "mgmt_rx_reo_list_release_entries" for A1 and B1 in
+ * 2 different CPUs. It is possible that frame B1 gets processed by
+ * upper layers before frame A1 and this could result in unwanted
+ * disconnection. Hence it is required to serialize the delivery
+ * of management frames to upper layers in the strict order of MLO
+ * global time stamp.
+ * @sim_context: Management rx-reorder simulation context
+ * @ingress_debug_info_init_count: Initialization count of
+ * object @ingress_frame_debug_info
+ * @ingress_frame_debug_info: Debug object to log incoming frames
+ * @egress_frame_debug_info: Debug object to log outgoing frames
+ * @egress_debug_info_init_count: Initialization count of
+ * object @egress_frame_debug_info
+ * @simulation_in_progress: Flag to indicate whether simulation is
+ * in progress
+ * @init_done: This will indicate intialization of management rx-reorder list.
+ * @timer_init_done: This will indicate firing of management rx-reorder timer.
+ */
+struct ath12k_mgmt_rx_reo_context {
+	struct mgmt_rx_reo_list reo_list;
+	/* Protects the rx reorder process entry critical section exec */
+	spinlock_t rx_reorder_entry_lock;
+	/* Lock to Serialize the frame delivery */
+	spinlock_t frame_release_lock;
+	atomic_t ingress_debug_info_init_count;
+	struct  reo_ingress_debug_info ingress_frame_debug_info;
+	atomic_t egress_debug_info_init_count;
+	struct  ath12k_reo_egress_debug_info egress_frame_debug_info;
+	bool init_done;
+	bool timer_init_done;
+};
+
+/** MGMT RX REO Changes */
+/* Macros for having versioning info for compatibility check between host and firmware */
+#define MLO_SHMEM_MAJOR_VERSION 2
+#define MLO_SHMEM_MINOR_VERSION 1
+
+/**
+ * Enum which defines different versions of management Rx reorder snapshots.
+ */
+enum {
+	/**
+	 * DWORD Lower:
+	 * [15:0]  : Management packet counter
+	 * [30:16] : Redundant global time stamp = Global time stamp[14:0]
+	 * [31]    : Valid
+	 *
+	 * DWORD Upper:
+	 * [31:0]  : Global time stamp
+	 *
+	 */
+	MGMT_RX_REO_SNAPSHOT_VERSION_TIMESTAMP_REDUNDANCY = 0,
+
+	/**
+	 * DWORD Lower:
+	 * [14:0]  : Global time stamp[14:0]
+	 * [30:15] : Management packet counter
+	 * [31]    : Valid
+	 *
+	 * DWORD Upper:
+	 * [14:0]  : Redundant management packet counter = Management packet
+	 *           counter[14:0]
+	 * [31:15] : Global time stamp[31:15]
+	 */
+	MGMT_RX_REO_SNAPSHOT_VERSION_PKT_CTR_REDUNDANCY = 1,
+};
+
+#define ATH12K_MGMT_RX_REO_INVALID_SNAPSHOT_VERSION      (-1)
+
+/**
+ * struct mgmt_rx_reo_list_entry - Entry in the Management reorder list
+ * @node: List node
+ * @nbuf: nbuf corresponding to this frame
+ * @rx_params: Management rx event parameters
+ * @wait_count: Wait counts for the frame
+ * @initial_wait_count: Wait count when the frame is queued
+ * @insertion_ts: Host time stamp when this entry is inserted to the list.
+ * @removal_ts: Host time stamp when this entry is removed from the list
+ * @ingress_timestamp: Host time stamp when this frame has arrived reorder
+ * module
+ * @egress_timestamp: Host time stamp when this frame has exited reorder
+ * module
+ * @status: Status for this entry
+ * @pdev: Pointer to pdev object corresponding to this frame
+ * @release_reason: Release reason
+ * @is_delivered: Indicates whether the frame is delivered successfully
+ * @is_premature_delivery: Indicates whether the frame is delivered
+ * prematurely
+ * @is_parallel_rx: Indicates that this frame is received in parallel to the
+ * last frame which is delivered to the upper layer.
+ * @shared_snapshots: snapshots shared b/w host and target
+ * @host_snapshot: host snapshot
+ */
+struct mgmt_rx_reo_list_entry {
+	struct list_head node;
+	struct sk_buff *nbuf;
+	struct mgmt_rx_event_params *rx_params;
+	struct ath12k_mgmt_rx_reo_wait_count wait_count;
+	struct ath12k_mgmt_rx_reo_wait_count initial_wait_count;
+	u64 insertion_ts;
+	u64 removal_ts;
+	u64 ingress_timestamp;
+	u64 egress_timestamp;
+	u32 status;
+	struct ath12k *ar;
+	u8 release_reason;
+	bool is_delivered;
+	bool is_premature_delivery;
+	bool is_parallel_rx;
+	struct ath12k_mgmt_rx_reo_snapshot_params shared_snapshots
+		[ATH12K_WMI_MLO_MAX_LINKS][ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX];
+	struct ath12k_mgmt_rx_reo_snapshot_params host_snapshot[ATH12K_WMI_MLO_MAX_LINKS];
+};
+
+#define ATH12K_MGMT_RX_REO_STATUS_WAIT_FOR_FRAME_ON_OTHER_LINKS         (BIT(0))
+#define ATH12K_MGMT_RX_REO_STATUS_AGED_OUT                              (BIT(1))
+#define ATH12K_MGMT_RX_REO_STATUS_OLDER_THAN_LATEST_AGED_OUT_FRAME      (BIT(2))
+#define ATH12K_MGMT_RX_REO_STATUS_LIST_MAX_SIZE_EXCEEDED                (BIT(3))
+
+#define ATH12K_MGMT_RX_REO_LIST_ENTRY_IS_WAITING_FOR_FRAME_ON_OTHER_LINK(entry)   \
+	((entry)->status & ATH12K_MGMT_RX_REO_STATUS_WAIT_FOR_FRAME_ON_OTHER_LINKS)
+#define ATH12K_MGMT_RX_REO_LIST_ENTRY_IS_AGED_OUT(entry)   \
+	((entry)->status & ATH12K_MGMT_RX_REO_STATUS_AGED_OUT)
+#define ATH12K_MGMT_RX_REO_LIST_ENTRY_IS_OLDER_THAN_LATEST_AGED_OUT_FRAME(entry)  \
+	((entry)->status & ATH12K_MGMT_RX_REO_STATUS_OLDER_THAN_LATEST_AGED_OUT_FRAME)
+#define ATH12K_MGMT_RX_REO_LIST_ENTRY_IS_MAX_SIZE_EXCEEDED(entry)  \
+	((entry)->status & ATH12K_MGMT_RX_REO_STATUS_LIST_MAX_SIZE_EXCEEDED)
+
+#define ATH12K_MGMT_RX_REO_GLOBAL_MGMT_RX_INACTIVITY_TIMEOUT	(600 * HZ)
+#define ATH12K_MGMT_RX_REO_LIST_MAX_SIZE             (100)
+#define ATH12K_MGMT_RX_REO_LIST_TIMEOUT_US           (500000)
+#define ATH12K_MGMT_RX_REO_AGEOUT_TIMER_PERIOD_MS    (250)
+
+#define ATH12K_MGMT_RX_REO_PKT_CTR_HALF_RANGE	(0x8000)
+#define ATH12K_MGMT_RX_REO_PKT_CTR_FULL_RANGE	(ATH12K_MGMT_RX_REO_PKT_CTR_HALF_RANGE << 1)
+
 struct mgmt_rx_event_params {
 	u32 chan_freq;
 	u32 channel;
@@ -5455,6 +6134,61 @@ struct mgmt_rx_event_params {
 	int rssi;
 	u32 tsf_delta;
 	u8 pdev_id;
+	struct ath12k_mgmt_rx_reo_params reo_params;
+};
+
+/**
+ * struct ath12k_mgmt_rx_reo_frame_descriptor - Frame Descriptor used to describe
+ * a management frame in mgmt rx reo module.
+ * @type: Frame descriptor type
+ * @frame_type: frame type
+ * @frame_subtype: frame subtype
+ * @nbuf: nbuf corresponding to this frame
+ * @rx_params: Management rx event parameters
+ * @wait_count: Wait counts for the frame
+ * @ingress_timestamp: Host time stamp when the frames enters the reorder
+ * process
+ * @is_stale: Indicates whether this frame is stale. Any frame older than the
+ * last frame delivered to upper layer is a stale frame. Stale frames should not
+ * be delivered to the upper layers. These frames can be discarded after
+ * updating the host snapshot and wait counts of entries currently residing in
+ * the reorder list.
+ * @zero_wait_count_rx: Indicates whether this frame's wait count was
+ * zero when received by host
+ * @immediate_delivery: Indicates whether this frame can be delivered
+ * immediately to the upper layers
+ * @list_size_rx: Size of the reorder list when this frame is received (before
+ * updating the list based on this frame).
+ * @list_insertion_pos: Position in the reorder list where this frame is going
+ * to get inserted (Applicable for only host consumed frames)
+ * @shared_snapshots: snapshots shared b/w host and target
+ * @host_snapshot: host snapshot
+ * @is_parallel_rx: Indicates that this frame is received in parallel to the
+ * last frame which is delivered to the upper layer.
+ * @pkt_ctr_delta: Packet counter delta of the current and last frame
+ * @reo_required: Indicates whether reorder is required for the current frame.
+ * If reorder is not required, current frame will just be used for updating the
+ * wait count of frames already part of the reorder list.
+ */
+struct ath12k_mgmt_rx_reo_frame_descriptor {
+	enum ath12k_mgmt_rx_reo_frame_descriptor_type type;
+	u8 frame_type;
+	u8 frame_subtype;
+	struct sk_buff *nbuf;
+	struct mgmt_rx_event_params *rx_params;
+	struct ath12k_mgmt_rx_reo_wait_count wait_count;
+	u64 ingress_timestamp;
+	bool is_stale;
+	bool zero_wait_count_rx;
+	bool immediate_delivery;
+	s16 list_size_rx;
+	s16 list_insertion_pos;
+	struct ath12k_mgmt_rx_reo_snapshot_params shared_snapshots
+		[ATH12K_WMI_MLO_MAX_LINKS][ATH12K_MGMT_RX_REO_SHARED_SNAPSHOT_MAX];
+	struct ath12k_mgmt_rx_reo_snapshot_params host_snapshot[ATH12K_WMI_MLO_MAX_LINKS];
+	bool is_parallel_rx;
+	int pkt_ctr_delta;
+	bool reo_required;
 };
 
 #define ATH_MAX_ANTENNA 4
@@ -5480,6 +6214,8 @@ struct wmi_tlv_mgmt_rx_parse {
 	struct wmi_mgmt_rx_hdr *fixed;
 	u8 *frame_buf;
 	bool frame_buf_done;
+	struct ath12k_wmi_mgmt_rx_reo_params *reo_params;
+	struct ath12k_wmi_mgmt_rx_fw_consumed_hdr *fw_consumed_reo_params;
 };
 
 #define MAX_ANTENNA_EIGHT 8
@@ -7475,4 +8211,10 @@ int ath12k_wmi_pdev_multiple_vdev_restar
 int ath12k_wmi_vdev_adfs_ch_cfg_cmd_send(struct ath12k *ar,u32 vdev_id,
                                         struct cfg80211_chan_def *chandef);
 int ath12k_wmi_vdev_adfs_ocac_abort_cmd_send(struct ath12k *ar,u32 vdev_id);
+int ath12k_wmi_mgmt_rx_reo_filter_config(struct ath12k *ar,
+					 struct ath12k_mgmt_rx_reo_filter *filter);
+int
+ath12k_mgmt_rx_reo_init_context(struct ath12k_base *ab);
+int
+ath12k_mgmt_rx_reo_deinit_context(struct ath12k_base *ab);
 #endif
--- a/drivers/net/wireless/ath/ath12k/mac.c
+++ b/drivers/net/wireless/ath/ath12k/mac.c
@@ -9072,14 +9072,52 @@ static void ath12k_mac_radio_stop(struct
         spin_unlock_bh(&ar->data_lock);
 }
 
+static void ath12k_mgmt_rx_reo_init_timer(struct ath12k_hw_group *ag)
+{
+	struct ath12k_mgmt_rx_reo_context *reo_context = &ag->rx_reo;
+
+	if (!(ag->mlo_mem.is_mlo_mem_avail && ag->mgmt_rx_reorder))
+		return;
+
+	if (reo_context->timer_init_done)
+		return;
+
+	mod_timer(&reo_context->reo_list.ageout_timer, jiffies +
+			msecs_to_jiffies(ATH12K_MGMT_RX_REO_AGEOUT_TIMER_PERIOD_MS));
+
+	mod_timer(&reo_context->reo_list.global_mgmt_rx_inactivity_timer, jiffies +
+			ATH12K_MGMT_RX_REO_GLOBAL_MGMT_RX_INACTIVITY_TIMEOUT);
+
+	reo_context->timer_init_done = true;
+}
+
+static void ath12k_mgmt_rx_reo_deinit_timer(struct ath12k_hw_group *ag)
+{
+	struct ath12k_mgmt_rx_reo_context *reo_context = &ag->rx_reo;
+
+	if (!(ag->mlo_mem.is_mlo_mem_avail && ag->mgmt_rx_reorder))
+		return;
+
+	if (!reo_context->timer_init_done)
+		return;
+
+	del_timer_sync(&reo_context->reo_list.global_mgmt_rx_inactivity_timer);
+	del_timer_sync(&reo_context->reo_list.ageout_timer);
+
+	reo_context->timer_init_done = false;
+}
+
 static int ath12k_mac_op_start(struct ieee80211_hw *hw)
 {
 	struct ath12k_hw *ah = hw->priv;
 	struct ath12k *ar;
 	struct ath12k_base *ab;
+	struct ath12k_hw_group *ag = ah->ag;
 	int i;
 	int ret;
 
+	ath12k_mgmt_rx_reo_init_timer(ag);
+
 	mutex_lock(&ah->conf_mutex);
 	ar = ah->radio;
 	ab = ar->ab;
@@ -9119,8 +9157,11 @@ static void ath12k_mac_op_stop(struct ie
 {
 	struct ath12k_hw *ah = hw->priv;
 	struct ath12k *ar;
+	struct ath12k_hw_group *ag = ah->ag;
 	int i;
 
+	ath12k_mgmt_rx_reo_deinit_timer(ag);
+
 	mutex_lock(&ah->conf_mutex);
 	ar = ah->radio;
 
--- a/drivers/net/wireless/ath/ath12k/qmi.c
+++ b/drivers/net/wireless/ath/ath12k/qmi.c
@@ -3927,7 +3927,7 @@ int ath12k_qmi_mlo_global_snapshot_mem_i
 	struct target_mem_chunk *mlo_chunk;
 	int ret = 0, mlo_idx = 0;
 
-	if (!ab->mgmt_rx_reorder)
+	if (!ag->mgmt_rx_reorder)
 		return 0;
 
 	if (!ag->mlo_mem.is_mlo_mem_avail)
