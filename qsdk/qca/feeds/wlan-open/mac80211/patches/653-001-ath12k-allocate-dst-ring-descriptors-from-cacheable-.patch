From f2d16cde5dca4490a3a777c02e2ec92b7a16a71e Mon Sep 17 00:00:00 2001
From: Balamurugan Selvarajan <quic_bselvara@quicinc.com>
Date: Wed, 27 Jul 2022 12:43:04 +0530
Subject: [PATCH 1/2] ath12k:allocate dst ring descriptors from cacheable
 memory

tcl_data and reo_dst rings are currently being allocated
using dma_allocate_coherent() which is non cachable.

Allocating ring memory from cacheable memory area allows
cached descriptor access and prefetch next descriptors to
optimize CPU usage during descriptor processing on NAPI.

Signed-off-by: Balamurugan Selvarajan <quic_bselvara@quicinc.com>
---
 drivers/net/wireless/ath/ath12k/core.c  |  1 +
 drivers/net/wireless/ath/ath12k/dp.c    | 36 +++++++++--
 drivers/net/wireless/ath/ath12k/dp.h    |  1 +
 drivers/net/wireless/ath/ath12k/dp_tx.c | 12 +++-
 drivers/net/wireless/ath/ath12k/hal.c   | 86 ++++++++++++++++++++++++-
 drivers/net/wireless/ath/ath12k/hal.h   |  5 ++
 drivers/net/wireless/ath/ath12k/hw.h    |  1 +
 7 files changed, 134 insertions(+), 8 deletions(-)

--- a/drivers/net/wireless/ath/ath12k/dp.c
+++ b/drivers/net/wireless/ath/ath12k/dp.c
@@ -104,8 +104,11 @@ void ath12k_dp_srng_cleanup(struct ath12
 	if (!ring->vaddr_unaligned)
 		return;
 
-	dma_free_coherent(ab->dev, ring->size, ring->vaddr_unaligned,
-			  ring->paddr_unaligned);
+	if (ring->cached)
+		kfree(ring->vaddr_unaligned);
+	else
+		dma_free_coherent(ab->dev, ring->size, ring->vaddr_unaligned,
+				  ring->paddr_unaligned);
 
 	ring->vaddr_unaligned = NULL;
 }
@@ -223,6 +226,7 @@ int ath12k_dp_srng_setup(struct ath12k_b
 	int entry_sz = ath12k_hal_srng_get_entrysize(ab, type);
 	int max_entries = ath12k_hal_srng_get_max_entries(ab, type);
 	int ret;
+	bool cached = false;
 
 	if (max_entries < 0 || entry_sz < 0)
 		return -EINVAL;
@@ -231,9 +235,26 @@ int ath12k_dp_srng_setup(struct ath12k_b
 		num_entries = max_entries;
 
 	ring->size = (num_entries * entry_sz) + HAL_RING_BASE_ALIGN - 1;
-	ring->vaddr_unaligned = dma_alloc_coherent(ab->dev, ring->size,
-						   &ring->paddr_unaligned,
-						   GFP_KERNEL);
+	if (ab->hw_params->alloc_cacheable_memory) {
+		/* Allocate the reo dst and tx completion rings from cacheable memory */
+		switch (type) {
+		case HAL_REO_DST:
+		case HAL_WBM2SW_RELEASE:
+			cached = true;
+			break;
+		default:
+			cached = false;
+		}
+
+		if (cached) {
+			ring->vaddr_unaligned = kzalloc(ring->size, GFP_KERNEL);
+			ring->paddr_unaligned = virt_to_phys(ring->vaddr_unaligned);
+		}
+	}
+	if (!cached)
+		ring->vaddr_unaligned = dma_alloc_coherent(ab->dev, ring->size,
+							   &ring->paddr_unaligned,
+							   GFP_KERNEL);
 	if (!ring->vaddr_unaligned)
 		return -ENOMEM;
 
@@ -299,6 +320,11 @@ int ath12k_dp_srng_setup(struct ath12k_b
 		return -EINVAL;
 	}
 
+	if (cached) {
+		params.flags |= HAL_SRNG_FLAGS_CACHED;
+		ring->cached = 1;
+	}
+
 	ret = ath12k_hal_srng_setup(ab, type, ring_num, mac_id, &params);
 	if (ret < 0) {
 		ath12k_warn(ab, "failed to setup srng: %d ring_id %d\n",
--- a/drivers/net/wireless/ath/ath12k/dp.h
+++ b/drivers/net/wireless/ath/ath12k/dp.h
@@ -29,6 +29,7 @@ struct dp_srng {
 	dma_addr_t paddr;
 	int size;
 	u32 ring_id;
+	u8 cached;
 };
 
 struct dp_rxdma_ring {
--- a/drivers/net/wireless/ath/ath12k/hal.c
+++ b/drivers/net/wireless/ath/ath12k/hal.c
@@ -1558,6 +1558,21 @@ u32 *ath12k_hal_srng_dst_peek(struct ath
 	return NULL;
 }
 
+static void ath12k_hal_srng_prefetch_desc(struct ath12k_base *ab,
+                                          struct hal_srng *srng)
+{
+	u32 *desc;
+
+	/* prefetch only if desc is available */
+	desc = ath12k_hal_srng_dst_peek(ab, srng);
+	if (likely(desc)) {
+		dma_sync_single_for_cpu(ab->dev, virt_to_phys(desc),
+				        (srng->entry_size * sizeof(u32)),
+					DMA_FROM_DEVICE);
+		prefetch(desc);
+	}
+}
+
 u32 *ath12k_hal_srng_dst_get_next_entry(struct ath12k_base *ab,
 					struct hal_srng *srng)
 {
@@ -1571,11 +1586,74 @@ u32 *ath12k_hal_srng_dst_get_next_entry(
 	desc = srng->ring_base_vaddr + srng->u.dst_ring.tp;
 
 	srng->u.dst_ring.tp = (srng->u.dst_ring.tp + srng->entry_size) %
-			      srng->ring_size;
+			       srng->ring_size;
+
+	/* wrap around to start of ring*/
+	if (srng->u.dst_ring.tp == srng->ring_size)
+		srng->u.dst_ring.tp = 0;
+
+	/* Try to prefetch the next descriptor in the ring */
+	if (srng->flags & HAL_SRNG_FLAGS_CACHED)
+		ath12k_hal_srng_prefetch_desc(ab, srng);
 
 	return desc;
 }
 
+u32 *ath12k_hal_srng_dst_get_next_cache_entry(struct ath12k_base *ab,
+                                              struct hal_srng *srng)
+{
+	u32 *desc,*desc_next;
+	lockdep_assert_held(&srng->lock);
+
+	if (srng->u.dst_ring.tp == srng->u.dst_ring.cached_hp)
+	        return NULL;
+
+	desc = srng->ring_base_vaddr + srng->u.dst_ring.tp;
+
+	srng->u.dst_ring.tp = (srng->u.dst_ring.tp + srng->entry_size) %
+			       srng->ring_size;
+
+	/* Try to prefetch the next descriptor in the ring */
+	if (srng->u.dst_ring.tp != srng->u.dst_ring.cached_hp) {
+		/* prefetch only if desc is available */
+		desc_next = srng->ring_base_vaddr + srng->u.dst_ring.tp;
+		prefetch(desc_next);
+	}
+	return desc;
+}
+
+void ath12k_hal_srng_dst_invalidate_entry(struct ath12k_base *ab,
+					  struct hal_srng *srng, int entries)
+{
+	u32 *desc;
+	u32 tp, hp;
+
+	lockdep_assert_held(&srng->lock);
+
+	if (!(srng->flags & HAL_SRNG_FLAGS_CACHED) || !entries)
+	        return;
+
+	tp = srng->u.dst_ring.tp;
+	hp = srng->u.dst_ring.cached_hp;
+
+	desc = srng->ring_base_vaddr + tp;
+	if (hp > tp) {
+		dma_sync_single_for_cpu(ab->dev, virt_to_phys(desc),
+					entries * srng->entry_size * sizeof(u32),
+					DMA_FROM_DEVICE);
+	} else {
+		entries = srng->ring_size - tp;
+		dma_sync_single_for_cpu(ab->dev, virt_to_phys(desc),
+					entries * sizeof(u32),
+					DMA_FROM_DEVICE);
+
+		entries = hp;
+		dma_sync_single_for_cpu(ab->dev, virt_to_phys(srng->ring_base_vaddr),
+					entries * sizeof(u32),
+					DMA_FROM_DEVICE);
+	}
+}
+
 int ath12k_hal_srng_dst_num_free(struct ath12k_base *ab, struct hal_srng *srng,
 				 bool sync_hw_ptr)
 {
@@ -1706,11 +1784,15 @@ void ath12k_hal_srng_access_begin(struct
 {
 	lockdep_assert_held(&srng->lock);
 
-	if (srng->ring_dir == HAL_SRNG_DIR_SRC)
+	if (srng->ring_dir == HAL_SRNG_DIR_SRC) {
 		srng->u.src_ring.cached_tp =
 			*(volatile u32 *)srng->u.src_ring.tp_addr;
-	else
+	} else {
 		srng->u.dst_ring.cached_hp = *srng->u.dst_ring.hp_addr;
+		/* Try to prefetch the next descriptor in the ring */
+		if (srng->flags & HAL_SRNG_FLAGS_CACHED)
+			ath12k_hal_srng_prefetch_desc(ab, srng);
+	}
 }
 
 /* Update cached ring head/tail pointers to HW. ath12k_hal_srng_access_begin()
--- a/drivers/net/wireless/ath/ath12k/hal.h
+++ b/drivers/net/wireless/ath/ath12k/hal.h
@@ -626,6 +626,7 @@ enum hal_srng_dir {
 #define HAL_SRNG_FLAGS_MSI_INTR			0x00020000
 #define HAL_SRNG_FLAGS_HIGH_THRESH_INTR_EN	0x00080000
 #define HAL_SRNG_FLAGS_LMAC_RING		0x80000000
+#define HAL_SRNG_FLAGS_CACHED                   0x20000000
 
 #define HAL_SRNG_TLV_HDR_TAG		GENMASK(9, 1)
 #define HAL_SRNG_TLV_HDR_LEN		GENMASK(25, 10)
@@ -1152,4 +1153,8 @@ int ath12k_hal_srng_update_shadow_config
 void ath12k_hal_srng_shadow_config(struct ath12k_base *ab);
 void ath12k_hal_srng_shadow_update_hp_tp(struct ath12k_base *ab,
 					 struct hal_srng *srng);
+u32 *ath12k_hal_srng_dst_get_next_cache_entry(struct ath12k_base *ab,
+					      struct hal_srng *srng);
+void ath12k_hal_srng_dst_invalidate_entry(struct ath12k_base *ab,
+					  struct hal_srng *srng, int entries);
 #endif
--- a/drivers/net/wireless/ath/ath12k/hw.h
+++ b/drivers/net/wireless/ath/ath12k/hw.h
@@ -199,6 +199,7 @@ struct ath12k_hw_params {
 	bool supports_ap_ps;
 	bool credit_flow;
 	bool wakeup_mhi;
+	bool alloc_cacheable_memory;
 };
 
 /* BRINGUP: move to dp.h */
--- a/drivers/net/wireless/ath/ath12k/hw.c
+++ b/drivers/net/wireless/ath/ath12k/hw.c
@@ -881,6 +881,7 @@ static const struct ath12k_hw_params ath
 		.supports_ap_ps = true,
 		.credit_flow = false,
 		.wakeup_mhi = false,
+		.alloc_cacheable_memory = true,
 	},
 	{
 		.name = "wcn7850 hw2.0",
@@ -946,6 +947,7 @@ static const struct ath12k_hw_params ath
 		.supports_ap_ps = true,
 		.credit_flow = false,
 		.wakeup_mhi = false,
+		.alloc_cacheable_memory = false,
 	},
 };
 
--- a/drivers/net/wireless/ath/ath12k/dp_tx.c
+++ b/drivers/net/wireless/ath/ath12k/dp_tx.c
@@ -688,14 +688,24 @@ void ath12k_dp_tx_completion_handler(str
 	u32 *desc;
 	u8 mac_id;
 	u64 desc_va;
+	int valid_entries;
 
 	spin_lock_bh(&status_ring->lock);
 
 	ath12k_hal_srng_access_begin(ab, status_ring);
 
+	valid_entries = ath12k_hal_srng_dst_num_free(ab, status_ring, false);
+	if (!valid_entries) {
+		ath12k_hal_srng_access_end(ab, status_ring);
+		spin_unlock_bh(&status_ring->lock);
+		return;
+	}
+
+	ath12k_hal_srng_dst_invalidate_entry(ab, status_ring, valid_entries);
+
 	while ((ATH12K_TX_COMPL_NEXT(tx_ring->tx_status_head) !=
 		tx_ring->tx_status_tail) &&
-	       (desc = ath12k_hal_srng_dst_get_next_entry(ab, status_ring))) {
+	       (desc = ath12k_hal_srng_dst_get_next_cache_entry(ab, status_ring))) {
 		memcpy(&tx_ring->tx_status[tx_ring->tx_status_head],
 		       desc, sizeof(struct hal_wbm_release_ring));
 		tx_ring->tx_status_head =
