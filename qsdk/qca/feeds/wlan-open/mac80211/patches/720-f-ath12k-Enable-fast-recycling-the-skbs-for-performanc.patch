From 65180a508b669fe6c7dc09a72528513bba10e0ac Mon Sep 17 00:00:00 2001
From: Ramanathan Choodamani <quic_rchoodam@quicinc.com>
Date: Tue, 7 Mar 2023 02:14:19 -0800
Subject: [PATCH] ath12k: Enable fast recycling the skbs for performance and 
 desc prefetch

The SKB fast recycling features reduces the cpu overhead by quickly
returning the skb list to the per cpu recycler pool and also
provides information to driver for handling dma cache invalidation
operations in a efficient way

Signed-off-by: Balamurugan Mahalingam <quic_bmahalin@quicinc.com>
Signed-off-by: Ramanathan Choodamani <quic_rchoodam@quicinc.com>
---
 drivers/net/wireless/ath/ath12k/dp.h    |  1 +
 drivers/net/wireless/ath/ath12k/dp_tx.c | 23 ++++++++++++++++++++---
 2 files changed, 21 insertions(+), 3 deletions(-)

--- a/drivers/net/wireless/ath/ath12k/dp.h
+++ b/drivers/net/wireless/ath/ath12k/dp.h
@@ -290,6 +290,7 @@ struct ath12k_tx_desc_info {
 	u32 desc_id; /* Cookie */
 	u8 mac_id;
 	u8 pool_id;
+	u8 recycler_fast_xmit;
 };
 
 struct ath12k_spt_info {
--- a/drivers/net/wireless/ath/ath12k/dp_tx.c
+++ b/drivers/net/wireless/ath/ath12k/dp_tx.c
@@ -90,6 +90,7 @@ struct ath12k_tx_desc_info *ath12k_dp_tx
 	}
 
 	list_move_tail(&desc->list, &dp->tx_desc_used_list[pool_id]);
+	prefetch(desc);
 
 	return desc;
 }
@@ -189,6 +190,12 @@ int ath12k_dp_tx_direct(struct ath12k_li
 
 	tx_desc->skb = skb;
 	tx_desc->mac_id = arvif->pdev_idx;
+	tx_desc->recycler_fast_xmit = 0;
+
+	/* the edma driver uses this flags to optimize the cache invalidation */
+	skb->fast_recycled = 1;
+	if (skb->is_from_recycler)
+		tx_desc->recycler_fast_xmit = 1;
 
 	skb_cb->vif = arvif->ahvif->vif;
 	skb_cb->paddr =  paddr;
@@ -414,6 +421,7 @@ int ath12k_dp_tx(struct ath12k *ar, stru
 
 	tx_desc->skb = skb;
 	tx_desc->mac_id = ar->pdev_idx;
+	tx_desc->recycler_fast_xmit = 0;
 	ti.desc_id = tx_desc->desc_id;
 	ti.data_len = skb->len - ti.pkt_offset;
 	skb_cb->paddr = ti.paddr;
@@ -978,6 +986,8 @@ int ath12k_dp_tx_completion_handler(stru
 	struct hal_wbm_completion_ring_tx *tx_status;
 	enum hal_wbm_rel_src_module buf_rel_source;
 	enum hal_wbm_tqm_rel_reason rel_status;
+	struct sk_buff_head free_list_head;
+	int recycler_fast_xmit;
 
 
 	ath12k_hal_srng_access_dst_ring_begin_nolock(ab, status_ring);
@@ -989,6 +999,7 @@ int ath12k_dp_tx_completion_handler(stru
 	}
 
 	ath12k_hal_srng_dst_invalidate_entry(ab, status_ring, valid_entries);
+	skb_queue_head_init(&free_list_head);
 
 	while (budget && (tx_status = ath12k_hal_srng_dst_get_next_cache_entry(ab, status_ring))) {
 		if (!ath12k_dp_tx_completion_valid(tx_status))
@@ -1013,6 +1024,7 @@ int ath12k_dp_tx_completion_handler(stru
 
 		msdu = tx_desc->skb;
 		mac_id = tx_desc->mac_id;
+		recycler_fast_xmit = tx_desc->recycler_fast_xmit;
 		skb_ext_desc = tx_desc->skb_ext_desc;
 		/* Release descriptor as soon as extracting necessary info
 		 * to reduce contention
@@ -1032,6 +1044,7 @@ int ath12k_dp_tx_completion_handler(stru
 		}
 		budget--;
 
+		ar = ab->pdevs[mac_id].ar;
 		if (atomic_dec_and_test(&ar->dp.num_tx_pending))
 			wake_up(&ar->dp.tx_empty_waitq);
 
@@ -1063,9 +1076,12 @@ int ath12k_dp_tx_completion_handler(stru
 				kfree_skb_list(skb_shinfo(msdu)->frag_list);
 				skb_shinfo(msdu)->frag_list = NULL;
 			}
-			dev_kfree_skb(msdu);
+			if (likely(recycler_fast_xmit)) {
+				__skb_queue_head(&free_list_head, msdu);
+			} else {
+				dev_kfree_skb(msdu);
+			}
 		} else {
-			ar = ab->pdevs[mac_id].ar;
 
 			ath12k_dp_tx_complete_msdu(ar, msdu, tx_status,
 						   buf_rel_source,
@@ -1073,6 +1089,7 @@ int ath12k_dp_tx_completion_handler(stru
 		}
 	}
 	ath12k_hal_srng_access_umac_dst_ring_end_nolock(ab->mem, status_ring);
+	dev_kfree_skb_list_fast(&free_list_head);
 
 	return (orig_budget - budget);
 }
