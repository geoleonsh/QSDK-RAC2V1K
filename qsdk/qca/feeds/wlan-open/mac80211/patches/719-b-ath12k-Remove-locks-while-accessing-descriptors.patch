From c85cdbb7cb0a534cb1dfcba626fdc89591150153 Mon Sep 17 00:00:00 2001
From: Ramanathan Choodamani <quic_rchoodam@quicinc.com>
Date: Fri, 17 Feb 2023 04:38:22 -0800
Subject: [PATCH 2/4] ath12k: Remove locks while accessing descriptors

Each core has its own tx sw descriptors pools. No locks are needed to
access sw descriptor pools

Optimize ring selection operation

Signed-off-by: Balamurugan Mahalingam <quic_bmahalin@quicinc.com>
Signed-off-by: Ramanathan Choodamani <quic_rchoodam@quicinc.com>
---
 drivers/net/wireless/ath/ath12k/dp_tx.c | 37 +++++----------------------------
 1 file changed, 5 insertions(+), 32 deletions(-)

diff --git a/drivers/net/wireless/ath/ath12k/dp_tx.c b/drivers/net/wireless/ath/ath12k/dp_tx.c
index 1a167ff..b3acce9 100644
--- a/drivers/net/wireless/ath/ath12k/dp_tx.c
+++ b/drivers/net/wireless/ath/ath12k/dp_tx.c
@@ -94,26 +94,6 @@ struct ath12k_tx_desc_info *ath12k_dp_tx_assign_buffer_nolock(struct ath12k_dp *
 	return desc;
 }
 
-static struct ath12k_tx_desc_info *ath12k_dp_tx_assign_buffer(struct ath12k_dp *dp,
-							      u8 ring_id)
-{
-	struct ath12k_tx_desc_info *desc = NULL;
-
-	spin_lock_bh(&dp->tx_desc_lock[ring_id]);
-	desc = list_first_entry_or_null(&dp->tx_desc_free_list[ring_id],
-					struct ath12k_tx_desc_info,
-					list);
-	if (unlikely(!desc)) {
-		spin_unlock_bh(&dp->tx_desc_lock[ring_id]);
-		ath12k_warn(dp->ab, "failed to allocate data Tx buffer\n");
-		return NULL;
-	}
-
-	list_move_tail(&desc->list, &dp->tx_desc_used_list[ring_id]);
-	spin_unlock_bh(&dp->tx_desc_lock[ring_id]);
-
-	return desc;
-}
 
 static void ath12k_hal_tx_cmd_ext_desc_setup(struct ath12k_base *ab, void *cmd,
 					     struct hal_tx_info *ti)
@@ -274,14 +254,13 @@ int ath12k_dp_tx(struct ath12k *ar, struct ath12k_link_vif *arvif,
 		return -ENOTSUPP;
 
 	ti.ring_id =
-		ab->hw_params->hw_ops->get_ring_selector(skb) %
-		ab->hw_params->max_tx_ring;
+		ab->hw_params->hw_ops->get_ring_selector(skb);
 
 	ti.rbm_id = ab->hw_params->hal_ops->tcl_to_wbm_rbm_map[ti.ring_id].rbm_id;
 
 	tx_ring = &dp->tx_ring[ti.ring_id];
 
-	tx_desc = ath12k_dp_tx_assign_buffer(dp, ti.ring_id);
+	tx_desc = ath12k_dp_tx_assign_buffer_nolock(dp, ti.ring_id);
 	if (unlikely(!tx_desc)) {
 		ab->soc_stats.tx_err.txbuf_na[ti.ring_id]++;
 		return -ENOSPC;
@@ -419,18 +398,14 @@ int ath12k_dp_tx(struct ath12k *ar, struct ath12k_link_vif *arvif,
 	hal_ring_id = tx_ring->tcl_data_ring.ring_id;
 	tcl_ring = &ab->hal.srng_list[hal_ring_id];
 
-	spin_lock_bh(&tcl_ring->lock);
-
-	ath12k_hal_srng_access_begin(ab, tcl_ring);
-
+	ath12k_hal_srng_access_src_ring_begin_nolock(ab, tcl_ring);
 	hal_tcl_desc = ath12k_hal_srng_src_get_next_entry(ab, tcl_ring);
 	if (unlikely(!hal_tcl_desc)) {
 		/* NOTE: It is highly unlikely we'll be running out of tcl_ring
 		 * desc because the desc is directly enqueued onto hw queue.
 		 */
-		ath12k_hal_srng_access_end(ab, tcl_ring);
+		ath12k_hal_srng_access_umac_src_ring_end_nolock(ab->mem, tcl_ring);
 		ab->soc_stats.tx_err.desc_na[ti.ring_id]++;
-		spin_unlock_bh(&tcl_ring->lock);
 		ret = -ENOMEM;
 
 		goto fail_unmap_dma_ext_desc;
@@ -458,9 +433,7 @@ int ath12k_dp_tx(struct ath12k *ar, struct ath12k_link_vif *arvif,
 	tcl_cmd->info4 = arvif->desc.info4;
 	tcl_cmd->info5 = 0;
 
-	ath12k_hal_srng_access_end(ab, tcl_ring);
-
-	spin_unlock_bh(&tcl_ring->lock);
+	ath12k_hal_srng_access_umac_src_ring_end_nolock(ab->mem, tcl_ring);
 
 	ath12k_dbg_dump(ab, ATH12K_DBG_DP_TX, NULL, "dp tx msdu: ",
 			skb->data, skb->len);
-- 
2.7.4

